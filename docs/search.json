[
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "First session\nSession time\nLocation (Room)\n\n\n\n\n23.10.2023\nWednesdays, 13:15 - 14:45\nFindelgasse, 2.024\n\n\n\n\n\n\n\n\n\nAssignment and course language\n\n\n\n\nThe course can be held in English or German (incl.¬†presentations, pitches, discussions, etc.), depending on the preferences voiced by the students in the survey before the beginning of the course.\nRegardless of the feedback on the course language, the written assignments can be in German or English, depending on what the student/groups prefer.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-information",
    "href": "course-syllabus.html#course-information",
    "title": "Syllabus",
    "section": "",
    "text": "First session\nSession time\nLocation (Room)\n\n\n\n\n23.10.2023\nWednesdays, 13:15 - 14:45\nFindelgasse, 2.024\n\n\n\n\n\n\n\n\n\nAssignment and course language\n\n\n\n\nThe course can be held in English or German (incl.¬†presentations, pitches, discussions, etc.), depending on the preferences voiced by the students in the survey before the beginning of the course.\nRegardless of the feedback on the course language, the written assignments can be in German or English, depending on what the student/groups prefer.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-description",
    "href": "course-syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course description",
    "text": "Course description\nIn this seminar, students are introduced to working with digital behavioral data (DBD). DBD refer to digital traces of human behavior that are knowingly or unknowingly left in online environments (e.g.¬†social media, messengers, entertainment media, or digital collaboration tools). These rich data is increasingly available to social scientific research in the public interest, but can also be used to derive strategic insights for business decisions.\nStudents learn how to work with DBD alongside the entire research process, from data collection, preprocessing and analysis, to reporting and provision (e.g.¬†via open science tools). Students first get a comprehensive overview of the ways in which DBD can be collected (e.g., API scraping, usage logging, mock-up virtual environments, or data donations), as well as the requirements for data protection, research ethics, and data quality. Afterwards, students practice and apply their newly acquired knowledge in small projects on use cases from media and communication research. In doing so, they learn important computer-based methods with which large digital behavioral data sets (e.g.¬†texts, images, usage behavior logs) can be processed and analyzed. By completing this module, participants will get an up-to-date overview and practical insights into how the potential of observational data (digital traces) can be used to better understand the behavior of media users in digital environments.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nStudents will\n\noverview and understand central opportunities of DBD and accompanying challenges for data collection and preprocessing\nevaluate the strengths and weaknesses of different ways of collecting DBD\nget to know and understand central requirements for data protection, research ethics, and data quality\nget to know and overview key computational social science methods to analyze DBD\npractice and apply knowledge on DBD, statistics, and data analysis in small projects of their own",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#recommended-prerequisites",
    "href": "course-syllabus.html#recommended-prerequisites",
    "title": "Syllabus",
    "section": "Recommended prerequisites",
    "text": "Recommended prerequisites\n\nInterest in social scientific perspectives on media, communication, and digital technologies.\nBasic knowledge of working with statistical software such as Stata, R, Python, or SPSS is required.\nStudents are recommended, but not required, to also visit the lecture Data Science: Foundations, Tools, Applications in Socio-Economics and Marketing.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#organization-of-the-course",
    "href": "course-syllabus.html#organization-of-the-course",
    "title": "Syllabus",
    "section": "Organization of the course",
    "text": "Organization of the course\nRegistration for the course takes place via  StudOn. There you will receive the first information and instructions. Please make sure that you complete the short survey before the seminar begins.\nAll slides, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website. I will regularly send out course announcements by e-mail, so please make sure to check your mail address associated with  StudOn regularly.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#preliminary-schedule",
    "href": "course-syllabus.html#preliminary-schedule",
    "title": "Syllabus",
    "section": "(Preliminary) Schedule",
    "text": "(Preliminary) Schedule\n\n\n\n\n\n\nImportant information\n\n\n\n\n‚ö†Ô∏è Please note that this is a provisional timetable which may change, especially after the kick-off meeting and the allocation of topics (see Course schedule for the latest version).\nAll sessions marked with a üî® are hands-on sessions actively working with R.\nAll sessions marked with a üìö are presentation sessions where groups of students will give a detailed presentation (see Assignments for more information).\nAll sessions marked with a üìä are presentation sessions where groups of students will present the results of their project work (see Assignments for more information).\n\n\n\n\n\n\n\n\n\n\n\nSession\nDatum\nTopic\n\n\n\n\n\nüìÇ Block 1\nIntroduction\n\n\n1\n23.10.2024\nKick-Off\n\n\n2\n30.10.2024\nDBD: Introduction & Overview\n\n\n3\n06.11.2024\nüî® Introduction to working with R\n\n\n\nüìÇ Block 2\nTheoretical background:  & TV election debates\n\n\n4\n13.11.2024\nüìö  usage in focus\n\n\n5\n20.11.2024\nüìö Effects of  & TV debates\n\n\n6\n27.11.2024\nüìö Political TV debates & social media\n\n\n\nüìÇ Block 3\nNatural language processing\n\n\n7\n04.12.2024\nüî® Text as data I: Introduction\n\n\n8\n11.12.2024\nüî® Text as data II: Advanced Methods\n\n\n9\n18.12.2024\nüî® Advanced Method I: Topic Modeling\n\n\n-\n-\nüéÑChristmas Break (No Lecture)\n\n\n10\n08.01.2025\nüî® Advanced Method II: Machine Learning\n\n\n\nüìÇ Block 5\nProject Work\n\n\n11\n15.01.2025\nüî® Project work\n\n\n12\n22.01.2025\nüî® Project work\n\n\n13\n29.01.2025\nüìä Project Presentation (I)\n\n\n14\n05.02.2025\nüìä Project Presentation (II) & üèÅ Evaluation\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor the latest, more detailed version of the course schedule as well as the linked content of the individual sessions (e.g.¬†slides or literature for the respective presentation), please see Schedule.\n\n\nThe course consists of several blocks:\n\nBlock I: Introduction\nThe first four sessions form the (theoretical) basis for the course.\n\nThe kick-off session is mainly for getting to know each other and organizing the course.¬†\nThe second session is to give you an extended introduction DBD, including challenges and important frameworks.¬†\nThe third session is about practical work with R and RStudio. ¬†\n\n\n\nBlock II: Theoretical foundation\nThe second block will contain the presentations by different groups of students about the research relevant for the course.\n\n\nBlock III: Natural Language Processing (NLP)\nThe third block is about working with text data. It is separated into four parts.\n\nThe first session is an introduction to working with text data.\nThe second session is about advanced methods for working with text data and introduces the two methods of the next two sessions.\nThe third session is about topic modeling, a method to extract topics from text data.\nThe fourth session is about machine learning, a method to classify text data.\n\n\n\nBlock IV: Project Work\nThe last block is about working on your project. The goal is to combine the theoretical and practical knowledge you have gained in the previous sessions and apply it to a research project of your choice. This means you have to find a research question, develop a concept of an analysis, run it, analyze it, and present your results (short presentation and written report).",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#sessions",
    "href": "course-syllabus.html#sessions",
    "title": "Syllabus",
    "section": "Sessions",
    "text": "Sessions\nThe goal of the sessions is to be as interactive as possible. In general, the sessions consist of two parts. In the first part (¬± 30 - 45 minutes) at the beginning of the session, there are usually presentations (including discussion), which are more or less detailed depending on the stage of the project. The second part (¬± 45 - 60 minutes) consists of a group activity (with concluding discussion), which should either be about deepening the presentation content or about independent work on one‚Äôs own or the group project.\nMy role as instructor is to introduce you new tools and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. You are expected to bring a laptop to each class so that you can take part in the in-session exercises. Please make sure your laptop is fully charged before you come to class as the number of outlets in the classroom will not be sufficient to accommodate everyone.\n\nWhere to ask questions\n\nIf you have a question during the lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nAny general questions about session content, assignments or about the project should be posted into the  StudOn-Forum, so that everyone can benefit from the answers. There is a chance another student has already asked a similar question, so please check the other posts before adding a new question. If you know the answer to a question, I encourage you to respond!\nE-mails should be reserved for personal matters.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#assessment",
    "href": "course-syllabus.html#assessment",
    "title": "Syllabus",
    "section": "Assessment",
    "text": "Assessment\nIn order to obtain credits and a grade, participants are required to\n\nattend regularly (at least 80% of the sessions) and participate actively. A maximum of two sessions can be missed without excuse. Absence in further sessions can only be excused in case of illness (i. e. with a medical certificate).\ncomplete various assignments as part of a portfolio. The type and scope of the assignments depends on the number of participants and the project(s). Detailed information can be found in the section Assignments.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#academic-integrity",
    "href": "course-syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic integrity",
    "text": "Academic integrity\n\n\n\n\n\n\nTL;DR\n\n\n\nDo not cheat!\n\n\nFor general information on formatting, style, citation, appendices, wording of the affidavit, etc., see our Guide to Academic Writing.\n\nPolicy on sharing and reusing code\nI am well aware that a huge volume of code is available on the web to solve any number of problems. Unless I explicitly tell you not to use something, the course‚Äôs policy is that you may make use of any online resources (e.g.¬†StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\n\n\nPolicy on use of generative artificial intelligence (AI):\nYou should treat generative AI, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course1: (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate‚Äîrather than hinder‚Äîlearning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n‚úÖ AI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\n‚ùå AI tools for narrative: Unless instructed otherwise, you may not use generative AI to write narrative on assignments. In general, you may use generative AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#recommended-textbooks",
    "href": "course-syllabus.html#recommended-textbooks",
    "title": "Syllabus",
    "section": "Recommended textbooks",
    "text": "Recommended textbooks\n\nAtteveldt, W. van, Trilling, D., & Arc√≠la, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Handbook of Computational Social Science, Volume 1: Theory, Case Studies and Ethics (1st ed.). Routledge. https://doi.org/10.4324/9781003024583\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Handbook of computational social science, volume 2. Routledge. https://doi.org/10.4324/9781003025245\nHaim, M. (2023). Computational Communication Science: Eine Einf√ºhrung. Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-40171-9\nSalganik, M. J. (2018). Bit by bit: Social research in the digital age. Princeton University Press.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#footnotes",
    "href": "course-syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D‚Ü©Ô∏é",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "computing/computing-textbooks.html",
    "href": "computing/computing-textbooks.html",
    "title": "R textbooks",
    "section": "",
    "text": "While there is no official R textbook for the course, there are a few to look at:\n\nüîó R for Data Science, 2nd Edition\nüîó Data Visualization: A Practical Introduction\nüîó Tidy modeling with R\nüîó Text Mining with R",
    "crumbs": [
      "Working with R",
      "R Textbooks"
    ]
  },
  {
    "objectID": "computing/computing-instructions_R.html",
    "href": "computing/computing-instructions_R.html",
    "title": "Introduction",
    "section": "",
    "text": "Assignment\n\n\n\n\nPlease watch all the videos in the tutorial series on Introduction to R, RStudio & Quarto and Practical Work with R by December 1st, 2024 at the latest. The content conveyed in the videos (e.g., knowledge and application of certain functions to filter, summarize, or edit data) will be assumed as a foundation for session 7 of the seminar.\nIf you have questions and/or problems, please write a post in the forum of the StudOn course so that everyone can benefit from the answers. Use the prefix ‚ÄúQuestion/Problem R:‚Äù in the subject line and tag/add me. If available, please always include the material/video to which your question refers.",
    "crumbs": [
      "Working with R",
      "R Video Tutorials"
    ]
  },
  {
    "objectID": "computing/computing-instructions_R.html#background",
    "href": "computing/computing-instructions_R.html#background",
    "title": "Introduction",
    "section": "Background",
    "text": "Background\nPractical work with R, RStudio, and Quarto is an integral part of the Digital Behavioral Data course. To accommodate different levels of prior knowledge while establishing a common ‚Äúbasic knowledge‚Äù for the course, we would like to provide you with a series of introductory videos to facilitate your (re)entry. Specifically, this consists of a mix of YouTube tutorials by  Andy Field, which initially teach the general handling of R, RStudio & Quarto, and the materials from CCS.Amsterdam, which focus on more ‚Äúsubstantive‚Äù work with R. All tutorials are in English.",
    "crumbs": [
      "Working with R",
      "R Video Tutorials"
    ]
  },
  {
    "objectID": "computing/computing-instructions_R.html#introduction-to-r-rstudio-quarto",
    "href": "computing/computing-instructions_R.html#introduction-to-r-rstudio-quarto",
    "title": "Introduction",
    "section": "Introduction to R, RStudio & Quarto",
    "text": "Introduction to R, RStudio & Quarto\n\nThis video tutorial series was created by  Andy Field to acompany his books An Adventure in Statistics (Field and Iles 2016) and Discovering Statistics Using R (Field, Miles, and Field 2012), and deals with the use of RStudio and Quarto for interacting with R.\nThe videos cover the installation of R, RStudio, and Quarto, the differences between them, a tour of RStudio, good workflows in RStudio, installing and loading packages, and using Quarto.\n\n\n\n\n\n\n\nPlease note ‚Ä¶\n\n\n\n\nMost of the exercises shown in the tutorial can be easily reproduced, either by manually entering the data or variables or by using your own files (e.g., when embedding graphics).\nHowever, some data used (e.g., in the session RStudio Working with Code: Part 3) is unfortunately not available publically. In this case, you can either use your own ‚Äúdata‚Äù to reproduce the examples or rely on built-in R datasets (such as airquality, mtcars, iris, etc.) by loading them with the data() command (e.g., data(mtcars)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession\nTitel\nDauer\n\n\n\n\n1\n\n\n\n\n Installing R and RStudio\n06:13\n\n\n2\n\n\n\n\n Installing Quarto\n02:55\n\n\n3\n\n\n\n\n R Studio desktop workflow (2022)\n08:16\n\n\n4\n\n\n\n\n Customizing Rstudio\n08:03\n\n\n5\n\n\n\n\n Quarto visual editor [Part 1]\n10:20\n\n\n6\n\n\n\n\n Quarto visual editor [Part 2]\n13:19\n\n\n7\n\n\n\n\n Quarto visual editor [Part 3]\n10:39\n\n\n8\n\n\n\n\n Quarto visual editor [Part 4]\n04:28\n\n\n9\n\n\n\n\n Quarto visual editor [Part 5]\n12:10\n\n\n10\n\n\n\n\n RStudio Working with Code: Part 1\n07:41\n\n\n11\n\n\n\n\n RStudio Working with Code: Part 2\n14:08\n\n\n12\n\n\n\n\n RStudio Working with Code: Part 3\n08:44\n\n\n13\n\n\n\n\n RStudio Working with Code: Part 4\n06:47\n\n\n14\n\n\n\n\n RStudio Working with Code: Part 5\n07:07\n\n\n15\n\n\n\n\n RStudio Working with Code: Part 6\n05:48\n\n\n\nYou can also play the videos as a YouTube playlist.",
    "crumbs": [
      "Working with R",
      "R Video Tutorials"
    ]
  },
  {
    "objectID": "computing/computing-instructions_R.html#practical-work-with-r",
    "href": "computing/computing-instructions_R.html#practical-work-with-r",
    "title": "Introduction",
    "section": "Practical Work with R",
    "text": "Practical Work with R\n\nCCS.Amsterdam is a group of ‚ÄúComputational‚Äù communication scientists from the University of Amsterdam and the Vrije Universiteit Amsterdam. In various research projects, these scientists aim to use and develop computational methods to answer social science research questions. This includes, among other things, the study of news streams, polarization, political microtargeting, fake news, and recommender design. A main goal of the group is to disseminate knowledge among a growing community of enthusiastic ‚ÄúComputational‚Äù communication scientists.\nThe series of tutorials curated by CCS.Amsterdam aims to teach the use of tidyverse functions for data cleaning, transformation, visualization, etc. The tutorials consist of both handouts, i.e., documents explaining the most important commands, and video tutorials covering the same material.\nThe table also lists chapters from Computational Analysis of Communication [CAC] and R for Data Science [R4DS], two 100% free and openly accessible books that also cover and possibly deepen the material of the respective session.\n\n\n\n\n\n\n\nPlease note\n\n\n\n\nThe video tutorials may be slightly older than the handouts. In case of doubt, follow the content of the handouts rather than the videos.\nPlease note that the CAC offers R and Python examples side by side. You may need to actively select the R code examples.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession\nVideo-Tutorial\nMaterialien\nLiteratur\nDauer\n\n\n\n\n1\n\n\n\n\n R basics: commands, objects, and functions\n\n\n\n\n [Handout]\n\n\n\n\n [CAC] [R4DS]\n29:30\n\n\n2\n\n\n\n\n R Tidyverse: Data transformation\n\n\n\n\n [Handout]\n\n\n\n\n [CAC] [R4DS]\n22:19\n\n\n3\n\n\n\n\n R Tidyverse: Data summarization\n\n\n\n\n [Handout]\n\n\n\n\n [CAC] [R4DS]\n11:00\n\n\n4\n\n\n\n\n R ggplot2: Basics of data visualization\n\n\n\n\n [Handout]\n\n\n\n\n [CAC] [R4DS]\n35:14\n\n\n\nYou can also play the videos as a YouTube playlist.",
    "crumbs": [
      "Working with R",
      "R Video Tutorials"
    ]
  },
  {
    "objectID": "tutorials/tutorial-08.html",
    "href": "tutorials/tutorial-08.html",
    "title": "üî® Advanced Methods",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "tutorials/tutorial-08.html#background",
    "href": "tutorials/tutorial-08.html#background",
    "title": "üî® Advanced Methods",
    "section": "Background",
    "text": "Background"
  },
  {
    "objectID": "tutorials/tutorial-08.html#preparation",
    "href": "tutorials/tutorial-08.html#preparation",
    "title": "üî® Advanced Methods",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, \n    magrittr, janitor,\n    ggpubr, \n    gt, gtExtras,\n    countdown, \n    quanteda, # quanteda text processing\n    quanteda.textplots, quanteda.textstats, quanteda.textmodels,\n    udpipe, spacyr, # POS tagging\n    easystats, tidyverse\n)\n\n\n# Import base data\nchats &lt;- qs::qread(here(\"local_data/chat-debates_full.qs\"))$correct\ntranscripts &lt;- qs::qread(here(\"local_data/transcripts-debates_full.qs\"))$correct\ndict_chat_emotes &lt;- readRDS(here(\"local_data/dictionary_chat_emotes.RDS\"))\n\n# Import corpora\ntranscripts_udpipe &lt;- qs::qread(here(\"local_data/transcripts-corpus_udpipe.qs\"))\ntranscripts_spacyr &lt;- qs::qread(here(\"local_data/transcripts-corpus_spacyr.qs\"))\ntranscripts_pos &lt;- transcripts_udpipe"
  },
  {
    "objectID": "tutorials/tutorial-08.html#codechunks-aus-der-sitzung",
    "href": "tutorials/tutorial-08.html#codechunks-aus-der-sitzung",
    "title": "üî® Advanced Methods",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nErstellung der Datengrundlage\n\n# Create corpus\ncorp_transcripts &lt;- transcripts %&gt;% \n  quanteda::corpus(\n    docid_field = \"id_sequence\", \n    text_field = \"dialogue\"\n  )\n\n# Tokenize corpus\ntoks_transcripts &lt;- corp_transcripts %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  ) %&gt;% \n  quanteda::tokens_remove(\n    pattern = quanteda::stopwords(\"en\")\n  )\n\n# Add n_grams\ntoks_transcripts_ngrams &lt;- toks_transcripts %&gt;% \n  quanteda::tokens_ngrams(n = 1:3)\n\n# Convert to DFM\ndfm_transcripts &lt;- toks_transcripts_ngrams %&gt;% \n  quanteda::dfm()\n\n\n# Create corpus\ncorp_chats &lt;- chats %&gt;% \n  quanteda::corpus(docid_field = \"message_id\", text_field = \"message_content\")\n\n# Tokenize corpus\ntoks_chats &lt;- corp_chats %&gt;% quanteda::tokens()\n\n# Convert to DFM\ndfm_chats &lt;- toks_chats %&gt;% quanteda::dfm()\n\n\n\nNgrams: Sequenzen von N aufeinanderfolgenden Token\n\ntoks_transcripts %&gt;% \n  quanteda::tokens_ngrams(n = 2) %&gt;% \n  quanteda::dfm() %&gt;%  \n  quanteda.textstats::textstat_frequency() %&gt;% \n  head(25) \n\n             feature frequency rank docfreq group\n1          know_know      1337    1      49   all\n2  t-mobile_t-mobile       864    2       6   all\n3       donald_trump       755    3     461   all\n4          going_say       666    4      30   all\n5          say_going       661    5      35   all\n6         saying_bad       558    6       4   all\n7         bad_saying       553    7       3   all\n8      kamala_harris       494    8     333   all\n9     vice_president       429    9     376   all\n10   curious_curious       373   10       7   all\n11    sekunden_pause       354   11     266   all\n12         right_now       269   12     234   all\n13     united_states       268   13     211   all\n14         feel_like       230   14     164   all\n15             oh_oh       229   15      10   all\n16         like_know       208   16     133   all\n17   president_trump       203   17     178   all\n18         like_like       191   18     144   all\n19  president_harris       186   19     179   all\n20        lot_people       181   20     138   all\n21         know_like       168   21     109   all\n22   american_people       163   22     118   all\n23            oh_god       154   23     139   all\n24         just_like       153   24     129   all\n25  former_president       141   25     115   all\n\n\n\n\nKollokationen: Identifikation von bedeutungsvollen Wortkombinationen\n\ntoks_transcripts %&gt;% \n  quanteda.textstats::textstat_collocations(\n    size = 2, \n    min_count = 5\n  ) %&gt;% \n  head(25)\n\n        collocation count count_nested length    lambda        z\n1         know know  1337            0      2  3.890787 98.31370\n2        saying bad   558            0      2  6.503305 81.19215\n3        bad saying   553            0      2  6.481795 80.98625\n4         going say   666            0      2  4.555737 80.92246\n5         say going   661            0      2  4.562963 80.82524\n6      donald trump   755            0      2  7.422847 75.19267\n7     kamala harris   494            0      2  7.873933 68.88003\n8    vice president   429            0      2  7.258104 56.72753\n9             oh oh   229            0      2  4.679549 54.95066\n10        right now   269            0      2  3.996328 53.27167\n11    senator vance   129            0      2  6.583164 49.48173\n12       little bit   132            0      2  8.268099 45.80914\n13 president harris   186            0      2  4.027681 45.69845\n14           oh god   154            0      2  6.175930 44.94423\n15        years ago   102            0      2  6.436807 43.06424\n16         tim walz    90            0      2  7.588398 43.05596\n17  president trump   203            0      2  3.468589 42.70406\n18       four years   100            0      2  6.381600 42.53221\n19      health care   136            0      2  7.400206 41.83123\n20      white house    85            0      2  8.071701 41.52827\n21   donald trump's   132            0      2  6.408658 41.05938\n22 former president   141            0      2  5.790480 41.04486\n23  curious curious   373            0      2 11.836611 40.77202\n24    governor walz    77            0      2  6.512020 39.65499\n25      two minutes    84            0      2  6.490910 39.39074\n\n\n\n\nArbeiten mit quanteda: corpus\n\n# Create corpus\ncorp_transcripts &lt;- transcripts %&gt;% \n  quanteda::corpus(\n    docid_field = \"id_sequence\", \n    text_field = \"dialogue\"\n  )\n\n# Output\ncorp_transcripts\n\nCorpus consisting of 5,861 documents and 10 docvars.\np1_s0001 :\n\"Tonight, the high-stakes showdown here in Philadelphia betwe...\"\n\np1_s0002 :\n\"A historic race for president upended just weeks ago, Presid...\"\n\np1_s0003 :\n\"The candidates separated by the smallest of margins, essenti...\"\n\np1_s0004 :\n\"This is an ABC News special. The most consequential moment o...\"\n\np1_s0005 :\n\"Together, we'll chart a... (..)\"\n\np1_s0006 :\n\"Donald Trump.\"\n\n[ reached max_ndoc ... 5,855 more documents ]\n\n\n\n\nKeywords-in-Context (KWIC)\n\nUnmittelbarer Wortkontext ohne statistische Gewichtung\n\ntoks_transcripts %&gt;% \n  kwic(\"know\", window = 3) %&gt;% \nhead(10)\n\nKeyword-in-context with 10 matches.                                                                              \n [p1_s0018, 29]  opportunity economy thing | know | shortage homes housing    \n [p1_s0018, 39]            far many people | know | young families need       \n [p1_s0020, 25]  billions billions dollars | know | China fact never          \n [p1_s0022, 44]          done intend build | know | aspirations hopes American\n  [p1_s0024, 2]                    nothing | know | knows better anyone       \n  [p1_s0025, 1]                            | know | everybody else Vice       \n [p1_s0026, 64]        stand issues invite | know | Donald Trump actually     \n [p1_s0028, 38]       goods coming country | know | many economists say       \n [p1_s0029, 24] billions dollars countries | know | like gone immediately     \n [p1_s0031, 90]         Thank President Xi | know | Xi responsible lacking    \n\n\n\n\nEinsatz zur Qualit√§tskontrolle\n\ntoks_transcripts %&gt;% \n  kwic(\n    phrase(\"know know\"),\n    window = 3) %&gt;%\n  tibble() %&gt;% \n  select(-pattern) %&gt;% \n  slice(35:45) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(\n        table.width = gt::pct(100), \n        table.font.size = \"10px\"\n    )\n\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\n\n\n\n\nvp2_s0723\n94\n95\nkiss just kiss\nknow know\njust kiss kiss\n\n\nvp2_s0732\n119\n120\ndefault press even\nknow know\ndifference campaign strategy\n\n\nvp3_s0151\n32\n33\ncop able assess\nknow know\nJ.D Vance lying\n\n\nvp3_s0332\n3\n4\nreally mean\nknow know\nmany people tune\n\n\nvp3_s0332\n116\n117\ngenerous Sekunden Pause\nknow know\ntype like know\n\n\nvp3_s0332\n120\n121\nknow type like\nknow know\ntype like know\n\n\nvp3_s0332\n124\n125\nknow type like\nknow know\nknow know know\n\n\nvp3_s0332\n125\n126\ntype like know\nknow know\nknow know know\n\n\nvp3_s0332\n126\n127\nlike know know\nknow know\nknow know know\n\n\nvp3_s0332\n127\n128\nknow know know\nknow know\nknow know know\n\n\nvp3_s0332\n128\n129\nknow know know\nknow know\nknow know know\n\n\n\n\n\n\n\n\n\n\nNgrams als Features definieren\n\n# Definition von Features\ncustom_ngrams &lt;- c(\"donald trump\", \"joe biden\", \"kamala harris\")\n\n# Anwendung auf DFM\ndfm_with_custom_ngrams &lt;- toks_transcripts %&gt;% \n  tokens_compound(pattern = phrase(custom_ngrams)) %&gt;% \n  dfm() %&gt;% \n  dfm_trim(min_docfreq = 0.005, max_docfreq = 0.99, docfreq_type = \"prop\") \n\n# √úberpr√ºfung\ndfm_with_custom_ngrams %&gt;% \n  convert(to = \"data.frame\") %&gt;% \n  select(doc_id, starts_with(\"donald\")) %&gt;% \n  head()\n\n    doc_id donald_trump donald\n1 p1_s0001            1      0\n2 p1_s0002            1      0\n3 p1_s0003            0      0\n4 p1_s0004            0      0\n5 p1_s0005            0      0\n6 p1_s0006            1      0\n\n\n\n\nSemantische Netzwerke: Visualisierung von Tokenbeziehungen\n\n# Lookup emotes in DFM of chats\ndfm_emotes &lt;- dfm_chats %&gt;% \n  quanteda::dfm_lookup(\n    dictionary = dict_chat_emotes)\n\n# Output frequency of emojis\ntop50_emotes &lt;- dfm_emotes %&gt;% \n  topfeatures(50) %&gt;% \n  names()\n\n# Visualize\ndfm_emotes  %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top50_emotes) %&gt;% \n  textplot_network()\n\n\n\n\n\n\n\n\n\n\nPOS-Tagging & Dependency Parsing\n\nudmodel &lt;- udpipe::udpipe_download_model(language = \"english\")\n\ntranscripts_pos &lt;- transcripts %&gt;%\n  rename(doc_id = id_sequence, text = dialogue) %&gt;% \n  udpipe::udpipe(udmodel)\n\n\ntranscripts_pos %&gt;% \n  select(doc_id, sentence_id, token_id, token, head_token_id, lemma, upos, xpos) %&gt;% \n  head(n = 7) %&gt;% \n  gt() %&gt;% gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(table.width = gt::pct(100), table.font.size = \"12px\")\n\n\n\n\n\n\n\ndoc_id\nsentence_id\ntoken_id\ntoken\nhead_token_id\nlemma\nupos\nxpos\n\n\n\n\np1_s0001\n1\n1\nTonight\n0\ntonight\nNOUN\nNN\n\n\np1_s0001\n1\n2\n,\n1\n,\nPUNCT\n,\n\n\np1_s0001\n1\n3\nthe\n7\nthe\nDET\nDT\n\n\np1_s0001\n1\n4\nhigh\n6\nhigh\nADJ\nJJ\n\n\np1_s0001\n1\n5\n-\n6\n-\nPUNCT\nHYPH\n\n\np1_s0001\n1\n6\nstakes\n7\nstake\nNOUN\nNNS\n\n\np1_s0001\n1\n7\nshowdown\n1\nshowdown\nNOUN\nNN\n\n\n\n\n\n\n\n\n\nMit welchen W√∂rtern wird Trump beschrieben?\n\ntranscripts_pos %&gt;% \n    filter(\n      upos == \"NOUN\" &\n      lemma == \"trump\") %&gt;%\n    inner_join(\n      transcripts_pos,\n      by = c(\n        \"doc_id\",\n        \"sentence_id\"),\n      relationship = \n        \"many-to-many\") %&gt;%\n    filter(\n      upos.y == \"ADJ\" &\n      head_token_id.y == token_id.x) %&gt;% \n    rename(\n      token_id = token_id.y,\n      token = token.y) %&gt;% \n    select(\n      doc_id, sentence_id,\n      token_id, token) %&gt;%\n    sjmisc::frq(token, sort.frq = \"desc\") \n\ntoken &lt;character&gt; \n# total N=161 valid N=161 mean=3.72 sd=4.67\n\nValue        |   N | Raw % | Valid % | Cum. %\n---------------------------------------------\ndonald       | 132 | 81.99 |   81.99 |  81.99\nDonald       |   4 |  2.48 |    2.48 |  84.47\num           |   3 |  1.86 |    1.86 |  86.34\nformer       |   2 |  1.24 |    1.24 |  87.58\nnarcissistic |   2 |  1.24 |    1.24 |  88.82\nbad          |   1 |  0.62 |    0.62 |  89.44\ngood         |   1 |  0.62 |    0.62 |  90.06\ngreat        |   1 |  0.62 |    0.62 |  90.68\niran         |   1 |  0.62 |    0.62 |  91.30\nlaura        |   1 |  0.62 |    0.62 |  91.93\nmuch         |   1 |  0.62 |    0.62 |  92.55\nokay         |   1 |  0.62 |    0.62 |  93.17\nother        |   1 |  0.62 |    0.62 |  93.79\npast         |   1 |  0.62 |    0.62 |  94.41\nSaid         |   1 |  0.62 |    0.62 |  95.03\nselfish      |   1 |  0.62 |    0.62 |  95.65\nsocial       |   1 |  0.62 |    0.62 |  96.27\ntighter      |   1 |  0.62 |    0.62 |  96.89\ntotal        |   1 |  0.62 |    0.62 |  97.52\nunfit        |   1 |  0.62 |    0.62 |  98.14\nunseat       |   1 |  0.62 |    0.62 |  98.76\nweaker       |   1 |  0.62 |    0.62 |  99.38\nweird        |   1 |  0.62 |    0.62 | 100.00\n&lt;NA&gt;         |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\ntranscripts_spacyr %&gt;%  \n    filter(\n      pos == \"NOUN\" &\n      lemma == \"trump\") %&gt;%\n    inner_join(\n      transcripts_spacyr,\n      by = c(\n        \"doc_id\",\n        \"sentence_id\"),\n      relationship = \n        \"many-to-many\") %&gt;%\n    filter(\n      pos.y == \"ADJ\" &\n      head_token_id.y == token_id.x) %&gt;% \n    rename(\n      token_id = token_id.y,\n      token = token.y) %&gt;% \n    select(\n      doc_id, sentence_id,\n      token_id, token) %&gt;%\n    sjmisc::frq(token, sort.frq = \"desc\") \n\ntoken &lt;character&gt; \n# total N=10 valid N=10 mean=5.40 sd=2.88\n\nValue        | N | Raw % | Valid % | Cum. %\n-------------------------------------------\nunfit        | 2 |    20 |      20 |     20\nbad          | 1 |    10 |      10 |     30\ndonald       | 1 |    10 |      10 |     40\nfucking      | 1 |    10 |      10 |     50\nnarcissistic | 1 |    10 |      10 |     60\nother        | 1 |    10 |      10 |     70\nsame         | 1 |    10 |      10 |     80\ntighter      | 1 |    10 |      10 |     90\ntotal        | 1 |    10 |      10 |    100\n&lt;NA&gt;         | 0 |     0 |    &lt;NA&gt; |   &lt;NA&gt;"
  },
  {
    "objectID": "tutorials/tutorial-07.html",
    "href": "tutorials/tutorial-07.html",
    "title": "üî® Text as data in R",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "tutorials/tutorial-07.html#background",
    "href": "tutorials/tutorial-07.html#background",
    "title": "üî® Text as data in R",
    "section": "Background",
    "text": "Background"
  },
  {
    "objectID": "tutorials/tutorial-07.html#preparation",
    "href": "tutorials/tutorial-07.html#preparation",
    "title": "üî® Text as data in R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, taylor,\n    magrittr, janitor,\n    ggpubr, \n    gt, gtExtras,\n    countdown, \n    quanteda, # quanteda text processing\n    quanteda.textplots, \n    easystats, tidyverse\n)\n\n\n# Import data from URL\nchats &lt;- qs::qread(here(\"local_data/chat-debates_full.qs\"))$correct\ntranscripts &lt;- qs::qread(here(\"local_data/transcripts-debates_full.qs\"))$correct\nstreamer_stats &lt;- qs::qread(here(\"local_data/twitch_streamer_stats.qs\"))"
  },
  {
    "objectID": "tutorials/tutorial-07.html#codechunks-aus-der-sitzung",
    "href": "tutorials/tutorial-07.html#codechunks-aus-der-sitzung",
    "title": "üî® Text as data in R",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\n√úberblick √ºber verschiedenen Statistiken der betrachteten Streamer\n\nstreamer_stats %&gt;% \n  pivot_longer(cols = c(avg_viewers, followers, hours_streamed), names_to = \"statistic\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = month, y = value, fill = streamer)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_grid(statistic ~ ., scales = \"free_y\", labeller = as_labeller(c(\n    avg_viewers = \"Average Viewers\",\n    followers = \"Followers\",\n    hours_streamed = \"Hours Streamed\"))) +\n  theme_minimal() +\n  labs(\n    x = \"Month\",\n    y = \"\",\n    title = \"Streamer Statistics Over Time\", \n    fill = \"Streamer\") +\n  scale_y_continuous(labels = scales::comma) +\n  ggsci::scale_fill_cosmic()\n\n\n\n\n\n\n\n\n\n\n√úberblick √ºber den chats-Datensatz\n\nchats %&gt;% glimpse\n\nRows: 913,375\nColumns: 33\n$ streamer              &lt;chr&gt; \"hasanabi\", \"hasanabi\", \"hasanabi\", \"hasanabi\", ‚Ä¶\n$ url                   &lt;chr&gt; \"https://www.twitch.tv/videos/2247664726\", \"http‚Ä¶\n$ platform              &lt;chr&gt; \"twitch\", \"twitch\", \"twitch\", \"twitch\", \"twitch\"‚Ä¶\n$ debate                &lt;chr&gt; \"presidential\", \"presidential\", \"presidential\", ‚Ä¶\n$ user_name             &lt;chr&gt; \"bendaspur\", \"spackle_pirate\", \"texaschollima\", ‚Ä¶\n$ user_id               &lt;chr&gt; \"54058406\", \"182041182\", \"185502300\", \"159018462‚Ä¶\n$ user_display_name     &lt;chr&gt; \"BenDaSpur\", \"spackle_pirate\", \"TexasChollima\", ‚Ä¶\n$ user_badges           &lt;list&gt; [], [], [], [[\"twitch_recap_2023\", 1, \"Twitch R‚Ä¶\n$ message_timestamp     &lt;dbl&gt; 19, 19, 20, 20, 21, 21, 22, 22, 24, 25, 25, 25, ‚Ä¶\n$ message_id            &lt;chr&gt; \"dc03b89a-722d-4eaa-a895-736533a68aca\", \"6be50e1‚Ä¶\n$ message_type          &lt;chr&gt; \"text_message\", \"text_message\", \"text_message\", ‚Ä¶\n$ message_content       &lt;chr&gt; \"60fps LETSGO 60fps LETSGO 60fps LETSGO 60fps LE‚Ä¶\n$ message_emotes        &lt;list&gt; [], [], [], [], [], [], [], [], [], [], [], [[\"‚Ä¶\n$ message_length        &lt;int&gt; 51, 17, 20, 27, 35, 14, 20, 5, 10, 9, 106, 97, 3‚Ä¶\n$ message_timecode      &lt;Period&gt; 19S, 19S, 20S, 20S, 21S, 21S, 22S, 22S, 24S, ‚Ä¶\n$ message_time          &lt;chr&gt; \"00:00:19\", \"00:00:19\", \"00:00:20\", \"00:00:20\", ‚Ä¶\n$ message_during_debate &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_has_badge        &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, ‚Ä¶\n$ user_is_premium       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, ‚Ä¶\n$ user_is_subscriber    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_turbo         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_moderator     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ‚Ä¶\n$ user_is_partner       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ‚Ä¶\n$ user_is_subgifter     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_broadcaster   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_vip           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_twitchdj      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_founder       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_staff         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_game_dev      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_ambassador    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_no_audio         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_no_video         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n\nchats %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n913375\n\n\nNumber of columns\n33\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n11\n\n\nlist\n2\n\n\nnumeric\n19\n\n\nTimespan\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nstreamer\n0\n1\n8\n19\n0\n3\n0\n\n\nurl\n0\n1\n39\n43\n0\n6\n0\n\n\nplatform\n0\n1\n6\n7\n0\n2\n0\n\n\ndebate\n0\n1\n12\n17\n0\n2\n0\n\n\nuser_name\n0\n1\n0\n36\n9\n89201\n0\n\n\nuser_id\n0\n1\n2\n24\n0\n89055\n0\n\n\nuser_display_name\n0\n1\n0\n36\n9\n89217\n0\n\n\nmessage_id\n0\n1\n36\n40\n0\n913375\n0\n\n\nmessage_type\n0\n1\n12\n12\n0\n1\n0\n\n\nmessage_content\n0\n1\n0\n601\n121\n589125\n1\n\n\nmessage_time\n0\n1\n8\n8\n0\n38911\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\nuser_badges\n0\n1\n707\n0\n3\n\n\nmessage_emotes\n0\n1\n15840\n0\n13\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmessage_timestamp\n0\n1\n20052.35\n11102.30\n-152\n9498\n22129\n29762\n38954\n‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÜ\n\n\nmessage_length\n0\n1\n26.71\n31.68\n0\n7\n16\n34\n601\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nmessage_during_debate\n0\n1\n0.28\n0.45\n0\n0\n0\n1\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÉ\n\n\nuser_has_badge\n0\n1\n0.64\n0.48\n0\n0\n1\n1\n1\n‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nuser_is_premium\n42\n1\n0.21\n0.41\n0\n0\n0\n0\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÇ\n\n\nuser_is_subscriber\n42\n1\n0.12\n0.32\n0\n0\n0\n0\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_is_turbo\n42\n1\n0.03\n0.17\n0\n0\n0\n0\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_is_moderator\n42\n1\n0.02\n0.14\n0\n0\n0\n0\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_is_partner\n42\n1\n0.02\n0.13\n0\n0\n0\n0\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_is_subgifter\n42\n1\n0.00\n0.00\n0\n0\n0\n0\n0\n‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÅ\n\n\nuser_is_broadcaster\n42\n1\n0.00\n0.03\n0\n0\n0\n0\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_is_vip\n42\n1\n0.00\n0.03\n0\n0\n0\n0\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_is_twitchdj\n42\n1\n0.00\n0.02\n0\n0\n0\n0\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_is_founder\n42\n1\n0.00\n0.02\n0\n0\n0\n0\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_is_staff\n42\n1\n0.00\n0.01\n0\n0\n0\n0\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_is_game_dev\n42\n1\n0.00\n0.01\n0\n0\n0\n0\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_is_ambassador\n42\n1\n0.00\n0.00\n0\n0\n0\n0\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_no_audio\n42\n1\n0.02\n0.14\n0\n0\n0\n0\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_no_video\n42\n1\n0.01\n0.12\n0\n0\n0\n0\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\nVariable type: Timespan\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nmessage_timecode\n0\n1\n-59\n60\n6H 8M 49S\n17167\n\n\n\n\n\n\n\nKurzer √úberblick √ºber den transcripts-Datensatz\n\ntranscripts %&gt;% glimpse \n\nRows: 5,861\nColumns: 12\n$ id_sequence            &lt;chr&gt; \"p1_s0001\", \"p1_s0002\", \"p1_s0003\", \"p1_s0004\",‚Ä¶\n$ source                 &lt;chr&gt; \"presidential_debate-abc\", \"presidential_debate‚Ä¶\n$ speaker                &lt;chr&gt; \"S27\", \"S35\", \"S27\", \"S55\", \"S61\", \"S55\", \"S43\"‚Ä¶\n$ timestamp              &lt;time&gt; 00:00:00, 00:00:11, 00:00:20, 00:00:34, 00:00:‚Ä¶\n$ dialogue               &lt;chr&gt; \"Tonight, the high-stakes showdown here in Phil‚Ä¶\n$ dialogue_length        &lt;int&gt; 229, 148, 245, 91, 31, 13, 37, 102, 316, 409, 6‚Ä¶\n$ duration               &lt;dbl&gt; 11, 9, 14, 6, 4, 1, 4, 10, 17, 21, 28, 8, 13, 4‚Ä¶\n$ debate                 &lt;chr&gt; \"presidential\", \"presidential\", \"presidential\",‚Ä¶\n$ streamer               &lt;chr&gt; \"tv_station\", \"tv_station\", \"tv_station\", \"tv_s‚Ä¶\n$ id_streamer            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n$ id_speaker             &lt;chr&gt; \"p1_s27\", \"p1_s35\", \"p1_s27\", \"p1_s55\", \"p1_s61‚Ä¶\n$ sequence_during_debate &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n\ntranscripts %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n5861\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n7\n\n\ndifftime\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nid_sequence\n0\n1\n8\n9\n0\n5861\n0\n\n\nsource\n0\n1\n23\n44\n0\n8\n0\n\n\nspeaker\n0\n1\n3\n4\n0\n152\n0\n\n\ndialogue\n0\n1\n2\n16523\n0\n5697\n0\n\n\ndebate\n0\n1\n12\n17\n0\n2\n0\n\n\nstreamer\n0\n1\n8\n19\n0\n4\n0\n\n\nid_speaker\n0\n1\n6\n8\n0\n640\n0\n\n\n\nVariable type: difftime\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ntimestamp\n0\n1\n0 secs\n38738 secs\n03:33:08\n5433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndialogue_length\n0\n1\n460.75\n887.09\n2\n66\n163\n507\n16523\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nduration\n8\n1\n30.72\n59.80\n0\n5\n12\n33\n1079\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nid_streamer\n0\n1\n2.73\n1.11\n1\n2\n2\n4\n4\n‚ñÉ‚ñá‚ñÅ‚ñÇ‚ñá\n\n\nsequence_during_debate\n0\n1\n0.29\n0.45\n0\n0\n0\n1\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÉ\n\n\n\n\n\n\n\nArbeiten mit quanteda: corpus\n\n# Create corpus\ncorp_transcripts &lt;- transcripts %&gt;% \n  quanteda::corpus(\n    docid_field = \"id_sequence\", \n    text_field = \"dialogue\"\n  )\n\n# Output\ncorp_transcripts\n\nCorpus consisting of 5,861 documents and 10 docvars.\np1_s0001 :\n\"Tonight, the high-stakes showdown here in Philadelphia betwe...\"\n\np1_s0002 :\n\"A historic race for president upended just weeks ago, Presid...\"\n\np1_s0003 :\n\"The candidates separated by the smallest of margins, essenti...\"\n\np1_s0004 :\n\"This is an ABC News special. The most consequential moment o...\"\n\np1_s0005 :\n\"Together, we'll chart a... (..)\"\n\np1_s0006 :\n\"Donald Trump.\"\n\n[ reached max_ndoc ... 5,855 more documents ]\n\n\n\n\nEinfluss der Preporcessing-Schritte am Beispiel\n\nEinfache Tokenisierung\n\n# Tokenize corpus\ntoks_simple &lt;- corp_transcripts %&gt;% \n  quanteda::tokens() \n\n# Output\nhead(toks_simple[[1]], 100)\n\n [1] \"Tonight\"      \",\"            \"the\"          \"high-stakes\"  \"showdown\"    \n [6] \"here\"         \"in\"           \"Philadelphia\" \"between\"      \"Vice\"        \n[11] \"President\"    \"Kamala\"       \"Harris\"       \"and\"          \"former\"      \n[16] \"President\"    \"Donald\"       \"Trump\"        \".\"            \"Their\"       \n[21] \"first\"        \"face-to-face\" \"meeting\"      \"in\"           \"this\"        \n[26] \"presidential\" \"election\"     \",\"            \"their\"        \"first\"       \n[31] \"face-to-face\" \"meeting\"      \"ever\"         \".\"           \n\n\n\n\nmit Entfernung von Satz- und Sonderzeichen\n\ntoks_nopunct &lt;- corp_transcripts %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  )\n\nhead(toks_nopunct[[1]], 100)\n\n [1] \"Tonight\"      \"the\"          \"high-stakes\"  \"showdown\"     \"here\"        \n [6] \"in\"           \"Philadelphia\" \"between\"      \"Vice\"         \"President\"   \n[11] \"Kamala\"       \"Harris\"       \"and\"          \"former\"       \"President\"   \n[16] \"Donald\"       \"Trump\"        \"Their\"        \"first\"        \"face-to-face\"\n[21] \"meeting\"      \"in\"           \"this\"         \"presidential\" \"election\"    \n[26] \"their\"        \"first\"        \"face-to-face\" \"meeting\"      \"ever\"        \n\n\n\n\nund ohne Stopw√∂rter\n\ntoks_nostopw &lt;- corp_transcripts %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  ) %&gt;% \n  quanteda::tokens_remove(\n    pattern = quanteda::stopwords(\"en\")\n  )\n\nhead(toks_nostopw[[1]], 100)\n\n [1] \"Tonight\"      \"high-stakes\"  \"showdown\"     \"Philadelphia\" \"Vice\"        \n [6] \"President\"    \"Kamala\"       \"Harris\"       \"former\"       \"President\"   \n[11] \"Donald\"       \"Trump\"        \"first\"        \"face-to-face\" \"meeting\"     \n[16] \"presidential\" \"election\"     \"first\"        \"face-to-face\" \"meeting\"     \n[21] \"ever\"        \n\n\n\n\nDirekter Vergleich\n\nhead(toks_simple[[1]], 100)\n\n [1] \"Tonight\"      \",\"            \"the\"          \"high-stakes\"  \"showdown\"    \n [6] \"here\"         \"in\"           \"Philadelphia\" \"between\"      \"Vice\"        \n[11] \"President\"    \"Kamala\"       \"Harris\"       \"and\"          \"former\"      \n[16] \"President\"    \"Donald\"       \"Trump\"        \".\"            \"Their\"       \n[21] \"first\"        \"face-to-face\" \"meeting\"      \"in\"           \"this\"        \n[26] \"presidential\" \"election\"     \",\"            \"their\"        \"first\"       \n[31] \"face-to-face\" \"meeting\"      \"ever\"         \".\"           \n\nhead(toks_nopunct[[1]], 100)\n\n [1] \"Tonight\"      \"the\"          \"high-stakes\"  \"showdown\"     \"here\"        \n [6] \"in\"           \"Philadelphia\" \"between\"      \"Vice\"         \"President\"   \n[11] \"Kamala\"       \"Harris\"       \"and\"          \"former\"       \"President\"   \n[16] \"Donald\"       \"Trump\"        \"Their\"        \"first\"        \"face-to-face\"\n[21] \"meeting\"      \"in\"           \"this\"         \"presidential\" \"election\"    \n[26] \"their\"        \"first\"        \"face-to-face\" \"meeting\"      \"ever\"        \n\nhead(toks_nostopw[[1]], 100)\n\n [1] \"Tonight\"      \"high-stakes\"  \"showdown\"     \"Philadelphia\" \"Vice\"        \n [6] \"President\"    \"Kamala\"       \"Harris\"       \"former\"       \"President\"   \n[11] \"Donald\"       \"Trump\"        \"first\"        \"face-to-face\" \"meeting\"     \n[16] \"presidential\" \"election\"     \"first\"        \"face-to-face\" \"meeting\"     \n[21] \"ever\"        \n\n\n\n\n\nTokenisierung von Bi & Skipgrams\n\n# Bigrams\ntoks_nostopw %&gt;% \n  tokens_ngrams(n = 2) %&gt;% \n  .[[1]]\n\n [1] \"Tonight_high-stakes\"   \"high-stakes_showdown\"  \"showdown_Philadelphia\"\n [4] \"Philadelphia_Vice\"     \"Vice_President\"        \"President_Kamala\"     \n [7] \"Kamala_Harris\"         \"Harris_former\"         \"former_President\"     \n[10] \"President_Donald\"      \"Donald_Trump\"          \"Trump_first\"          \n[13] \"first_face-to-face\"    \"face-to-face_meeting\"  \"meeting_presidential\" \n[16] \"presidential_election\" \"election_first\"        \"first_face-to-face\"   \n[19] \"face-to-face_meeting\"  \"meeting_ever\"         \n\n\n\n# Skipgrams\ntoks_nostopw %&gt;% \n  tokens_ngrams(n = 2, skip = 0:1) %&gt;% \n  .[[1]]\n\n [1] \"Tonight_high-stakes\"       \"Tonight_showdown\"         \n [3] \"high-stakes_showdown\"      \"high-stakes_Philadelphia\" \n [5] \"showdown_Philadelphia\"     \"showdown_Vice\"            \n [7] \"Philadelphia_Vice\"         \"Philadelphia_President\"   \n [9] \"Vice_President\"            \"Vice_Kamala\"              \n[11] \"President_Kamala\"          \"President_Harris\"         \n[13] \"Kamala_Harris\"             \"Kamala_former\"            \n[15] \"Harris_former\"             \"Harris_President\"         \n[17] \"former_President\"          \"former_Donald\"            \n[19] \"President_Donald\"          \"President_Trump\"          \n[21] \"Donald_Trump\"              \"Donald_first\"             \n[23] \"Trump_first\"               \"Trump_face-to-face\"       \n[25] \"first_face-to-face\"        \"first_meeting\"            \n[27] \"face-to-face_meeting\"      \"face-to-face_presidential\"\n[29] \"meeting_presidential\"      \"meeting_election\"         \n[31] \"presidential_election\"     \"presidential_first\"       \n[33] \"election_first\"            \"election_face-to-face\"    \n[35] \"first_face-to-face\"        \"first_meeting\"            \n[37] \"face-to-face_meeting\"      \"face-to-face_ever\"        \n[39] \"meeting_ever\"             \n\n\n\n\nKollokationen f√ºr Identifkation prominenter Bigramme\n\ntoks_nostopw %&gt;% \n  quanteda.textstats::textstat_collocations(\n    size = 2, \n    min_count = 5\n  ) %&gt;% \n  head(25)\n\n        collocation count count_nested length    lambda        z\n1         know know  1337            0      2  3.890787 98.31370\n2        saying bad   558            0      2  6.503305 81.19215\n3        bad saying   553            0      2  6.481795 80.98625\n4         going say   666            0      2  4.555737 80.92246\n5         say going   661            0      2  4.562963 80.82524\n6      donald trump   755            0      2  7.422847 75.19267\n7     kamala harris   494            0      2  7.873933 68.88003\n8    vice president   429            0      2  7.258104 56.72753\n9             oh oh   229            0      2  4.679549 54.95066\n10        right now   269            0      2  3.996328 53.27167\n11    senator vance   129            0      2  6.583164 49.48173\n12       little bit   132            0      2  8.268099 45.80914\n13 president harris   186            0      2  4.027681 45.69845\n14           oh god   154            0      2  6.175930 44.94423\n15        years ago   102            0      2  6.436807 43.06424\n16         tim walz    90            0      2  7.588398 43.05596\n17  president trump   203            0      2  3.468589 42.70406\n18       four years   100            0      2  6.381600 42.53221\n19      health care   136            0      2  7.400206 41.83123\n20      white house    85            0      2  8.071701 41.52827\n21   donald trump's   132            0      2  6.408658 41.05938\n22 former president   141            0      2  5.790480 41.04486\n23  curious curious   373            0      2 11.836611 40.77202\n24    governor walz    77            0      2  6.512020 39.65499\n25      two minutes    84            0      2  6.490910 39.39074\n\n\n\n\nAnwendung der DFM\n\n# Check top 25 features\ntoks_nostopw %&gt;%\n  quanteda::dfm() %&gt;% \n  quanteda.textstats::textstat_frequency(\n    n = 25) \n\n     feature frequency rank docfreq group\n1       like      6350    1    1617   all\n2       know      3872    2    1325   all\n3      think      2932    3    1202   all\n4     people      2710    4    1158   all\n5       yeah      2551    5    1107   all\n6      going      2364    6     956   all\n7       just      2145    7    1322   all\n8        say      1632    8     715   all\n9      right      1556    9     930   all\n10     trump      1492   10     859   all\n11       one      1461   11     958   all\n12 president      1372   12     787   all\n13       get      1320   13     850   all\n14      said      1262   14     822   all\n15       now      1222   15     883   all\n16      want      1207   16     755   all\n17       can      1137   17     723   all\n18    really      1137   17     620   all\n19        uh      1134   19     421   all\n20   fucking      1074   20     522   all\n21       lot      1049   21     632   all\n22    saying      1042   22     376   all\n23        oh      1003   23     546   all\n24      well       974   24     740   all\n25       bad       963   25     251   all\n\n\n\nBeispiel f√ºr den Loop des (Pre-)Processing\n\n# Customize stopwords\ncustom_stopwords &lt;- c(\"uh\", \"oh\")\n\n# Remove custom stopwords\ntoks_no_custom_stopw &lt;- toks_nostopw %&gt;% \n  quanteda::tokens_remove(\n    pattern = custom_stopwords\n  )\n\n# Check top 25 features\ntoks_no_custom_stopw %&gt;%\n  quanteda::dfm() %&gt;% \n  quanteda.textstats::textstat_frequency(\n    n = 25) \n\n     feature frequency rank docfreq group\n1       like      6350    1    1617   all\n2       know      3872    2    1325   all\n3      think      2932    3    1202   all\n4     people      2710    4    1158   all\n5       yeah      2551    5    1107   all\n6      going      2364    6     956   all\n7       just      2145    7    1322   all\n8        say      1632    8     715   all\n9      right      1556    9     930   all\n10     trump      1492   10     859   all\n11       one      1461   11     958   all\n12 president      1372   12     787   all\n13       get      1320   13     850   all\n14      said      1262   14     822   all\n15       now      1222   15     883   all\n16      want      1207   16     755   all\n17       can      1137   17     723   all\n18    really      1137   17     620   all\n19   fucking      1074   19     522   all\n20       lot      1049   20     632   all\n21    saying      1042   21     376   all\n22      well       974   22     740   all\n23       bad       963   23     251   all\n24      mean       935   24     557   all\n25       way       905   25     572   all\n\n\n\n\n\nVerschiedene Analysen auf Basis der DFM\n\nAuswahl bestimmter Muster/Features\n\n# Create corpus\ncorp_chats &lt;- chats %&gt;% \n  quanteda::corpus(\n    docid_field = \"message_id\", \n    text_field = \"message_content\"\n  )\n\n# Create DFM\ndfm_chats &lt;- corp_chats %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  ) %&gt;% \n  quanteda::dfm() \n\n# Output\ndfm_chats %&gt;% \n  quanteda::dfm_select(pattern = \"@*\") %&gt;% \n  quanteda.textstats::textstat_frequency(\n    n = 25) \n\n            feature frequency rank docfreq group\n1         @hasanabi     29173    1   28371   all\n2        @zackrawrr     11430    2   11381   all\n3       @gizmomacks       246    3     243   all\n4         @toxicsjw       167    4     167   all\n5       @beteljuice       158    5     158   all\n6       @megaphonix       154    6     150   all\n7            @hasan        76    7      76   all\n8  @depressedaether        68    8      68   all\n9     @nicebathroom        68    8      68   all\n10    @littlebear36        64   10      61   all\n11   @david_leonard        61   11      61   all\n12     @hasandpiker        58   12      58   all\n13      @sambarty2k        58   12      58   all\n14         @tiamani        55   14      55   all\n15     @matefeedart        50   15      50   all\n16           @wihby        47   16      47   all\n17      @freejam013        44   17      44   all\n18      @austinshow        42   18      42   all\n19         @mf_jewm        41   19      41   all\n20            @chat        41   19      33   all\n21    @lakemcgroove        37   21      37   all\n22            @mhud        37   21      37   all\n23  @thistwitchname        36   23      36   all\n24     @mangobreezy        35   24      35   all\n25        @mijnboot        35   24      35   all\n\n\n\n\n\nGezielte Suche nach spezifischen Worten\n\n# Load custom emoji-dictionary\ndict_chat_emotes &lt;- readRDS(here(\"local_data/dictionary_chat_emotes.RDS\"))\n\n# Output\ndict_chat_emotes\n\nDictionary object with 5546 key entries.\n- [0Unroll]:\n  - 0unroll\n- [1]:\n  - 1\n- [2020Celebrate]:\n  - 2020celebrate\n- [2020Forward]:\n  - 2020forward\n- [2020Glitchy]:\n  - 2020glitchy\n- [2020Pajamas]:\n  - 2020pajamas\n[ reached max_nkey ... 5,540 more keys ]\n\n\n\n# Lookup emojis in DFM of chats\ndfm_emotes &lt;- dfm_chats %&gt;% \n  quanteda::dfm_lookup(\n    dictionary = dict_chat_emotes)\n\n# Output frequency of emojis\ndfm_emotes %&gt;% \n  quanteda.textstats::textstat_frequency(\n    n = 25) \n\n         feature frequency rank docfreq group\n1            LUL     20194    1   14967   all\n2           hasL     12455    2    5856   all\n3    bleedPurple      5188    3    5174   all\n4          Kappa      4971    4    4240   all\n5        hasSlam      2989    5    1002   all\n6    NotLikeThis      2341    6    1354   all\n7             üáµüá∏      1968    7     780   all\n8        hasChud      1792    8    1206   all\n9          hasHi      1401    9     851   all\n10          hasO      1375   10     609   all\n11       hasBoot      1209   11     551   all\n12       hasRaid      1092   12     470   all\n13      elbyBlom      1001   13    1001   all\n14       WutFace       964   14     687   all\n15     hasBaited       901   15     396   all\n16       hasMods       853   16     604   all\n17          Guns       755   17     728   all\n18      hasKkona       727   18     384   all\n19     DinoDance       721   19     269   all\n20       PopNemo       709   20     334   all\n21 TwitchConHYPE       630   21     236   all\n22      hasSadge       601   22     481   all\n23      has0head       599   23     301   all\n24       hasFlex       569   24     294   all\n25             e       563   25     496   all"
  },
  {
    "objectID": "tutorials/tutorial-03.html",
    "href": "tutorials/tutorial-03.html",
    "title": "üî® Working with R",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "tutorials/tutorial-03.html#background",
    "href": "tutorials/tutorial-03.html#background",
    "title": "üî® Working with R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays‚Äôs data basis: Hollywood Age Gaps\n\n\n\n\nAn informational site showing the age gap between movie love interests.\n\nThe data follows certain rules:\n\nThe two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)\nThe youngest of the two actors is at least 17 years old\nNot animated characters\n\n\n\n\nThe best way to learn R is by trying. This document tries to display a version of the ‚Äúnormal‚Äù data processing procedure.\nUse tidytuesday data as an example to showcase the potential"
  },
  {
    "objectID": "tutorials/tutorial-03.html#packages",
    "href": "tutorials/tutorial-03.html#packages",
    "title": "üî® Working with R",
    "section": "Packages",
    "text": "Packages\n\nThe pacman::p_load() package is used to load the packages, which has several advantages over the conventional method with library():\nConcise syntax\nAutomatic installation (if the package is not already installed)\nLoading multiple packages at once\nAutomatic search for dependencies\n\n\npacman::p_load(\n  here, \n  magrittr, \n  tidyverse,\n  janitor,\n  easystats,\n  sjmisc,\n  ggpubr)"
  },
  {
    "objectID": "tutorials/tutorial-03.html#codechunks-aus-der-sitzung",
    "href": "tutorials/tutorial-03.html#codechunks-aus-der-sitzung",
    "title": "üî® Working with R",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nDie erste ‚ÄúRunde‚Äù der Datenaufbereitung\n\nDatenimport via URL\n\nVariablennamen und -beschreibungen\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nmovie_name\nName of the film\n\n\nrelease_year\nRelease year\n\n\ndirector\nDirector of the film\n\n\nage_difference\nAge difference between the characters in whole years\n\n\ncouple_number\nAn identifier for the couple in case multiple couples are listed for this film\n\n\nactor_1_name\nThe name of the older actor in this couple\n\n\nactor_2_name\nThe name of the younger actor in this couple\n\n\nactor_1_birthdate\nThe birthdate of the older member of the couple\n\n\nactor_2_birthdate\nThe birthdate of the younger member of the couple\n\n\nactor_1_age\nThe age of the older actor when the film was released\n\n\nactor_2_age\nThe age of the younger actor when the film was released\n\n\n\n\n# Import data from URL\nage_gaps &lt;- read_csv(\"http://hollywoodagegap.com/movies.csv\") %&gt;% \n  janitor::clean_names()\n\n# Check data set\nage_gaps\n\n# A tibble: 1,199 √ó 12\n   movie_name   release_year director age_difference actor_1_name actor_1_gender\n   &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;         \n 1 Harold and ‚Ä¶         1971 Hal Ash‚Ä¶             52 Bud Cort     man           \n 2 Venus                2006 Roger M‚Ä¶             50 Peter O'Too‚Ä¶ man           \n 3 The Quiet A‚Ä¶         2002 Phillip‚Ä¶             49 Michael Cai‚Ä¶ man           \n 4 Solitary Man         2009 Brian K‚Ä¶             45 Michael Dou‚Ä¶ man           \n 5 The Big Leb‚Ä¶         1998 Joel Co‚Ä¶             45 David Huddl‚Ä¶ man           \n 6 Beginners            2010 Mike Mi‚Ä¶             43 Christopher‚Ä¶ man           \n 7 Poison Ivy           1992 Katt Sh‚Ä¶             42 Tom Skerritt man           \n 8 Dirty Grand‚Ä¶         2016 Dan Maz‚Ä¶             41 Robert De N‚Ä¶ man           \n 9 Whatever Wo‚Ä¶         2009 Woody A‚Ä¶             40 Larry David  man           \n10 Entrapment           1999 Jon Ami‚Ä¶             39 Sean Connery man           \n# ‚Ñπ 1,189 more rows\n# ‚Ñπ 6 more variables: actor_1_birthdate &lt;date&gt;, actor_1_age &lt;dbl&gt;,\n#   actor_2_name &lt;chr&gt;, actor_2_gender &lt;chr&gt;, actor_2_birthdate &lt;chr&gt;,\n#   actor_2_age &lt;dbl&gt;\n\n\n\n\nInitiale √úberpr√ºfung der Daten\n\n\n\n\n\n\nSind die Daten ‚Äútechnisch korrekt‚Äù?\n\n\n\n\n‚úÖ Wie viele F√§lle sind enthalten? Wie viele Variablen?\n‚úÖ Wie lauten die Variablennamen? Sind sie sinnvoll?\n‚úÖ Welchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\n‚úÖ Wie viele eindeutige Werte hat jede Variable?\n‚úÖ Welcher Wert tritt am h√§ufigsten auf, und wie oft kommt er vor?\n‚úÖ Gibt es fehlende Werte? Wenn ja, wie h√§ufig ist dies der Fall?\n\n\n\n\n√úberblick √ºber die Daten\n\nage_gaps %&gt;% glimpse()\n\nRows: 1,199\nColumns: 12\n$ movie_name        &lt;chr&gt; \"Harold and Maude\", \"Venus\", \"The Quiet American\", \"‚Ä¶\n$ release_year      &lt;dbl&gt; 1971, 2006, 2002, 2009, 1998, 2010, 1992, 2016, 2009‚Ä¶\n$ director          &lt;chr&gt; \"Hal Ashby\", \"Roger Michell\", \"Phillip Noyce\", \"Bria‚Ä¶\n$ age_difference    &lt;dbl&gt; 52, 50, 49, 45, 45, 43, 42, 41, 40, 39, 38, 38, 36, ‚Ä¶\n$ actor_1_name      &lt;chr&gt; \"Bud Cort\", \"Peter O'Toole\", \"Michael Caine\", \"Micha‚Ä¶\n$ actor_1_gender    &lt;chr&gt; \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"ma‚Ä¶\n$ actor_1_birthdate &lt;date&gt; 1948-03-29, 1932-08-02, 1933-03-14, 1944-09-25, 193‚Ä¶\n$ actor_1_age       &lt;dbl&gt; 23, 74, 69, 65, 68, 81, 59, 73, 62, 69, 57, 77, 59, ‚Ä¶\n$ actor_2_name      &lt;chr&gt; \"Ruth Gordon\", \"Jodie Whittaker\", \"Do Thi Hai Yen\", ‚Ä¶\n$ actor_2_gender    &lt;chr&gt; \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"man\", ‚Ä¶\n$ actor_2_birthdate &lt;chr&gt; \"1896-10-30\", \"1982-06-03\", \"1982-10-01\", \"1989-01-0‚Ä¶\n$ actor_2_age       &lt;dbl&gt; 75, 24, 20, 20, 23, 38, 17, 32, 22, 30, 19, 39, 23, ‚Ä¶\n\n\n\n\nKorrekturen\n\nage_gaps_correct &lt;- age_gaps %&gt;% \n  mutate(\n    across(ends_with(\"_birthdate\"), ~as.Date(.)) # set dates to dates\n  )\n\n\n\n√úberpr√ºfung Lageparameter\n\nage_gaps_correct %&gt;% descr()\n\n\n## Basic descriptive statistics\n\n            var    type          label    n NA.prc    mean    sd   se   md\n   release_year numeric   release_year 1199      0 2000.53 17.07 0.49 2004\n age_difference numeric age_difference 1199      0   10.62  8.62 0.25    8\n    actor_1_age numeric    actor_1_age 1199      0   40.07 10.93 0.32   39\n    actor_2_age numeric    actor_2_age 1199      0   31.22  8.47 0.24   30\n trimmed          range iqr  skew\n 2003.48 89 (1935-2024)  15 -1.62\n    9.55      52 (0-52)  12  1.19\n   39.51     64 (17-81)  15  0.54\n   30.38     64 (17-81)  10  1.39\n\n\n\n\n\n\nDie ersten Datenexplorationen\n\nWie sind die Altersunterschiede verteilt?\n\nage_gaps_correct %&gt;% \n    ggplot(aes(x = age_difference)) +\n    geom_bar() +\n    theme_pubr()\n\n\n\n\n\n\n\n\n\n\nIn welchen Filmen ist der Altersunterschied am h√∂chsten?\n\nage_gaps_correct %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(movie_name, age_difference, release_year) \n\n# A tibble: 1,199 √ó 3\n   movie_name         age_difference release_year\n   &lt;chr&gt;                       &lt;dbl&gt;        &lt;dbl&gt;\n 1 Harold and Maude               52         1971\n 2 Venus                          50         2006\n 3 The Quiet American             49         2002\n 4 Solitary Man                   45         2009\n 5 The Big Lebowski               45         1998\n 6 Beginners                      43         2010\n 7 Poison Ivy                     42         1992\n 8 Dirty Grandpa                  41         2016\n 9 Whatever Works                 40         2009\n10 Entrapment                     39         1999\n# ‚Ñπ 1,189 more rows\n\n\n\nage_gaps_correct %&gt;% \n    filter(release_year &gt;= 2022) %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(\n        movie_name, age_difference, release_year, \n        actor_1_name, actor_2_name) \n\n# A tibble: 19 √ó 5\n   movie_name              age_difference release_year actor_1_name actor_2_name\n   &lt;chr&gt;                            &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       \n 1 Poor Things                         21         2023 Mark Ruffalo Emma Stone  \n 2 The Bubble                          21         2022 Pedro Pascal Maria Bakal‚Ä¶\n 3 Oppenheimer                         20         2023 Cillian Mur‚Ä¶ Florence Pu‚Ä¶\n 4 The Northman                        20         2022 Alexander S‚Ä¶ Anya Taylor‚Ä¶\n 5 Spaceman                            19         2024 Adam Sandler Carey Mulli‚Ä¶\n 6 The Lost City                       16         2022 Channing Ta‚Ä¶ Sandra Bull‚Ä¶\n 7 We Live in Time                     13         2024 Andrew Garf‚Ä¶ Florence Pu‚Ä¶\n 8 The Idea of You                     12         2024 Nicholas Ga‚Ä¶ Anne Hathaw‚Ä¶\n 9 Barbie                              10         2023 Ryan Gosling Margot Robb‚Ä¶\n10 Twisters                            10         2024 Glen Powell  Daisy Edgar‚Ä¶\n11 Anyone but You                       9         2023 Glen Powell  Sydney Swee‚Ä¶\n12 Everything Everywhere ‚Ä¶              9         2022 Ke Huy Quan  Michelle Ye‚Ä¶\n13 Top Gun: Maverick                    8         2022 Tom Cruise   Jennifer Co‚Ä¶\n14 Oppenheimer                          7         2023 Cillian Mur‚Ä¶ Emily Blunt \n15 Your Place or Mine                   7         2023 Ashton Kutc‚Ä¶ Zo√´ Chao    \n16 Your Place or Mine                   5         2023 Jesse Willi‚Ä¶ Reese Withe‚Ä¶\n17 Poor Things                          2         2023 Christopher‚Ä¶ Emma Stone  \n18 Your Place or Mine                   2         2023 Ashton Kutc‚Ä¶ Reese Withe‚Ä¶\n19 You People                           1         2023 Jonah Hill   Lauren Lond‚Ä¶\n\n\n\n\nGibt es einen Zusammenhang zwischen Altersunterschied und Release?\n\n(Durchschnitts-)Unterschied nach Jahren\n\nage_gaps_correct %&gt;% \n    group_by(release_year) %&gt;% \n    summarise(age_difference_mean = mean(age_difference)) %&gt;% \n    ggplot(aes(release_year, age_difference_mean)) +\n    geom_col() +\n    theme_pubr()\n\n\n\n\n\n\n\n\n\n\nVerteilung nach Jahren\n\nggpubr::ggboxplot(\n    data = age_gaps_correct, \n    x = \"release_year\", \n    y = \"age_difference\", \n  ) + \n   # Rotate x-axis labels by 90 degrees\n  theme(\n    axis.text.x = element_text(\n        angle = 90,\n        vjust = 0.5,\n         hjust=1))  \n\n\n\n\n\n\n\n\n\n\n√úberpr√ºfung der Korrelation\n\nage_gaps %&gt;%\n  select(release_year, age_difference) %&gt;% \n  correlation::correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1   |     Parameter2 |     r |         95% CI | t(1197) |         p\n----------------------------------------------------------------------------\nrelease_year | age_difference | -0.22 | [-0.27, -0.17] |   -7.83 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 1199\n\n\n\n\nSch√§tzung OLS\n\n# Sch√§tzung des Models\nmdl &lt;- lm(age_difference ~ release_year, data = age_gaps_correct)\n\n# Output\nmdl %&gt;% parameters::parameters()\n\nParameter    | Coefficient |    SE |           95% CI | t(1197) |      p\n------------------------------------------------------------------------\n(Intercept)  |      233.69 | 28.48 | [177.82, 289.57] |    8.21 | &lt; .001\nrelease year |       -0.11 |  0.01 | [ -0.14,  -0.08] |   -7.83 | &lt; .001\n\nmdl %&gt;% performance::model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n8512.891 | 8512.911 | 8528.159 | 0.049 |     0.048 | 8.403 | 8.410\n\n\n\nmdl %&gt;% report::report()\n\nWe fitted a linear model (estimated using OLS) to predict age_difference with\nrelease_year (formula: age_difference ~ release_year). The model explains a\nstatistically significant and weak proportion of variance (R2 = 0.05, F(1,\n1197) = 61.35, p &lt; .001, adj. R2 = 0.05). The model's intercept, corresponding\nto release_year = 0, is at 233.69 (95% CI [177.82, 289.57], t(1197) = 8.21, p &lt;\n.001). Within this model:\n\n  - The effect of release year is statistically significant and negative (beta =\n-0.11, 95% CI [-0.14, -0.08], t(1197) = -7.83, p &lt; .001; Std. beta = -0.22, 95%\nCI [-0.28, -0.17])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "exercises/exercise-07.html",
    "href": "exercises/exercise-07.html",
    "title": "Twitch Chat Analysis",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "exercises/exercise-07.html#background",
    "href": "exercises/exercise-07.html#background",
    "title": "Twitch Chat Analysis",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays‚Äôs data basis: Twitch Chat & Transcripts\n\n\n\nTranscripts & Chats of the Live-Streams from  hasanabi and  zackrawrr and | TheMajorityReport for the Presidential (Harris vs.¬†Trump) and Vice-Presidential (Vance vs.¬†Walz) Debates 2024\n\n\n\nThe best way to learn R is by trying. This document tries to display a version of the ‚Äúnormal‚Äù data processing procedure.\nUse tidytuesday data as an example to showcase the potential"
  },
  {
    "objectID": "exercises/exercise-07.html#preparation",
    "href": "exercises/exercise-07.html#preparation",
    "title": "Twitch Chat Analysis",
    "section": "Preparation",
    "text": "Preparation\n\nPackages\nThe pacman::p_load() function from the pacman package is used to load the packages, which has several advantages over the conventional method with library():\n\nConcise syntax\nAutomatic installation (if the package is not already installed)\nLoading multiple packages at once\nAutomatic search for dependencies\n\n\npacman::p_load(\n    here, taylor,\n    magrittr, janitor,\n    ggpubr, \n    gt, gtExtras,\n    countdown, \n    quanteda, # quanteda text processing\n    quanteda.textplots, \n    easystats, tidyverse\n)\n\n\n\nImport und Vorverarbeitung der Daten\n\nchats &lt;- qs::qread(here(\"local_data/chat-debates_full.qs\"))$correct\ntranscripts &lt;- qs::qread(here(\"local_data/transcripts-debates_full.qs\"))$correct"
  },
  {
    "objectID": "exercises/exercise-07.html#praktische-√ºbung",
    "href": "exercises/exercise-07.html#praktische-√ºbung",
    "title": "Twitch Chat Analysis",
    "section": "üõ†Ô∏è Praktische √úbung",
    "text": "üõ†Ô∏è Praktische √úbung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden üìã Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation ausgef√ºhrt haben. Das k√∂nnen Sie tun, indem Sie den ‚ÄúRun all chunks above‚Äù-Knopf  des n√§chsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in das Tutorial (.qmd oder .html). Beim Tutorial handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Tutorial also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden."
  },
  {
    "objectID": "exercises/exercise-07.html#kennenlernen-des-chat-datensatzes",
    "href": "exercises/exercise-07.html#kennenlernen-des-chat-datensatzes",
    "title": "Twitch Chat Analysis",
    "section": "üîé Kennenlernen des Chat-Datensatzes",
    "text": "üîé Kennenlernen des Chat-Datensatzes\n\nüìã Exercise 1: Create corpus\n\nCreate new dataset corp_chats\n\nBased on the dataset chats, create a corpus object with the quanteda package.\nUse the corpus() function with the docid_field argument set to ‚Äúmessage_id‚Äù and the text_field argument set to ‚Äúmessage_content‚Äù.\nCheck if the transformation was successful by using the summary() function.\n\n\n\n# INSERT CODE HERE\n\n\n\nüìã Exercise 2: Tokenization & DFM conversion\n\nCreate new datasets toks_chats & dfm_chats\n\n\nBased on the dataset corp_chats, create tokens using the tokens() function from the quanteda package.\nConvert the tokens to a document-feature matrix (DFM) using the dfm() function from the quanteda package.\nCheck if the transformations were successful (e.g.¬†by using the print() function).\n\n\n# INSERT CODE HERE\n\n\n\nüìã Exercise 3: Analyse DFM\n\nBased on dfm_chats\n\nUse the textstat_frequency() function from the quanteda package to get the top 50 tokens.\nDisplay the results.\n\nBased on the results, what preprocessing steps could be useful?\n\n\n# INSERT CODE HERE\n\n\n\nüìã Exercise 4: Preprocessing\n\nCreate a new dataset dfm_chats_preprocessed\n\nBased on corp_chats, preprocess the data according to the steps you think are necessary (e.g.¬†removing punctuation, symbols, numbers, URLs, and stopwords).\nDepending on the steps you choose, you might need to use the tokens_remove() function from the quanteda package.\nCreate a new DFM object dfm_chats_preprocessed.\nUse the textstat_frequency() function from the quanteda package on the newly created dataset to get the top 50 tokens and compare the result with the results of Exercise 3.\n\n\n\n# INSERT CODE HERE"
  },
  {
    "objectID": "exercises/exercise-08_solution.html",
    "href": "exercises/exercise-08_solution.html",
    "title": "Advanced Twitch Chat Analysis",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "exercises/exercise-08_solution.html#background",
    "href": "exercises/exercise-08_solution.html#background",
    "title": "Advanced Twitch Chat Analysis",
    "section": "Background",
    "text": "Background"
  },
  {
    "objectID": "exercises/exercise-08_solution.html#todayss-data-basis-advanced-text-analysis",
    "href": "exercises/exercise-08_solution.html#todayss-data-basis-advanced-text-analysis",
    "title": "Advanced Twitch Chat Analysis",
    "section": "Todays‚Äôs data basis: Advanced Text Analysis",
    "text": "Todays‚Äôs data basis: Advanced Text Analysis\nTranscripts & Chats of the Live-Streams from  hasanabi and  zackrawrr and | TheMajorityReport for the Presidential (Harris vs.¬†Trump) and Vice-Presidential (Vance vs.¬†Walz) Debates 2024"
  },
  {
    "objectID": "exercises/exercise-08_solution.html#preparation",
    "href": "exercises/exercise-08_solution.html#preparation",
    "title": "Advanced Twitch Chat Analysis",
    "section": "Preparation",
    "text": "Preparation\n\nPackages\nThe pacman::p_load() function from the pacman package is used to load the packages, which has several advantages over the conventional method with library():\n\nConcise syntax\nAutomatic installation (if the package is not already installed)\nLoading multiple packages at once\nAutomatic search for dependencies\n\n\npacman::p_load(\n    here, taylor,\n    magrittr, janitor,\n    ggpubr, \n    gt, gtExtras,\n    countdown, \n    quanteda, # quanteda text processing\n    quanteda.textplots, \n    easystats, tidyverse\n)\n\n\n\nImport und Vorverarbeitung der Daten\n\nchats &lt;- qs::qread(here(\"local_data/chats.qs\"))\ntranscripts &lt;- qs::qread(here(\"local_data/transcripts.qs\"))\nchats_spacyr &lt;- qs::qread(here(\"local_data/chat-corpus_spacyr.qs\"))"
  },
  {
    "objectID": "exercises/exercise-08_solution.html#praktische-√ºbung",
    "href": "exercises/exercise-08_solution.html#praktische-√ºbung",
    "title": "Advanced Twitch Chat Analysis",
    "section": "üõ†Ô∏è Praktische √úbung",
    "text": "üõ†Ô∏è Praktische √úbung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden üìã Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation ausgef√ºhrt haben. Das k√∂nnen Sie tun, indem Sie den ‚ÄúRun all chunks above‚Äù-Knopf  des n√§chsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in das Tutorial (.qmd oder .html). Beim Tutorial handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Tutorial also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden."
  },
  {
    "objectID": "exercises/exercise-08_solution.html#kennenlernen-des-chat-datensatzes",
    "href": "exercises/exercise-08_solution.html#kennenlernen-des-chat-datensatzes",
    "title": "Advanced Twitch Chat Analysis",
    "section": "üîé Kennenlernen des Chat-Datensatzes",
    "text": "üîé Kennenlernen des Chat-Datensatzes\n\nüìã Exercise 1: Create corpus, token & DFM\n\nCreate new dataset corp_chats\n\nBased on the dataset chats, create a corpus object with the quanteda package.\nUse the corpus() function with the docid_field argument set to ‚Äúmessage_id‚Äù and the text_field argument set to ‚Äúmessage_content‚Äù.\n\nCreate new dataset toks_chats\n\nBased on the dataset corp_chats, create tokens using the tokens() function from the quanteda package, including the removal of punctuation, symbols, numbers, URLs, and stopwords.\nUse the tokens_remove() function to remove stopwords (en).\n\nCreate new dataset dfm_chats\n\nConvert the tokens to a document-feature matrix (DFM) using the dfm() function from the quanteda package.\n\n\n\n# Create corpus \ncorp_chats &lt;- chats %&gt;% \n  quanteda::corpus(\n    docid_field = \"message_id\", \n    text_field = \"message_content\"\n  )\n\n# Create tokens\ntoks_chats &lt;- corp_chats %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  ) %&gt;% \n  quanteda::tokens_remove(\n    pattern = quanteda::stopwords(\"en\")\n  ) \n\n# Create DFM\ndfm_chats &lt;- toks_chats %&gt;% \n    quanteda::dfm()\n\n\n\nüìã Exercise 2: Semantisches Netzwerk\n\nCreate a semantic network based on the top 50 tokens from dfm_chats.\n\nBased on dfm_chats, create an object called top_50_tokens by using the topfeatures() & names() function from the quanteda package to get the top 50 tokens.\nBased on dfm_chats, create a feature co-occurrence matrix (FCM) using the fcm() function from the quanteda package.\nSelect the top 50 tokens from the FCM using the fcm_select() function.\nCreate a network plot using the textplot_network() function from the quanteda package.\n\n\n\ntop50_tokens &lt;- dfm_chats %&gt;% \n    topfeatures(n = 50) %&gt;% \n    names()\n\ndfm_chats %&gt;% \n    fcm() %&gt;% \n    fcm_select(pattern = top50_tokens) %&gt;% \n    textplot_network()\n\n\n\nüìã Exercise 3: Analyse auf Basis von POS-Tagging\n\nBased on chats_spacyr, analyse the adjectives associated with Trump.\n\nFilter the dataset by using filter() and the arguments pos == \"NOUN\" and lemma == \"trump\".\nJoin the dataset with itself by using inner_join() and the arguments doc_id, sentence_id, and relationship = \"many-to-many\".\nFilter the dataset again for adjectives with the head token id equal to the token id of the noun. To do that, use filter() and the arguments pos.y == \"ADJ\" and head_token_id.y == token_id.x.\nRename the columns and select the relevant columns.\nDisplay the results using the sjmisc::frq() function.\n\n\n\nchats_spacyr %&gt;% \n    filter(\n      pos == \"NOUN\" &\n      lemma == \"trump\") %&gt;%\n    inner_join(\n      chats_spacyr,\n      by = c(\n        \"doc_id\",\n        \"sentence_id\"),\n      relationship = \n        \"many-to-many\") %&gt;% \n    filter(\n      pos.y == \"ADJ\" &\n      head_token_id.y == token_id.x) %&gt;% \n    rename(\n      token_id = token_id.y,\n      token = token.y) %&gt;% \n    select(\n      doc_id, sentence_id,\n      token_id, token) %&gt;%\n    sjmisc::frq(token, sort.frq = \"desc\") \n\n\n\nüìã Exercise 6: Named Entity Recognition (NER)\n\nAnalyse the named entities in the chat data.\n\nBased on chats_spacyr, use the frq() function from the sjmisc package to get the frequency of named entities.\nAgain based on chats_spacyr, filter the dataset for named entities of that indicate a person is mentioned (by using filter and the varialbe entity). Use the output of the previous step to identify the correct entity. Additionally, base all further analysis only on nouns, by using filter and the variable pos== \"NOUN.\nUse the frq() function from the sjmisc package to get the frequency. To avoid display errors, use the min.frq = 10 argument to only display tokens with a frequency of at least 10.\n\n\n\n# Identify named entities\nchats_spacyr %&gt;% \n    sjmisc::frq(entity, sort.frq = \"desc\")\n\n# Analyse named entities\nchats_spacyr %&gt;% \n    filter(entity == \"PERSON_B\") %&gt;%\n    filter(pos == \"NOUN\") %&gt;% \n    sjmisc::frq(token, sort.frq = \"desc\", min.frq = 10)"
  },
  {
    "objectID": "exercises/exercise-08.html",
    "href": "exercises/exercise-08.html",
    "title": "Advanced Twitch Chat Analysis",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "exercises/exercise-08.html#background",
    "href": "exercises/exercise-08.html#background",
    "title": "Advanced Twitch Chat Analysis",
    "section": "Background",
    "text": "Background"
  },
  {
    "objectID": "exercises/exercise-08.html#todayss-data-basis-advanced-text-analysis",
    "href": "exercises/exercise-08.html#todayss-data-basis-advanced-text-analysis",
    "title": "Advanced Twitch Chat Analysis",
    "section": "Todays‚Äôs data basis: Advanced Text Analysis",
    "text": "Todays‚Äôs data basis: Advanced Text Analysis\nTranscripts & Chats of the Live-Streams from  hasanabi and  zackrawrr and | TheMajorityReport for the Presidential (Harris vs.¬†Trump) and Vice-Presidential (Vance vs.¬†Walz) Debates 2024"
  },
  {
    "objectID": "exercises/exercise-08.html#preparation",
    "href": "exercises/exercise-08.html#preparation",
    "title": "Advanced Twitch Chat Analysis",
    "section": "Preparation",
    "text": "Preparation\n\nPackages\nThe pacman::p_load() function from the pacman package is used to load the packages, which has several advantages over the conventional method with library():\n\nConcise syntax\nAutomatic installation (if the package is not already installed)\nLoading multiple packages at once\nAutomatic search for dependencies\n\n\npacman::p_load(\n    here, taylor,\n    magrittr, janitor,\n    ggpubr, \n    gt, gtExtras,\n    countdown, \n    quanteda, # quanteda text processing\n    quanteda.textplots, \n    easystats, tidyverse\n)\n\n\n\nImport und Vorverarbeitung der Daten\n\nchats &lt;- qs::qread(here(\"local_data/chats.qs\"))\ntranscripts &lt;- qs::qread(here(\"local_data/transcripts.qs\"))\nchats_spacyr &lt;- qs::qread(here(\"local_data/chat-corpus_spacyr.qs\"))"
  },
  {
    "objectID": "exercises/exercise-08.html#praktische-√ºbung",
    "href": "exercises/exercise-08.html#praktische-√ºbung",
    "title": "Advanced Twitch Chat Analysis",
    "section": "üõ†Ô∏è Praktische √úbung",
    "text": "üõ†Ô∏è Praktische √úbung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden üìã Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation ausgef√ºhrt haben. Das k√∂nnen Sie tun, indem Sie den ‚ÄúRun all chunks above‚Äù-Knopf  des n√§chsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in das Tutorial (.qmd oder .html). Beim Tutorial handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Tutorial also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden."
  },
  {
    "objectID": "exercises/exercise-08.html#kennenlernen-des-chat-datensatzes",
    "href": "exercises/exercise-08.html#kennenlernen-des-chat-datensatzes",
    "title": "Advanced Twitch Chat Analysis",
    "section": "üîé Kennenlernen des Chat-Datensatzes",
    "text": "üîé Kennenlernen des Chat-Datensatzes\n\nüìã Exercise 1: Create corpus, token & DFM\n\nCreate new dataset corp_chats\n\nBased on the dataset chats, create a corpus object with the quanteda package.\nUse the corpus() function with the docid_field argument set to ‚Äúmessage_id‚Äù and the text_field argument set to ‚Äúmessage_content‚Äù.\n\nCreate new dataset toks_chats\n\nBased on the dataset corp_chats, create tokens using the tokens() function from the quanteda package, including the removal of punctuation, symbols, numbers, URLs, and stopwords.\nUse the tokens_remove() function to remove stopwords (en).\n\nCreate new dataset dfm_chats\n\nConvert the tokens to a document-feature matrix (DFM) using the dfm() function from the quanteda package.\n\n\n\n# INSERT CODE HERE\n\n\n\nüìã Exercise 2: Semantisches Netzwerk\n\nCreate a semantic network based on the top 50 tokens from dfm_chats.\n\nBased on dfm_chats, create an object called top_50_tokens by using the topfeatures() & names() function from the quanteda package to get the top 50 tokens.\nBased on dfm_chats, create a feature co-occurrence matrix (FCM) using the fcm() function from the quanteda package.\nSelect the top 50 tokens from the FCM using the fcm_select() function.\nCreate a network plot using the textplot_network() function from the quanteda package.\n\n\n\n# INSERT CODE HERE\n\n\n\nüìã Exercise 3: Analyse auf Basis von POS-Tagging\n\nBased on chats_spacyr, analyse the adjectives associated with Trump.\n\nFilter the dataset by using filter() and the arguments pos == \"NOUN\" and lemma == \"trump\".\nJoin the dataset with itself by using inner_join() and the arguments doc_id, sentence_id, and relationship = \"many-to-many\".\nFilter the dataset again for adjectives with the head token id equal to the token id of the noun. To do that, use filter() and the arguments pos.y == \"ADJ\" and head_token_id.y == token_id.x.\nRename the columns and select the relevant columns.\nDisplay the results using the sjmisc::frq() function.\n\n\n\n# INSERT CODE HERE\n\n\n\nüìã Exercise 6: Named Entity Recognition (NER)\n\nAnalyse the named entities in the chat data.\n\nBased on chats_spacyr, use the frq() function from the sjmisc package to get the frequency of named entities.\nAgain based on chats_spacyr, filter the dataset for named entities of that indicate a person is mentioned (by using filter and the varialbe entity). Use the output of the previous step to identify the correct entity. Additionally, base all further analysis only on nouns, by using filter and the variable pos== \"NOUN.\nUse the frq() function from the sjmisc package to get the frequency. To avoid display errors, use the min.frq = 10 argument to only display tokens with a frequency of at least 10.\n\n\n\n# INSERT CODE HERE"
  },
  {
    "objectID": "exercises/exercise-03_solution.html",
    "href": "exercises/exercise-03_solution.html",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "",
    "text": "Link to slides\n Download source file"
  },
  {
    "objectID": "exercises/exercise-03_solution.html#background",
    "href": "exercises/exercise-03_solution.html#background",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays‚Äôs data basis: Hollywood Age Gaps\n\n\n\n\nAn informational site showing the age gap between movie love interests.\n\nThe data follows certain rules:\n\nThe two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)\nThe youngest of the two actors is at least 17 years old\nNot animated characters\n\n\n\n\nThe best way to learn R is by trying. This document tries to display a version of the ‚Äúnormal‚Äù data processing procedure.\nUse tidytuesday data as an example to showcase the potential"
  },
  {
    "objectID": "exercises/exercise-03_solution.html#preparation",
    "href": "exercises/exercise-03_solution.html#preparation",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "Preparation",
    "text": "Preparation\n\nPackages\nThe pacman::p_load() package is used to load the packages, which has several advantages over the conventional method with library():\n\nConcise syntax\nAutomatic installation (if the package is not already installed)\nLoading multiple packages at once\nAutomatic search for dependencies\n\n\npacman::p_load(\n  here, \n  magrittr, \n  tidyverse,\n  janitor,\n  easystats,\n  sjmisc,\n  ggpubr)\n\n\n\nImport und Vorverarbeitung der Daten\n\nVariablennamen und -beschreibungen\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nmovie_name\nName of the film\n\n\nrelease_year\nRelease year\n\n\ndirector\nDirector of the film\n\n\nage_difference\nAge difference between the characters in whole years\n\n\ncouple_number\nAn identifier for the couple in case multiple couples are listed for this film\n\n\nactor_1_name\nThe name of the older actor in this couple\n\n\nactor_2_name\nThe name of the younger actor in this couple\n\n\nactor_1_birthdate\nThe birthdate of the older member of the couple\n\n\nactor_2_birthdate\nThe birthdate of the younger member of the couple\n\n\nactor_1_age\nThe age of the older actor when the film was released\n\n\nactor_2_age\nThe age of the younger actor when the film was released\n\n\n\n\n# Import data from URL\nage_gaps &lt;- read_csv(\"http://hollywoodagegap.com/movies.csv\") %&gt;% \n  janitor::clean_names()\n\n\n# Correct data\nage_gaps_correct &lt;- age_gaps %&gt;% \n  mutate(\n    across(ends_with(\"_birthdate\"), ~as.Date(.)) # set dates to dates\n  )"
  },
  {
    "objectID": "exercises/exercise-03_solution.html#praktische-√ºbung",
    "href": "exercises/exercise-03_solution.html#praktische-√ºbung",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "üõ†Ô∏è Praktische √úbung",
    "text": "üõ†Ô∏è Praktische √úbung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden üìã Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation ausgef√ºhrt haben. Das k√∂nnen Sie tun, indem Sie den ‚ÄúRun all chunks above‚Äù-Knopf  des n√§chsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in das Tutorial (.qmd oder .html). Beim Tutorial handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Tutorial also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\nüîé Welche Rolle spielt das Geschlecht?\n\n\n\n\n\n\nSpielt das Geschlecht eine Rolle?\n\n\n\n\nDer folgende Abschitt befasst sich nun erg√§nzend mit der Frage, welche Rolle das Geschlecht mit Blick auf die ‚ÄúG√ºltigkeit‚Äù der vorherigen Ergebnisse spielt\nDazu sind jedoch weitere Explorations- und √úberarbeitungsschritte notwendig\n\n\n\n\n\nüìã Exercise 1: √úbepr√ºfung der _gender-Variablen\n\n\n\n\n\n\nArbeitsauftrag 1.1\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_correct die Variablen actor_1_gender und actor_2_gender an.\n\n\n\nage_gaps_correct %&gt;% \n  sjmisc::frq(actor_1_gender, actor_2_gender)\n\nactor_1_gender &lt;character&gt; \n# total N=1199 valid N=1199 mean=1.01 sd=0.11\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\nman   | 1184 | 98.75 |   98.75 |  98.75\nwoman |   15 |  1.25 |    1.25 | 100.00\n&lt;NA&gt;  |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nactor_2_gender &lt;character&gt; \n# total N=1199 valid N=1199 mean=1.99 sd=0.11\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\nman   |   16 |  1.33 |    1.33 |   1.33\nwoman | 1183 | 98.67 |   98.67 | 100.00\n&lt;NA&gt;  |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n\n\n\n\n\nArbeitsauftrag 1.2\n\n\n\nNutzen Sie die Funktion sjmisc::flat_talbe() und den Datensatz age_gaps_correct um eine Kreuztabelle der Variablen actor_1_gender und actor_2_gender zu erstellen.\n\n\n\nage_gaps_correct %&gt;% \n  select(actor_1_gender, actor_2_gender) %&gt;% \n  sjmisc::flat_table()\n\n               actor_2_gender  man woman\nactor_1_gender                          \nman                             12  1172\nwoman                            4    11\n\n\n\n\nüîé Sind die Daten ‚Äúkonsistent‚Äù?\n\n√úberpr√ºfung der Sortierung\n\nage_gaps_correct %&gt;% \n  summarise(\n      p1_older = mean(actor_1_age &gt; actor_2_age), # older person first?\n      p1_male  = mean(actor_1_gender == \"man\"),  # male person first? \n      p_1_first_alpha = mean(actor_1_name &lt; actor_2_name) # alphabetical order?\n  )\n\n# A tibble: 1 √ó 3\n  p1_older p1_male p_1_first_alpha\n     &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;\n1    0.813   0.987           0.495\n\n\n\n\n\n√úberpr√ºfung der Anzahl pro Paare pro Film\n\n# Create data\ncouples &lt;- age_gaps_correct %&gt;% \n  group_by(movie_name) %&gt;% \n  summarise(n = n()) \n\n# Distribution\ncouples %&gt;% frq(n)\n\nn &lt;integer&gt; \n# total N=864 valid N=864 mean=1.39 sd=0.75\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    1 | 629 | 72.80 |   72.80 |  72.80\n    2 | 162 | 18.75 |   18.75 |  91.55\n    3 |  54 |  6.25 |    6.25 |  97.80\n    4 |  14 |  1.62 |    1.62 |  99.42\n    5 |   3 |  0.35 |    0.35 |  99.77\n    6 |   1 |  0.12 |    0.12 |  99.88\n    7 |   1 |  0.12 |    0.12 | 100.00\n &lt;NA&gt; |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n# Movies with a loot of couples \ncouples %&gt;% \n  filter(n &gt; 3) %&gt;% \n  arrange(desc(n))\n\n# A tibble: 19 √ó 2\n   movie_name                      n\n   &lt;chr&gt;                       &lt;int&gt;\n 1 Love Actually                   7\n 2 The Family Stone                6\n 3 A View to a Kill                5\n 4 He's Just Not That Into You     5\n 5 Mona Lisa Smile                 5\n 6 A Star Is Born                  4\n 7 American Pie                    4\n 8 Boogie Nights                   4\n 9 Book Club                       4\n10 Closer                          4\n11 Pushing Tin                     4\n12 Sex and the City                4\n13 Soul Food                       4\n14 Tag                             4\n15 The Favourite                   4\n16 The Girl on the Train           4\n17 The Other Woman                 4\n18 Tomorrow Never Dies             4\n19 Twilight                        4\n\n\n\n\nKorrekturen\n\nage_gaps_consistent &lt;- age_gaps_correct %&gt;% \n  # If multiple couples, assign couple number by movie\n  mutate(\n      couple_number = row_number(),\n      .by = \"movie_name\"\n  ) %&gt;% \n  # Change data structure (one line per actor in a coulpe of a movie)\n  pivot_longer(\n    cols = starts_with(c(\"actor_1_\", \"actor_2_\")),\n    names_to = c(NA, NA, \".value\"),\n    names_sep = \"_\"\n  ) %&gt;% \n  # Put older actor first\n  arrange(desc(age_difference), movie_name, birthdate) %&gt;% \n    mutate(\n    position = row_number(),\n    .by = c(\"movie_name\", \"couple_number\")\n  ) %&gt;% \n  pivot_wider(\n    names_from = \"position\",\n    names_glue = \"actor_{position}_{.value}\",\n    values_from = c(\"name\", \"gender\", \"birthdate\", \"age\")\n  ) %&gt;% \n  mutate(\n    couple_structure = case_when(\n      actor_1_gender == \"woman\" & actor_2_gender == \"woman\" ~ 1,\n      actor_1_gender == \"man\" & actor_2_gender == \"man\" ~ 2,\n      actor_1_gender != \"man\" ~ 3, \n      actor_1_gender == \"man\" ~ 4,\n    ),\n    older_male_hetero  = sjmisc::rec(\n      couple_structure, \n      rec=\"3=0; 4=1; ELSE=NA\", \n      to.factor = TRUE\n    )\n  )\n\n\n\nüîé Die zweite Datenexploration\n\n\nüìã Exercise 2: Alterskombinationen im √úberblick\n\n\n\n\n\n\nArbeitauftrag 2\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_consistent die Variablen couple_structure und older_male_hetero an.\n\n\n\nage_gaps_consistent %&gt;% \n  frq(couple_structure, older_male_hetero)\n\ncouple_structure &lt;numeric&gt; \n# total N=1199 valid N=1199 mean=3.78 sd=0.50\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    1 |  11 |  0.92 |    0.92 |   0.92\n    2 |  12 |  1.00 |    1.00 |   1.92\n    3 | 210 | 17.51 |   17.51 |  19.43\n    4 | 966 | 80.57 |   80.57 | 100.00\n &lt;NA&gt; |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nolder_male_hetero &lt;categorical&gt; \n# total N=1199 valid N=1176 mean=0.82 sd=0.38\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    0 | 210 | 17.51 |   17.86 |  17.86\n    1 | 966 | 80.57 |   82.14 | 100.00\n &lt;NA&gt; |  23 |  1.92 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n\nüìã Exercise 3: Wie sind die Altersunterschiede unterteilt, unter Ber√ºcksichtiung des Geschlechts?\n\n\n\n\n\n\nArbeitsauftrag 3.1 (graphische √úberpr√ºfung)\n\n\n\n\nErstellen Sie, auf Basis des Datensatzes age_gaps_consistent, einen ggplot.\nNutzen Sie im Argument aes() die Variable age_difference als x-Variable und older_male_hetero f√ºr das Argument fill.\nNutzen Sie geom_bar zur Erzeugung des Plots.\nOptional: Verwenden Sie theme_pubr\n\n\n\n\n# Simple \nage_gaps_consistent %&gt;% \n  ggplot(aes(age_difference, fill = older_male_hetero)) +\n  geom_bar() +\n  theme_pubr()\n\n\n\n\n\n\n\n# Detailed\nage_gaps_consistent %&gt;% \n  ggplot(aes(age_difference, fill = older_male_hetero)) +\n  geom_bar() +\n  labs(\n    x = \"Altersdifferenz (in Jahren)\",\n    y = 'Anzahl der \"Beziehungen\"'\n  ) +\n   scale_fill_manual(\n    name = \"Older partner in couple\",\n    values = c(\"0\" = \"#F8766D\", \"1\" = \"#00BFC4\", \"NA\" = \"grey\"),\n    labels = c(\"0\" = \"Woman\", \"1\" = \"Man\", \"NA\" = \"Same sex couples\")\n  ) +\n  theme_pubr() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArbeitsauftrag 3.2 (√úberpr√ºfung durch Modellierung)\n\n\n\n\nErstellen Sie ein lineares Modell (lm), das die Variable age_difference als abh√§ngige Variable und die Variablen release_year und older_male_hetero als unabh√§ngige Variablen verwendet. Nutzen Sie dazu den Datensatz age_gaps_consistent.\nGeben Sie die Parameter des Modells mit der Funktion parameters::parameters() aus.\nBewerten Sie die Modellleistung mit der Funktion performance::model_performance().\nErstellen Sie einen Bericht √ºber das Modell mit der Funktion report::report().\n\n\n\n\nmdl &lt;- lm(age_difference ~ release_year + older_male_hetero, data = age_gaps_consistent)\n\n# Output\nmdl %&gt;% parameters::parameters()\n\nParameter             | Coefficient |    SE |           95% CI | t(1173) |      p\n---------------------------------------------------------------------------------\n(Intercept)           |      202.27 | 27.53 | [148.25, 256.29] |    7.35 | &lt; .001\nrelease year          |       -0.10 |  0.01 | [ -0.13,  -0.07] |   -7.16 | &lt; .001\nolder male hetero [1] |        6.11 |  0.61 | [  4.90,   7.32] |    9.94 | &lt; .001\n\nmdl %&gt;% performance::model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n8236.693 | 8236.727 | 8256.972 | 0.127 |     0.125 | 8.002 | 8.012\n\nmdl %&gt;% report::report()\n\nWe fitted a linear model (estimated using OLS) to predict age_difference with\nrelease_year and older_male_hetero (formula: age_difference ~ release_year +\nolder_male_hetero). The model explains a statistically significant and weak\nproportion of variance (R2 = 0.13, F(2, 1173) = 85.10, p &lt; .001, adj. R2 =\n0.13). The model's intercept, corresponding to release_year = 0 and\nolder_male_hetero = 0, is at 202.27 (95% CI [148.25, 256.29], t(1173) = 7.35, p\n&lt; .001). Within this model:\n\n  - The effect of release year is statistically significant and negative (beta =\n-0.10, 95% CI [-0.13, -0.07], t(1173) = -7.16, p &lt; .001; Std. beta = -0.20, 95%\nCI [-0.25, -0.14])\n  - The effect of older male hetero [1] is statistically significant and positive\n(beta = 6.11, 95% CI [4.90, 7.32], t(1173) = 9.94, p &lt; .001; Std. beta = 0.71,\n95% CI [0.57, 0.85])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "sessions/session-09.html",
    "href": "sessions/session-09.html",
    "title": "Session 9",
    "section": "",
    "text": "üñ•Ô∏è Session 09"
  },
  {
    "objectID": "sessions/session-09.html#participate",
    "href": "sessions/session-09.html#participate",
    "title": "Session 9",
    "section": "",
    "text": "üñ•Ô∏è Session 09"
  },
  {
    "objectID": "sessions/session-09.html#suggested-readings",
    "href": "sessions/session-09.html#suggested-readings",
    "title": "Session 9",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nChen, Y., Peng, Z., Kim, S.-H., & Choi, C. W. (2023). What We Can Do and Cannot Do with Topic Modeling: A Systematic Review. Communication Methods and Measures, 17(2), 111‚Äì130. https://doi.org/10.1080/19312458.2023.2167965\nHase, V. (2023). Automated Content Analysis (F. Oehmer-Pedrazzi, S. H. Kessler, E. Humprecht, K. Sommer, & L. Castro, Eds.; pp. 23‚Äì36). Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-36179-2_3\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., H√§ussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93‚Äì118. https://doi.org/10.1080/19312458.2018.1430754"
  },
  {
    "objectID": "sessions/session-09.html#additional-readings",
    "href": "sessions/session-09.html#additional-readings",
    "title": "Session 9",
    "section": "Additional readings",
    "text": "Additional readings\n\nBernhard, J., Teuffenbach, M., & Boomgaarden, H. G. (2023). Topic Model Validation Methods and their Impact on Model Selection and Evaluation. Computational Communication Research, 5(1), 1. https://doi.org/10.5117/CCR2023.1.13.BERN\nChurchill, R., & Singh, L. (2022). The Evolution of Topic Modeling. ACM Computing Surveys, 54(10s), 1‚Äì35. https://doi.org/10.1145/3507900\nDenny, M. J., & Spirling, A. (2018). Text Preprocessing For Unsupervised Learning: Why It Matters, When It Misleads, And What To Do About It. Political Analysis, 26(2), 168‚Äì189. https://doi.org/10/gdjsqk\nMaier, D., Niekler, A., Wiedemann, G., & Stoltenberg, D. (2020). How Document Sampling and Vocabulary Pruning Affect the Results of Topic Models. Computational Communication Research, 2(2), 139‚Äì152. https://doi.org/10.5117/CCR2020.2.001.MAIE\nQuinn, K. M., Monroe, B. L., Colaresi, M., Crespin, M. H., & Radev, D. R. (2010). How to Analyze Political Attention with Minimal Assumptions and Costs. American Journal of Political Science, 54(1), 209‚Äì228. https://doi.org/10.1111/j.1540-5907.2009.00427.x\nRoberts, M. E., Stewart, B. M., Tingley, D., Lucas, C., Leder-Luis, J., Gadarian, S. K., Albertson, B., & Rand, D. G. (2014). Structural Topic Models for Open-Ended Survey Responses. American Journal of Political Science, 58(4), 1064‚Äì1082. https://doi.org/10.1111/ajps.12103"
  },
  {
    "objectID": "sessions/session-09.html#useful-packages",
    "href": "sessions/session-09.html#useful-packages",
    "title": "Session 9",
    "section": "Useful packages",
    "text": "Useful packages\n\nquanteda üåê | \nBenoit, K., Watanabe, K., Wang, H., Nulty, P., Obeng, A., M√ºller, S., & Matsuo, A. (2018). Quanteda: An r package for the quantitative analysis of textual data. Journal of Open Source Software, 3(30), 774. https://doi.org/10.21105/joss.00774\nstm üåê |  f√ºr Structural Topic Modeling\nRoberts, M. E., Stewart, B. M., & Tingley, D. (2019). stm: An R Package for Structural Topic Models. Journal of Statistical Software, 91(1), 1‚Äì40. https://doi.org/10.18637/jss.v091.i02\nkeyATM üåê |  f√ºr Keyword Assisted Topic Modeling\nEsser, F. (2019). Comparative international studies of election campaign communication: What should happen next? Journalism, 20(8), 1124‚Äì1138. https://doi.org/10.1177/1464884919845450\ntopicmodels üåê f√ºr LDA basiertes Verfahren\nGr√ºn, B., & Hornik, K. (n.d.). topicmodels: Topic Models. https://doi.org/10.32614/CRAN.package.topicmodels\ntidytext üåê |  f√ºr Extraktion z.B. der Theta- oder Phi-Matrix\nSilge, J., & Robinson, D. (2016). Tidytext: Text mining and analysis using tidy data principles in r. The Journal of Open Source Software, 1(3), 37. https://doi.org/10.21105/joss.00037\nLDAvis zur Visualisierung\nSievert, C., & Shirley, K. (n.d.). LDAvis: Interactive Visualization of Topic Models. https://doi.org/10.32614/CRAN.package.LDAvis\nstminsights üåê |  zur Visualisierung\nSchwemmer, C. (2021). Stminsights: A shiny application for inspecting structural topic models. https://github.com/cschwem2er/stminsights\noolong üåê |  f√ºr Validierungen\nChan, C., & S√§ltzer, M. (2020). Oolong: An r package for validating automated content analysis tools. Journal of Open Source Software, 5(55), 2461. https://doi.org/10.21105/joss.02461\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "sessions/session-06.html",
    "href": "sessions/session-06.html",
    "title": "Session 6",
    "section": "",
    "text": "The following sections list the mandatory articles for the each presenting group. Additional optional articles are provided under Further reading.\n\n\n\n\n\n\nTrilling, D. (2015). Two Different Debates? Investigating the Relationship Between a Political Debate on TV and Simultaneous Comments on Twitter. Social Science Computer Review, 33(3), 259‚Äì276. https://doi.org/10.1177/0894439314537886\nRobertson, C. T., Dutton, W. H., Ackland, R., & Peng, T.-Q. (2019). The democratic role of social media in political debates: The use of Twitter in the first televised US presidential debate of 2016. Journal of Information Technology & Politics, 16(2), 105‚Äì118. https://doi.org/10.1080/19331681.2019.1590283\nJennings, F. J., Warner, B. R., McKinney, M. S., Kearney, C. C., Funk, M. E., & Bramlett, J. C. (2020). Learning from Presidential Debates: Who Learns the Most and Why? Communication Studies, 71(5), 896‚Äì910. https://doi.org/10.1080/10510974.2020.1807377\n\n\n\n\nCoddington, M., Molyneux, L., & Lawrence, R. G. (2014). Fact Checking the Campaign: How Political Reporters Use Twitter to Set the Record Straight (or Not). The International Journal of Press/Politics, 19(4), 391‚Äì409. https://doi.org/10.1177/1940161214540942\nGoovaerts, I., & Turkenburg, E. (2023). How Contextual Features Shape Incivility Over Time: An Analysis of the Evolution and Determinants of Political Incivility in Televised Election Debates (19852019). Communication Research, 50(4), 480‚Äì507. https://doi.org/10.1177/00936502221135694\n\n\n\n\n\n\nAsbury-Kimmel, V., Chang, K.-C., McCabe, K. T., Munger, K., & Ventura, T. (2021). The effect of streaming chat on perceptions of political debates. Journal of Communication, 71(6), 947‚Äì974. https://doi.org/10.1093/joc/jqab041\nRuiz-Bravo, N., Selander, L., & Roshan, M. (2022). The Political Turn of Twitch  Understanding Live Chat as an Emergent Political Space. http://hdl.handle.net/10125/79723\nRiddick, S., & Shivener, R. (2022). Affective spamming on twitch: Rhetorics of an emote-only audience in a presidential inauguration livestream. Computers and Composition, 64. https://doi.org/10.1016/j.compcom.2022.102711\nEaton, J. (2024). From the comments section: Analyzing online public discourse on the first 2020 presidential debate. Research & Politics, 11(3), 20531680241271758. https://doi.org/10.1177/20531680241271758\n\n\n\n\nPowell, A., & Williams-Johnson, D. (2023). ‚ÄúYou dumb cracker b*tch‚Äù: The legitimizing of White supremacy during a Twitch ban of HasanAbi. New Media & Society, 14614448231191776. https://doi.org/10.1177/14614448231191776\n\n\n\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "sessions/session-06.html#literature",
    "href": "sessions/session-06.html#literature",
    "title": "Session 6",
    "section": "",
    "text": "The following sections list the mandatory articles for the each presenting group. Additional optional articles are provided under Further reading.\n\n\n\n\n\n\nTrilling, D. (2015). Two Different Debates? Investigating the Relationship Between a Political Debate on TV and Simultaneous Comments on Twitter. Social Science Computer Review, 33(3), 259‚Äì276. https://doi.org/10.1177/0894439314537886\nRobertson, C. T., Dutton, W. H., Ackland, R., & Peng, T.-Q. (2019). The democratic role of social media in political debates: The use of Twitter in the first televised US presidential debate of 2016. Journal of Information Technology & Politics, 16(2), 105‚Äì118. https://doi.org/10.1080/19331681.2019.1590283\nJennings, F. J., Warner, B. R., McKinney, M. S., Kearney, C. C., Funk, M. E., & Bramlett, J. C. (2020). Learning from Presidential Debates: Who Learns the Most and Why? Communication Studies, 71(5), 896‚Äì910. https://doi.org/10.1080/10510974.2020.1807377\n\n\n\n\nCoddington, M., Molyneux, L., & Lawrence, R. G. (2014). Fact Checking the Campaign: How Political Reporters Use Twitter to Set the Record Straight (or Not). The International Journal of Press/Politics, 19(4), 391‚Äì409. https://doi.org/10.1177/1940161214540942\nGoovaerts, I., & Turkenburg, E. (2023). How Contextual Features Shape Incivility Over Time: An Analysis of the Evolution and Determinants of Political Incivility in Televised Election Debates (19852019). Communication Research, 50(4), 480‚Äì507. https://doi.org/10.1177/00936502221135694\n\n\n\n\n\n\nAsbury-Kimmel, V., Chang, K.-C., McCabe, K. T., Munger, K., & Ventura, T. (2021). The effect of streaming chat on perceptions of political debates. Journal of Communication, 71(6), 947‚Äì974. https://doi.org/10.1093/joc/jqab041\nRuiz-Bravo, N., Selander, L., & Roshan, M. (2022). The Political Turn of Twitch  Understanding Live Chat as an Emergent Political Space. http://hdl.handle.net/10125/79723\nRiddick, S., & Shivener, R. (2022). Affective spamming on twitch: Rhetorics of an emote-only audience in a presidential inauguration livestream. Computers and Composition, 64. https://doi.org/10.1016/j.compcom.2022.102711\nEaton, J. (2024). From the comments section: Analyzing online public discourse on the first 2020 presidential debate. Research & Politics, 11(3), 20531680241271758. https://doi.org/10.1177/20531680241271758\n\n\n\n\nPowell, A., & Williams-Johnson, D. (2023). ‚ÄúYou dumb cracker b*tch‚Äù: The legitimizing of White supremacy during a Twitch ban of HasanAbi. New Media & Society, 14614448231191776. https://doi.org/10.1177/14614448231191776\n\n\n\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "sessions/session-04.html",
    "href": "sessions/session-04.html",
    "title": "Session 4",
    "section": "",
    "text": "The following sections list the mandatory articles for the each presenting group. Additional optional articles are provided under Further reading.\n\n\n\n\n\n\nHilvert-Bruce, Z., Neill, J. T., Sj√∂blom, M., & Hamari, J. (2018). Social motivations of live-streaming viewer engagement on twitch. Computers in Human Behavior, 84, 58‚Äì67. https://doi.org/10.1016/j.chb.2018.02.013\nCastro-Agirre, I., & Mart√≠nez-Fern√°ndez, G. (2024). From gamer niche to mainstream media: Twitch‚Äôs most popular media figures and content. Communication & Society, 179‚Äì196. https://doi.org/10.15581/003.37.2.179-196\nNguyen, T. T., & Veer, E. (2024). Why people watch user-generated videos? A systematic review and meta-analysis. International Journal of Human-Computer Studies, 181, 103144. https://doi.org/10.1016/j.ijhcs.2023.103144\n\n\n\n\nGros, D., Hackenholt, A., Zawadzki, P., & Wanner, B. (2018). Interactions of Twitch Users and Their Usage Behavior (G. Meiselwitz, Ed.; pp. 201‚Äì213). Springer International Publishing. https://doi.org/10.1007/978-3-319-91485-5_15\nZimmer, F., Scheibe, K., & Stock, W. G. (2018). A Model for Information Behavior Research on Social Live Streaming Services (SLSSs) (G. Meiselwitz, Ed.; pp. 429‚Äì448). Springer International Publishing. https://doi.org/10.1007/978-3-319-91485-5_33\nXu, X.-Y., Niu, W.-B., Jia, Q.-D., Nthoiwa, L., & Li, L.-W. (2021). Why Do Viewers Engage in Video Game Streaming? The Perspective of Cognitive Emotion Theory and the Moderation Effect of Personal Characteristics. Sustainability, 13(21), 11990. https://doi.org/10.3390/su132111990\nKneisel, A., & Sternadori, M. (2023). Effects of parasocial affinity and gender on live streaming fans‚Äô motivations. Convergence, 29(2), 322‚Äì341. https://doi.org/10.1177/13548565221114461\n\n\n\n\n\n\nJacobs, N., & Booth, P. (2021). Converging experiences, converging audiences: An analysis of doctor who on Twitch. Convergence: The International Journal of Research into New Media Technologies, 27(5), 1324‚Äì1342. https://doi.org/10.1177/1354856520976447\nDutt, S., & Graham, S. (2023). Video, talk and text: How do parties communicate coherently across modalities in live videostreams? Discourse, Context and Media, 55. https://doi.org/10.1016/j.dcm.2023.100726\nNavarro, A., & Tapiador, F. J. (2023). Twitch as a privileged locus to analyze young people‚Äôs attitudes in the climate change debate: a quantitative analysis. Humanities and Social Sciences Communications, 10(1), 1‚Äì13. https://doi.org/10.1057/s41599-023-02377-4\n\n\n\n\nYoung, A., & Wiedenfeld, G. (2022). A Motivation Analysis of Video Game Microstreamers: ‚ÄúFinding My People and Myself‚Äù on YouTube and Twitch. Journal of Broadcasting & Electronic Media, 66(2), 381‚Äì399. https://doi.org/10.1080/08838151.2022.2086549\nLessel, P., Altmeyer, M., Sahner, J., & Kr√ºger, A. (2022). Streamer‚Äôs hell - investigating audience influence in live-streams beyond the game. Proc. ACM Hum.-Comput. Interact., 6(CHI PLAY), 252:1252:27. https://doi.org/10.1145/3549515\nMao, E. (2022). How live stream content types impact viewers‚Äô support behaviors? Mediational analysis on psychological and social gratifications. Frontiers in Psychology, 13. https://doi.org/10.3389/fpsyg.2022.951055\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "sessions/session-04.html#literature",
    "href": "sessions/session-04.html#literature",
    "title": "Session 4",
    "section": "",
    "text": "The following sections list the mandatory articles for the each presenting group. Additional optional articles are provided under Further reading.\n\n\n\n\n\n\nHilvert-Bruce, Z., Neill, J. T., Sj√∂blom, M., & Hamari, J. (2018). Social motivations of live-streaming viewer engagement on twitch. Computers in Human Behavior, 84, 58‚Äì67. https://doi.org/10.1016/j.chb.2018.02.013\nCastro-Agirre, I., & Mart√≠nez-Fern√°ndez, G. (2024). From gamer niche to mainstream media: Twitch‚Äôs most popular media figures and content. Communication & Society, 179‚Äì196. https://doi.org/10.15581/003.37.2.179-196\nNguyen, T. T., & Veer, E. (2024). Why people watch user-generated videos? A systematic review and meta-analysis. International Journal of Human-Computer Studies, 181, 103144. https://doi.org/10.1016/j.ijhcs.2023.103144\n\n\n\n\nGros, D., Hackenholt, A., Zawadzki, P., & Wanner, B. (2018). Interactions of Twitch Users and Their Usage Behavior (G. Meiselwitz, Ed.; pp. 201‚Äì213). Springer International Publishing. https://doi.org/10.1007/978-3-319-91485-5_15\nZimmer, F., Scheibe, K., & Stock, W. G. (2018). A Model for Information Behavior Research on Social Live Streaming Services (SLSSs) (G. Meiselwitz, Ed.; pp. 429‚Äì448). Springer International Publishing. https://doi.org/10.1007/978-3-319-91485-5_33\nXu, X.-Y., Niu, W.-B., Jia, Q.-D., Nthoiwa, L., & Li, L.-W. (2021). Why Do Viewers Engage in Video Game Streaming? The Perspective of Cognitive Emotion Theory and the Moderation Effect of Personal Characteristics. Sustainability, 13(21), 11990. https://doi.org/10.3390/su132111990\nKneisel, A., & Sternadori, M. (2023). Effects of parasocial affinity and gender on live streaming fans‚Äô motivations. Convergence, 29(2), 322‚Äì341. https://doi.org/10.1177/13548565221114461\n\n\n\n\n\n\nJacobs, N., & Booth, P. (2021). Converging experiences, converging audiences: An analysis of doctor who on Twitch. Convergence: The International Journal of Research into New Media Technologies, 27(5), 1324‚Äì1342. https://doi.org/10.1177/1354856520976447\nDutt, S., & Graham, S. (2023). Video, talk and text: How do parties communicate coherently across modalities in live videostreams? Discourse, Context and Media, 55. https://doi.org/10.1016/j.dcm.2023.100726\nNavarro, A., & Tapiador, F. J. (2023). Twitch as a privileged locus to analyze young people‚Äôs attitudes in the climate change debate: a quantitative analysis. Humanities and Social Sciences Communications, 10(1), 1‚Äì13. https://doi.org/10.1057/s41599-023-02377-4\n\n\n\n\nYoung, A., & Wiedenfeld, G. (2022). A Motivation Analysis of Video Game Microstreamers: ‚ÄúFinding My People and Myself‚Äù on YouTube and Twitch. Journal of Broadcasting & Electronic Media, 66(2), 381‚Äì399. https://doi.org/10.1080/08838151.2022.2086549\nLessel, P., Altmeyer, M., Sahner, J., & Kr√ºger, A. (2022). Streamer‚Äôs hell - investigating audience influence in live-streams beyond the game. Proc. ACM Hum.-Comput. Interact., 6(CHI PLAY), 252:1252:27. https://doi.org/10.1145/3549515\nMao, E. (2022). How live stream content types impact viewers‚Äô support behaviors? Mediational analysis on psychological and social gratifications. Frontiers in Psychology, 13. https://doi.org/10.3389/fpsyg.2022.951055\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "sessions/session-01.html",
    "href": "sessions/session-01.html",
    "title": "Session 1",
    "section": "",
    "text": "Important\n\n\n\nPlease make sure that you answered the survey send to you by (StudOn) mail."
  },
  {
    "objectID": "sessions/session-01.html#prepare",
    "href": "sessions/session-01.html#prepare",
    "title": "Session 1",
    "section": "Prepare",
    "text": "Prepare\n‚úçÔ∏è Fill out the short survey before the 22.10.2024 (see  StudOn for Link)\nüìñ Read the syllabus"
  },
  {
    "objectID": "sessions/session-01.html#participate",
    "href": "sessions/session-01.html#participate",
    "title": "Session 1",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Session 01\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "sessions/session-03.html",
    "href": "sessions/session-03.html",
    "title": "Session 3",
    "section": "",
    "text": "‚úçÔ∏è Keep working on R-Video-Tutorials.\nüîç Check out the resources in the section ‚ÄúWorking with R‚Äù (z.B. R Textbooks & Useful R sources)"
  },
  {
    "objectID": "sessions/session-03.html#prepare",
    "href": "sessions/session-03.html#prepare",
    "title": "Session 3",
    "section": "",
    "text": "‚úçÔ∏è Keep working on R-Video-Tutorials.\nüîç Check out the resources in the section ‚ÄúWorking with R‚Äù (z.B. R Textbooks & Useful R sources)"
  },
  {
    "objectID": "sessions/session-03.html#participate",
    "href": "sessions/session-03.html#participate",
    "title": "Session 3",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Session 03"
  },
  {
    "objectID": "sessions/session-03.html#suggested-readings",
    "href": "sessions/session-03.html#suggested-readings",
    "title": "Session 3",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nBauer, P. C., & Landesvatter, C. (2023). Writing a reproducible paper with RStudio and quarto. https://osf.io/ur4xn\nJonge, E. de, & Loo, M. van der. (2013). An introduction to data cleaning with R.\nPearson, R. K. (2018). Exploratory data analysis using r. CRC Press/Taylor & Francis Group."
  },
  {
    "objectID": "sessions/session-03.html#useful-tools-resources",
    "href": "sessions/session-03.html#useful-tools-resources",
    "title": "Session 3",
    "section": "Useful tools & resources",
    "text": "Useful tools & resources\n\nüìñ Useful and detailed tutorial to use git/github with RStudio\nüìñ Happy Git and GitHub for the useR\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "slides/slides-08.html#seminarplan",
    "href": "slides/slides-08.html#seminarplan",
    "title": "üî® Advanced Methods",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\nSession\nDatum\nTopic\nPresenter\n\n\n\n\n\nüìÇ Block 1\nIntroduction\n\n\n\n1\n23.10.2024\nKick-Off\nChristoph Adrian\n\n\n2\n30.10.2024\nDBD: Overview & Introduction\nChristoph Adrian\n\n\n3\n06.11.2024\nüî® Introduction to working with R\nChristoph Adrian\n\n\n\nüìÇ Block 2\nTheoretical Background: Twitch & TV Election Debates\n\n\n\n4\n13.11.2024\nüìö Twitch-Nutzung im Fokus\nStudent groups\n\n\n5\n20.11.2024\nüìö (Wirkungs-)Effekte von Twitch & TV-Debatten\nStudent groups\n\n\n6\n27.11.2024\nüìö Politische Debatten & Social Media\nStudent groups\n\n\n\nüìÇ Block 3\nMethod: Natural Language Processing\n\n\n\n7\n04.12.2024\nüî® Text as data I: Introduction\nChristoph Adrian\n\n\n8\n11.12.2024\nüî® Text as data II: Advanced Methods\nChristoph Adrian\n\n\n9\n18.12.2024\nüî® Advanced Method I: Topic Modeling\nChristoph Adrian\n\n\n\nNo lecture\nüéÑChristmas Break\n\n\n\n10\n08.01.2025\nüî® Advanced Method II: Machine Learning\nChristoph Adrian\n\n\n\nüìÇ Block 4\nProject Work\n\n\n\n11\n15.01.2025\nüî® Project work\nStudent groups\n\n\n12\n22.01.2025\nüî® Project work\nStudent groups\n\n\n13\n29.01.2025\nüìä Project Presentation I\nStudent groups (TBD)\n\n\n14\n05.02.2025\nüìä Project Presentation & üèÅ Evaluation\nStudentds (TBD) & Christoph Adrian"
  },
  {
    "objectID": "slides/slides-08.html#possibilities-over-possibilities",
    "href": "slides/slides-08.html#possibilities-over-possibilities",
    "title": "üî® Advanced Methods",
    "section": "Possibilities over possibilities",
    "text": "Possibilities over possibilities\n√úberblick √ºber verschiedene Methoden der Textanalyse (Grimmer & Stewart, 2013)"
  },
  {
    "objectID": "slides/slides-08.html#clusteranalyse-on",
    "href": "slides/slides-08.html#clusteranalyse-on",
    "title": "üî® Advanced Methods",
    "section": "Clusteranalyse on üíâ",
    "text": "Clusteranalyse on üíâ\nGrundidee des Topic Modelings\n\n‚Äûcomputational content-analysis technique [‚Ä¶] used to investigate the ‚Äúhidden‚Äù thematic structure of [‚Ä¶] texts‚Äù (Maier et al., 2018, p. S.93)\n\n\nVerfahren des un√ºberwachten maschinellen Lernens, das sich daher insbesondere zur Exploration und Deskription gro√üer Textmengen eignet\nThemen werden strikt auf Basis von Worth√§ufigkeiten in den einzelnen Dokumenten vermeintlich objektiv berechnet, ganz ohne subjektive Einsch√§tzungen und damit einhergehenden etwaigen Verzerrungen\nBekanntesten dieser Verfahren sind LDA (Latent Dirichlet Allocation) sowie die darauf aufbauenden CTM (Correlated Topic Models) und STM (Structural Topic Models)"
  },
  {
    "objectID": "slides/slides-08.html#vom-korpus-zum-themenmodell",
    "href": "slides/slides-08.html#vom-korpus-zum-themenmodell",
    "title": "üî® Advanced Methods",
    "section": "Vom Korpus zum Themenmodell",
    "text": "Vom Korpus zum Themenmodell\nProzess des Topic Modelings nach Maier et al. (2018)"
  },
  {
    "objectID": "slides/slides-08.html#prominente-w√∂rter-eines-themas",
    "href": "slides/slides-08.html#prominente-w√∂rter-eines-themas",
    "title": "üî® Advanced Methods",
    "section": "Prominente W√∂rter eines Themas",
    "text": "Prominente W√∂rter eines Themas\nVorstellung der Word-topic oder Phi-Matrix\n\n\n\n\nBedingte Wahrscheinlichkeit (beta), mit der Features in Themen pr√§valent sind\nWortlisten, die Themen beschreiben (‚ÄúTop Features‚Äù)"
  },
  {
    "objectID": "slides/slides-08.html#zugeh√∂rige-dokumente-eines-themas",
    "href": "slides/slides-08.html#zugeh√∂rige-dokumente-eines-themas",
    "title": "üî® Advanced Methods",
    "section": "Zugeh√∂rige Dokumente eines Themas",
    "text": "Zugeh√∂rige Dokumente eines Themas\nVorstellung der Document-topic oder Theta-Matrix\n\n\n\n\nBedingte Wahrscheinlichkeit (gamma), mit der Themen in Dokumenten pr√§valent sind\nDokumentenlisten, die Themen beschreiben (‚ÄúTop Documents‚Äù)"
  },
  {
    "objectID": "slides/slides-08.html#themenmodelle-sind",
    "href": "slides/slides-08.html#themenmodelle-sind",
    "title": "üî® Advanced Methods",
    "section": "Themenmodelle sind ‚Ä¶",
    "text": "Themenmodelle sind ‚Ä¶\nPromises & Pitfalls von Themenmodellen\n\nprobabilistisch ‚ûú Zuordnung von Wahrscheinlichkeiten, nicht eindeutigen Klassen\n\nModell sagt nicht eindeutig, welches das ‚Äûeine‚Äú Thema je Dokument ist oder wie ein Thema zu interpretieren ist ‚ûú es gibt nur (probabilistische) Hinweise.\n\ngenerative ‚ûú Prozess findet das statistische ‚Äûpassendste‚Äú Modell, um unseren Korpus zu ‚Äûgenerieren‚Äú\n\nModell l√§uft in iterativen Schlaufen immer und immer wieder durch, bis eine ‚Äúoptimale‚Äù L√∂sung gefunden wurde\nAber: Es gibt z.T. nicht-deterministische (d.h. je nach Einstellungen unterschiedliche) L√∂sungen.\n\n\n\nProababilistisches Modell: - Features haben eine Wahrscheinlichkeit von gr√∂√üer gleich 0 je Thema (œï-matrix) - Themen haben eine Wahrscheinlichkeit von gr√∂√üer gleich 0 je Dokument (Œ∏-matrix) Generatvies Modell: - Gemeinsame Modellierung der beobachteten Variablen (Features i in den Dokumenten d) & der latenten Variablen (œï, Œ∏)"
  },
  {
    "objectID": "slides/slides-08.html#beyond-lda",
    "href": "slides/slides-08.html#beyond-lda",
    "title": "üî® Advanced Methods",
    "section": "Beyond LDA",
    "text": "Beyond LDA\nVerschiedene Ans√§tze der Themenmodellierung\n\n\nLatent Dirichlet Allocation [LDA] (Blei et al., 2003) ist ein probabilistisches generatives Modell, das davon ausgeht, dass jedes Dokumentin einem Korpus eine Mischung von Themen ist und jedes Wort im Dokument einem der Themen des Dokuments zuzuordnen ist.\nStructural Topic Modeling [STM] (Roberts et al., 2016; Roberts et al., 2019) erweitert LDA durch die Einbeziehung von Kovariaten auf Dokumentenebene und erm√∂glicht die Modellierung des Einflusses externer Faktoren auf die Themenpr√§valenz.\nWord embeddings (Word2Vec (Mikolov et al., 2013) , Glove (Pennington et al., 2014)) stellen W√∂rter als kontinuierliche Vektoren in einem hochdimensionalen Raum dar und erfassen semantische Beziehungen zwischen W√∂rtern basierend auf ihrem Kontext in den Daten.\nTopic Modeling mit Neural Networks (BERTopic(Devlin et al., 2019), Doc2Vec(Le & Mikolov, 2014)) nutzt Deep Learning-Architekturen, um automatisch latente Themen aus Textdaten zu lernen"
  },
  {
    "objectID": "slides/slides-08.html#opinion-matters",
    "href": "slides/slides-08.html#opinion-matters",
    "title": "üî® Advanced Methods",
    "section": "Opinion matters",
    "text": "Opinion matters\nSentimentanalyse: Einf√ºhrung und Anwedungsf√§lle\n\nAnwendung von Natural Language Processing (NLP), Textanalyse und Computational Linguistics, um\n\nsubjektive Informationen aus Texten zu extrahieren\nMeinung, Einstellung oder Emotionen zu bestimmten Themen oder Entit√§ten zu bestimmen \n\nWichtige Anwendungsgebiete sind Marketinganalysen, Produktbewertungen, politische Meinungsforschung und soziale Medien."
  },
  {
    "objectID": "slides/slides-08.html#the-very-good-the-good-and-the-ugly",
    "href": "slides/slides-08.html#the-very-good-the-good-and-the-ugly",
    "title": "üî® Advanced Methods",
    "section": "The very good, the good and the ugly",
    "text": "The very good, the good and the ugly\nVerschiedene Methode der Sentimentanalyse\n\nRegelbasierte Ans√§tze: Verwenden von definierten Regeln und W√∂rterb√ºchern.\nMaschinelles Lernen: Verwendung von Klassifikatoren wie Naive Bayes, SVM.\nDeep Learning: Einsatz von neuronalen Netzen wie RNNs oder Transformers.\n\nWelche Aspekte werden untersucht?\n\nPolarit√§t: positive, negative, neutrale.\nSubjektfunktion: Wer spricht? Wessen Meinung?\nIntensit√§t: St√§rke des Ausdrucks der Meinung."
  },
  {
    "objectID": "slides/slides-08.html#out-of-the-box-or-diy",
    "href": "slides/slides-08.html#out-of-the-box-or-diy",
    "title": "üî® Advanced Methods",
    "section": "Out of the box or DIY?",
    "text": "Out of the box or DIY?\nWerkzeuge und Tools f√ºr Sentimentanalyse\n\nLinguistic Inquiry and Word Count [LIWC] (Tausczik & Pennebaker, 2009): Textanalysesoftware-Tool & Off-the-shelf-Dictionary.\nValence Aware Dictionary and sEntiment Reasoner [VADER] (Hutto & Gilbert, 2014): Regelbasierte Sentimentanalyse-Tool, das speziell f√ºr Social Media Texte entwickelt wurde\nTextBlob: Python-Bibliothek f√ºr Textverarbeitung, die auch Sentimentanalyse unterst√ºtzt.\nCommercial Tools: IBM Watson, Google Cloud Natural Language API, Microsoft Text Analytics.\n\nABER: Zunehmender Einsatz von Transformer-Modellen wie BERT (Devlin et al., 2019) und GPT f√ºr genauere Analysen."
  },
  {
    "objectID": "slides/slides-08.html#quick-reminder",
    "href": "slides/slides-08.html#quick-reminder",
    "title": "üî® Advanced Methods",
    "section": "Quick reminder",
    "text": "Quick reminder\nDatengrundlage f√ºr die heutige Sitzung\n\n\n\n# Create corpus\ncorp_transcripts &lt;- transcripts %&gt;% \n  quanteda::corpus(\n    docid_field = \"id_sequence\", \n    text_field = \"dialogue\"\n  )\n\n# Tokenize corpus\ntoks_transcripts &lt;- corp_transcripts %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  ) %&gt;% \n  quanteda::tokens_remove(\n    pattern = quanteda::stopwords(\"en\")\n  )\n\n\n\n# Add n_grams\ntoks_transcripts_ngrams &lt;- toks_transcripts %&gt;% \n  quanteda::tokens_ngrams(n = 1:3)\n\n# Convert to DFM\ndfm_transcripts &lt;- toks_transcripts_ngrams %&gt;% \n  quanteda::dfm()"
  },
  {
    "objectID": "slides/slides-08.html#und-f√ºr-die-chats",
    "href": "slides/slides-08.html#und-f√ºr-die-chats",
    "title": "üî® Advanced Methods",
    "section": "Und f√ºr die Chats",
    "text": "Und f√ºr die Chats\nDatengrundlage f√ºr die heutige Sitzung\n\n# Create corpus\ncorp_chats &lt;- chats %&gt;% \n  quanteda::corpus(docid_field = \"message_id\", text_field = \"message_content\")\n\n# Tokenize corpus\ntoks_chats &lt;- corp_chats %&gt;% quanteda::tokens()\n\n# Convert to DFM\ndfm_chats &lt;- toks_chats %&gt;% quanteda::dfm()"
  },
  {
    "objectID": "slides/slides-08.html#better-bag-of-words",
    "href": "slides/slides-08.html#better-bag-of-words",
    "title": "üî® Advanced Methods",
    "section": "Better bag-of-words",
    "text": "Better bag-of-words\nCo-Occurence Verfahren und Ihre Einsatzgebiet\nAnnahme von bag-of-words Modellen (Inhalte eines Text lassen sich vollst√§ndig durch die H√§ufigkeit der in ihm vorkommenden W√∂rter beschreiben) problematisch\n\nVerbessung durch komplexere Verfahren bzw. Repr√§sentationen, wie z.B. Ngrams, Keywords-in-Context, Collocations, Semantische Netzwerke etc.\nPotentielle Anwendungsf√§lle:\n\nTextbereinigung, z.B. Entfernung von Duplikaten via ngram-shingling (Nicholls, 2019)\nTextanalyse, z.B. Stereotypen (Arendt & Karadas, 2017), Labeling, Frames (Ruigrok & Atteveldt, 2007)"
  },
  {
    "objectID": "slides/slides-08.html#h√§ufige-wortkombinationen",
    "href": "slides/slides-08.html#h√§ufige-wortkombinationen",
    "title": "üî® Advanced Methods",
    "section": "H√§ufige Wortkombinationen",
    "text": "H√§ufige Wortkombinationen\nNgrams: Sequenzen von N aufeinanderfolgenden Token\n\ntoks_transcripts %&gt;% \n  quanteda::tokens_ngrams(n = 2) %&gt;% \n  quanteda::dfm() %&gt;%  \n  quanteda.textstats::textstat_frequency() %&gt;% \n  head(25) \n\n             feature frequency rank docfreq group\n1          know_know      1337    1      49   all\n2  t-mobile_t-mobile       864    2       6   all\n3       donald_trump       755    3     461   all\n4          going_say       666    4      30   all\n5          say_going       661    5      35   all\n6         saying_bad       558    6       4   all\n7         bad_saying       553    7       3   all\n8      kamala_harris       494    8     333   all\n9     vice_president       429    9     376   all\n10   curious_curious       373   10       7   all\n11    sekunden_pause       354   11     266   all\n12         right_now       269   12     234   all\n13     united_states       268   13     211   all\n14         feel_like       230   14     164   all\n15             oh_oh       229   15      10   all\n16         like_know       208   16     133   all\n17   president_trump       203   17     178   all\n18         like_like       191   18     144   all\n19  president_harris       186   19     179   all\n20        lot_people       181   20     138   all\n21         know_like       168   21     109   all\n22   american_people       163   22     118   all\n23            oh_god       154   23     139   all\n24         just_like       153   24     129   all\n25  former_president       141   25     115   all"
  },
  {
    "objectID": "slides/slides-08.html#statistisch-h√§ufige-wortkombinationen",
    "href": "slides/slides-08.html#statistisch-h√§ufige-wortkombinationen",
    "title": "üî® Advanced Methods",
    "section": "Statistisch h√§ufige Wortkombinationen",
    "text": "Statistisch h√§ufige Wortkombinationen\nKollokationen: Identifikation von bedeutungsvollen Wortkombinationen\n\ntoks_transcripts %&gt;% \n  quanteda.textstats::textstat_collocations(\n    size = 2, \n    min_count = 5\n  ) %&gt;% \n  head(25)\n\n        collocation count count_nested length    lambda        z\n1         know know  1337            0      2  3.890787 98.31370\n2        saying bad   558            0      2  6.503305 81.19215\n3        bad saying   553            0      2  6.481795 80.98625\n4         going say   666            0      2  4.555737 80.92246\n5         say going   661            0      2  4.562963 80.82524\n6      donald trump   755            0      2  7.422847 75.19267\n7     kamala harris   494            0      2  7.873933 68.88003\n8    vice president   429            0      2  7.258104 56.72753\n9             oh oh   229            0      2  4.679549 54.95066\n10        right now   269            0      2  3.996328 53.27167\n11    senator vance   129            0      2  6.583164 49.48173\n12       little bit   132            0      2  8.268099 45.80914\n13 president harris   186            0      2  4.027681 45.69845\n14           oh god   154            0      2  6.175930 44.94423\n15        years ago   102            0      2  6.436807 43.06424\n16         tim walz    90            0      2  7.588398 43.05596\n17  president trump   203            0      2  3.468589 42.70406\n18       four years   100            0      2  6.381600 42.53221\n19      health care   136            0      2  7.400206 41.83123\n20      white house    85            0      2  8.071701 41.52827\n21   donald trump's   132            0      2  6.408658 41.05938\n22 former president   141            0      2  5.790480 41.04486\n23  curious curious   373            0      2 11.836611 40.77202\n24    governor walz    77            0      2  6.512020 39.65499\n25      two minutes    84            0      2  6.490910 39.39074\n\n\n\nEs wird die H√§ufigkeit von Wortkombinationen analysiert und mit erwarteten H√§ufigkeiten verglichen, um besonders signifikante Kollokationen zu identifizieren."
  },
  {
    "objectID": "slides/slides-08.html#spezifische-token-plus-kontext",
    "href": "slides/slides-08.html#spezifische-token-plus-kontext",
    "title": "üî® Advanced Methods",
    "section": "Spezifische Token plus Kontext",
    "text": "Spezifische Token plus Kontext\nKeywords-in-Context (KWIC): Unmittelbarer Wortkontext ohne statistische Gewichtung\n\ntoks_transcripts %&gt;% \n  kwic(\"know\", window = 3) %&gt;% \nhead(10)\n\nKeyword-in-context with 10 matches.                                                                              \n [p1_s0018, 29]  opportunity economy thing | know | shortage homes housing    \n [p1_s0018, 39]            far many people | know | young families need       \n [p1_s0020, 25]  billions billions dollars | know | China fact never          \n [p1_s0022, 44]          done intend build | know | aspirations hopes American\n  [p1_s0024, 2]                    nothing | know | knows better anyone       \n  [p1_s0025, 1]                            | know | everybody else Vice       \n [p1_s0026, 64]        stand issues invite | know | Donald Trump actually     \n [p1_s0028, 38]       goods coming country | know | many economists say       \n [p1_s0029, 24] billions dollars countries | know | like gone immediately     \n [p1_s0031, 90]         Thank President Xi | know | Xi responsible lacking"
  },
  {
    "objectID": "slides/slides-08.html#schl√ºsselphrase-plus-kontext",
    "href": "slides/slides-08.html#schl√ºsselphrase-plus-kontext",
    "title": "üî® Advanced Methods",
    "section": "Schl√ºsselphrase plus Kontext",
    "text": "Schl√ºsselphrase plus Kontext\nEinsatz von Keywords-in-Context (KWIC) zur Qualit√§tskontrolle\n\n\ntoks_transcripts %&gt;% \n  kwic(\n    phrase(\"know know\"),\n    window = 3) %&gt;%\n  tibble() %&gt;% \n  select(-pattern) %&gt;% \n  slice(35:45) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(\n        table.width = gt::pct(100), \n        table.font.size = \"10px\"\n    )\n\n\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\n\n\n\n\nvp2_s0723\n94\n95\nkiss just kiss\nknow know\njust kiss kiss\n\n\nvp2_s0732\n119\n120\ndefault press even\nknow know\ndifference campaign strategy\n\n\nvp3_s0151\n32\n33\ncop able assess\nknow know\nJ.D Vance lying\n\n\nvp3_s0332\n3\n4\nreally mean\nknow know\nmany people tune\n\n\nvp3_s0332\n116\n117\ngenerous Sekunden Pause\nknow know\ntype like know\n\n\nvp3_s0332\n120\n121\nknow type like\nknow know\ntype like know\n\n\nvp3_s0332\n124\n125\nknow type like\nknow know\nknow know know\n\n\nvp3_s0332\n125\n126\ntype like know\nknow know\nknow know know\n\n\nvp3_s0332\n126\n127\nlike know know\nknow know\nknow know know\n\n\nvp3_s0332\n127\n128\nknow know know\nknow know\nknow know know\n\n\nvp3_s0332\n128\n129\nknow know know\nknow know\nknow know know"
  },
  {
    "objectID": "slides/slides-08.html#ngrams-als-features-definieren",
    "href": "slides/slides-08.html#ngrams-als-features-definieren",
    "title": "üî® Advanced Methods",
    "section": "Ngrams als Features definieren",
    "text": "Ngrams als Features definieren\nSteigerung der Datenqualit√§t durch Ber√ºcksichtigung von Ngrams-Features\n\n# Definition von Features\ncustom_ngrams &lt;- c(\"donald trump\", \"joe biden\", \"kamala harris\")\n\n# Anwendung auf DFM\ndfm_with_custom_ngrams &lt;- toks_transcripts %&gt;% \n  tokens_compound(pattern = phrase(custom_ngrams)) %&gt;% \n  dfm() %&gt;% \n  dfm_trim(min_docfreq = 0.005, max_docfreq = 0.99, docfreq_type = \"prop\") \n\n# √úberpr√ºfung\ndfm_with_custom_ngrams %&gt;% \n  convert(to = \"data.frame\") %&gt;% \n  select(doc_id, starts_with(\"donald\")) %&gt;% \n  head()\n\n    doc_id donald_trump donald\n1 p1_s0001            1      0\n2 p1_s0002            1      0\n3 p1_s0003            0      0\n4 p1_s0004            0      0\n5 p1_s0005            0      0\n6 p1_s0006            1      0"
  },
  {
    "objectID": "slides/slides-08.html#h√§ufige-zusammen-verwendete-emotes",
    "href": "slides/slides-08.html#h√§ufige-zusammen-verwendete-emotes",
    "title": "üî® Advanced Methods",
    "section": "H√§ufige zusammen verwendete Emotes",
    "text": "H√§ufige zusammen verwendete Emotes\nSemantische Netzwerke: Visualisierung von Tokenbeziehungen\n\n\n# Lookup emotes in DFM of chats\ndfm_emotes &lt;- dfm_chats %&gt;% \n  quanteda::dfm_lookup(\n    dictionary = dict_chat_emotes)\n\n# Output frequency of emojis\ntop50_emotes &lt;- dfm_emotes %&gt;% \n  topfeatures(50) %&gt;% \n  names()\n\n# Visualize\ndfm_emotes  %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top50_emotes) %&gt;% \n  textplot_network()"
  },
  {
    "objectID": "slides/slides-08.html#ber√ºcksichtigung-der-syntax",
    "href": "slides/slides-08.html#ber√ºcksichtigung-der-syntax",
    "title": "üî® Advanced Methods",
    "section": "Ber√ºcksichtigung der Syntax",
    "text": "Ber√ºcksichtigung der Syntax\nPart-of-Speech Tagging: Hintergrund & Anwendungsbeispiele\n\n(Jurafsky & Martin, 2024, p. S.366)\n‚Äúprocess of assigning a part-of-speech to each word in a text‚Äù (Jurafsky & Martin, 2024, p. S.365)\n\n\nBeispiele f√ºr Anwendungsf√§lle:\n\nanalysieren, ob es sich bei einem Feature um ein Adjektiv handelt, das sich auf ein bestimmtes Substantiv bezieht\nzwischen gleichen Features mit unterschiedlichen Bedeutungen unterscheiden (‚ÄûSound solution‚Äú vs.¬†‚ÄûWhat is that sound‚Äú?)"
  },
  {
    "objectID": "slides/slides-08.html#dependency-parsing",
    "href": "slides/slides-08.html#dependency-parsing",
    "title": "üî® Advanced Methods",
    "section": "Dependency Parsing",
    "text": "Dependency Parsing\nHintegrund und Anwendungsf√§lle\n\n\n‚Äúthe syntactic structure of a sentence [‚Ä¶] in terms of directed binary grammatical relations between the words‚Äù (Jurafsky & Martin, 2024, p. S.411)\n\n\nBeispiele f√ºr Anwendungsf√§lle:\n\nanalysieren, ob es sich bei einem Feature um ein Adjektiv handelt, das sich auf ein bestimmtes Substantiv bezieht\nzwischen gleichen Features mit unterschiedlichen Bedeutungen unterscheiden (‚ÄûSound solution‚Äú vs.¬†‚ÄûWhat is that sound‚Äú?)"
  },
  {
    "objectID": "slides/slides-08.html#praktische-umsetzung-mit-udpipe-in-r",
    "href": "slides/slides-08.html#praktische-umsetzung-mit-udpipe-in-r",
    "title": "üî® Advanced Methods",
    "section": "Praktische Umsetzung mit udpipe in R",
    "text": "Praktische Umsetzung mit udpipe in R\nBeispiele f√ºr POS-Tagging & Dependency Parsing\n\nudmodel &lt;- udpipe::udpipe_download_model(language = \"english\")\n\ntranscripts_pos &lt;- transcripts %&gt;%\n  rename(doc_id = id_sequence, text = dialogue) %&gt;% \n  udpipe::udpipe(udmodel)\n\n\ntranscripts_pos %&gt;% \n  select(doc_id, sentence_id, token_id, token, head_token_id, lemma, upos, xpos) %&gt;% \n  head(n = 7) %&gt;% \n  gt() %&gt;% gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(table.width = gt::pct(100), table.font.size = \"12px\")\n\n\n\n\n\n\n\ndoc_id\nsentence_id\ntoken_id\ntoken\nhead_token_id\nlemma\nupos\nxpos\n\n\n\n\np1_s0001\n1\n1\nTonight\n0\ntonight\nNOUN\nNN\n\n\np1_s0001\n1\n2\n,\n1\n,\nPUNCT\n,\n\n\np1_s0001\n1\n3\nthe\n7\nthe\nDET\nDT\n\n\np1_s0001\n1\n4\nhigh\n6\nhigh\nADJ\nJJ\n\n\np1_s0001\n1\n5\n-\n6\n-\nPUNCT\nHYPH\n\n\np1_s0001\n1\n6\nstakes\n7\nstake\nNOUN\nNNS\n\n\np1_s0001\n1\n7\nshowdown\n1\nshowdown\nNOUN\nNN"
  },
  {
    "objectID": "slides/slides-08.html#mit-welchen-w√∂rtern-wird-trump-beschrieben",
    "href": "slides/slides-08.html#mit-welchen-w√∂rtern-wird-trump-beschrieben",
    "title": "üî® Advanced Methods",
    "section": "Mit welchen W√∂rtern wird Trump beschrieben?",
    "text": "Mit welchen W√∂rtern wird Trump beschrieben?\nAnwendung & Probleme von POS-Tagging\n\n\ntranscripts_pos %&gt;% \n    filter(\n      upos == \"NOUN\" &\n      lemma == \"trump\") %&gt;%\n    inner_join(\n      transcripts_pos,\n      by = c(\n        \"doc_id\",\n        \"sentence_id\"),\n      relationship = \n        \"many-to-many\") %&gt;%\n    filter(\n      upos.y == \"ADJ\" &\n      head_token_id.y == token_id.x) %&gt;% \n    rename(\n      token_id = token_id.y,\n      token = token.y) %&gt;% \n    select(\n      doc_id, sentence_id,\n      token_id, token) %&gt;%\n    sjmisc::frq(token, sort.frq = \"desc\") \n\n\ntoken &lt;character&gt; \n# total N=161 valid N=161 mean=3.72 sd=4.67\n\nValue        |   N | Raw % | Valid % | Cum. %\n---------------------------------------------\ndonald       | 132 | 81.99 |   81.99 |  81.99\nDonald       |   4 |  2.48 |    2.48 |  84.47\num           |   3 |  1.86 |    1.86 |  86.34\nformer       |   2 |  1.24 |    1.24 |  87.58\nnarcissistic |   2 |  1.24 |    1.24 |  88.82\nbad          |   1 |  0.62 |    0.62 |  89.44\ngood         |   1 |  0.62 |    0.62 |  90.06\ngreat        |   1 |  0.62 |    0.62 |  90.68\niran         |   1 |  0.62 |    0.62 |  91.30\nlaura        |   1 |  0.62 |    0.62 |  91.93\nmuch         |   1 |  0.62 |    0.62 |  92.55\nokay         |   1 |  0.62 |    0.62 |  93.17\nother        |   1 |  0.62 |    0.62 |  93.79\npast         |   1 |  0.62 |    0.62 |  94.41\nSaid         |   1 |  0.62 |    0.62 |  95.03\nselfish      |   1 |  0.62 |    0.62 |  95.65\nsocial       |   1 |  0.62 |    0.62 |  96.27\ntighter      |   1 |  0.62 |    0.62 |  96.89\ntotal        |   1 |  0.62 |    0.62 |  97.52\nunfit        |   1 |  0.62 |    0.62 |  98.14\nunseat       |   1 |  0.62 |    0.62 |  98.76\nweaker       |   1 |  0.62 |    0.62 |  99.38\nweird        |   1 |  0.62 |    0.62 | 100.00\n&lt;NA&gt;         |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;"
  },
  {
    "objectID": "slides/slides-08.html#besser-mit-spacyr-aber-noch-nicht-gut",
    "href": "slides/slides-08.html#besser-mit-spacyr-aber-noch-nicht-gut",
    "title": "üî® Advanced Methods",
    "section": "Besser mit spacyr, aber noch nicht gut",
    "text": "Besser mit spacyr, aber noch nicht gut\nAnwendung & Auswertung von POS-Tagging\n\n\ntranscripts_spacyr %&gt;%  \n    filter(\n      pos == \"NOUN\" &\n      lemma == \"trump\") %&gt;%\n    inner_join(\n      transcripts_spacyr,\n      by = c(\n        \"doc_id\",\n        \"sentence_id\"),\n      relationship = \n        \"many-to-many\") %&gt;%\n    filter(\n      pos.y == \"ADJ\" &\n      head_token_id.y == token_id.x) %&gt;% \n    rename(\n      token_id = token_id.y,\n      token = token.y) %&gt;% \n    select(\n      doc_id, sentence_id,\n      token_id, token) %&gt;%\n    sjmisc::frq(token, sort.frq = \"desc\") \n\n\ntoken &lt;character&gt; \n# total N=10 valid N=10 mean=5.40 sd=2.88\n\nValue        | N | Raw % | Valid % | Cum. %\n-------------------------------------------\nunfit        | 2 |    20 |      20 |     20\nbad          | 1 |    10 |      10 |     30\ndonald       | 1 |    10 |      10 |     40\nfucking      | 1 |    10 |      10 |     50\nnarcissistic | 1 |    10 |      10 |     60\nother        | 1 |    10 |      10 |     70\nsame         | 1 |    10 |      10 |     80\ntighter      | 1 |    10 |      10 |     90\ntotal        | 1 |    10 |      10 |    100\n&lt;NA&gt;         | 0 |     0 |    &lt;NA&gt; |   &lt;NA&gt;"
  },
  {
    "objectID": "slides/slides-08.html#welche-neue-daten-stehen-zur-verf√ºgung",
    "href": "slides/slides-08.html#welche-neue-daten-stehen-zur-verf√ºgung",
    "title": "üî® Advanced Methods",
    "section": "Welche neue Daten stehen zur Verf√ºgung?",
    "text": "Welche neue Daten stehen zur Verf√ºgung?\n√úberblick √ºber die neuen Datens√§tze\nSowohl f√ºr die Chats & Transkripte werden mehrere Korpora hinzugef√ºgt\n\nDatensatz ...-corpus_udpipe.qs enth√§lt einen mit dem Paket udpipe (v0.8.11, Wijffels, 2023) verarbeiteten Datensatz\nDatensatz ...-corpus_spacy.qsenth√§lt einen mit dem Paket spacyr (v1.3.0, Benoit & Matsuo, 2023) verarbeiteten Datensatz\n\nCode f√ºr die Erstellung der Datens√§tze in der Sektion ‚ÄúData colletion‚Äù auf der Homepage"
  },
  {
    "objectID": "slides/slides-08.html#quick-overview",
    "href": "slides/slides-08.html#quick-overview",
    "title": "üî® Advanced Methods",
    "section": "Quick overview",
    "text": "Quick overview\nudpipe-Korpus\n\n# Corpus processed with udpipe\ntranscripts_udpipe %&gt;% glimpse\n\nRows: 596,711\nColumns: 17\n$ doc_id        &lt;chr&gt; \"p1_s0001\", \"p1_s0001\", \"p1_s0001\", \"p1_s0001\", \"p1_s000‚Ä¶\n$ paragraph_id  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n$ sentence_id   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n$ sentence      &lt;chr&gt; \"Tonight, the high-stakes showdown here in Philadelphia ‚Ä¶\n$ start         &lt;int&gt; 1, 8, 10, 14, 18, 19, 26, 35, 40, 43, 56, 64, 69, 79, 86‚Ä¶\n$ end           &lt;int&gt; 7, 8, 12, 17, 18, 24, 33, 38, 41, 54, 62, 67, 77, 84, 91‚Ä¶\n$ term_id       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1‚Ä¶\n$ token_id      &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\",‚Ä¶\n$ token         &lt;chr&gt; \"Tonight\", \",\", \"the\", \"high\", \"-\", \"stakes\", \"showdown\"‚Ä¶\n$ lemma         &lt;chr&gt; \"tonight\", \",\", \"the\", \"high\", \"-\", \"stake\", \"showdown\",‚Ä¶\n$ upos          &lt;chr&gt; \"NOUN\", \"PUNCT\", \"DET\", \"ADJ\", \"PUNCT\", \"NOUN\", \"NOUN\", ‚Ä¶\n$ xpos          &lt;chr&gt; \"NN\", \",\", \"DT\", \"JJ\", \"HYPH\", \"NNS\", \"NN\", \"RB\", \"IN\", ‚Ä¶\n$ feats         &lt;chr&gt; \"Number=Sing\", NA, \"Definite=Def|PronType=Art\", \"Degree=‚Ä¶\n$ head_token_id &lt;chr&gt; \"0\", \"1\", \"7\", \"6\", \"6\", \"7\", \"1\", \"7\", \"13\", \"13\", \"13\"‚Ä¶\n$ dep_rel       &lt;chr&gt; \"root\", \"punct\", \"det\", \"amod\", \"punct\", \"compound\", \"ap‚Ä¶\n$ deps          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n$ misc          &lt;chr&gt; \"SpaceAfter=No\", NA, NA, \"SpaceAfter=No\", \"SpaceAfter=No‚Ä¶"
  },
  {
    "objectID": "slides/slides-08.html#quick-overview-1",
    "href": "slides/slides-08.html#quick-overview-1",
    "title": "üî® Advanced Methods",
    "section": "Quick overview",
    "text": "Quick overview\nspacyr-Korpus\n\n# Corpus processed with spacyr\ntranscripts_spacyr %&gt;% glimpse\n\nRows: 596,923\nColumns: 10\n$ doc_id        &lt;chr&gt; \"p1_s0001\", \"p1_s0001\", \"p1_s0001\", \"p1_s0001\", \"p1_s000‚Ä¶\n$ sentence_id   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n$ token_id      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1‚Ä¶\n$ token         &lt;chr&gt; \"Tonight\", \",\", \"the\", \"high\", \"-\", \"stakes\", \"showdown\"‚Ä¶\n$ lemma         &lt;chr&gt; \"tonight\", \",\", \"the\", \"high\", \"-\", \"stake\", \"showdown\",‚Ä¶\n$ pos           &lt;chr&gt; \"NOUN\", \"PUNCT\", \"DET\", \"ADJ\", \"PUNCT\", \"NOUN\", \"NOUN\", ‚Ä¶\n$ tag           &lt;chr&gt; \"NN\", \",\", \"DT\", \"JJ\", \"HYPH\", \"NNS\", \"NN\", \"RB\", \"IN\", ‚Ä¶\n$ head_token_id &lt;dbl&gt; 7, 7, 7, 6, 6, 7, 7, 7, 8, 9, 7, 13, 15, 15, 11, 15, 18,‚Ä¶\n$ dep_rel       &lt;chr&gt; \"npadvmod\", \"punct\", \"det\", \"amod\", \"punct\", \"compound\",‚Ä¶\n$ entity        &lt;chr&gt; \"TIME_B\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"GPE_B\", \"\", \"‚Ä¶"
  },
  {
    "objectID": "slides/slides-08.html#and-now-you",
    "href": "slides/slides-08.html#and-now-you",
    "title": "üî® Advanced Methods",
    "section": "üß™ And now ‚Ä¶ you!",
    "text": "üß™ And now ‚Ä¶ you!\nNext steps\n\nLaden das .zip-Archiv zur Sitzung von StudOn herunter und entpacke die Dateien an einen Ort deiner Wahl.\nDoppelklicke auf die Datei dbd_exercise.Rproj, um das RStudio-Projekt zu √∂ffnen. Dies stellt sicher, dass alle Abh√§ngigkeiten korrekt funktionieren.\n√ñffnen die Datei exercise_08.qmd und folge den Anweisungen.\nTipp: Alle im Vortrag verwendeten Code-Schnipsel findest du im der Tutorial-Datei zur Sitzung."
  },
  {
    "objectID": "slides/slides-08.html#references",
    "href": "slides/slides-08.html#references",
    "title": "üî® Advanced Methods",
    "section": "References",
    "text": "References\n\n\nArendt, F., & Karadas, N. (2017). Content analysis of mediated associations: An automated text-analytic approach. Communication Methods and Measures, 11(2), 105‚Äì120. https://doi.org/10.1080/19312458.2016.1276894\n\n\nBenoit, K., & Matsuo, A. (2023). Spacyr: Wrapper to the ‚ÄôspaCy‚Äô ‚ÄôNLP‚Äô library. https://spacyr.quanteda.io\n\n\nBlei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. The Journal of Machine Learning Research, 3, 9931022.\n\n\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding (J. Burstein, C. Doran, & T. Solorio, Eds.; p. 41714186). Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1423\n\n\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267‚Äì297. https://doi.org/10/f458q9\n\n\nHutto, C., & Gilbert, E. (2014). VADER: A parsimonious rule-based model for sentiment analysis of social media text. Proceedings of the International AAAI Conference on Web and Social Media, 8(1), 216‚Äì225. https://doi.org/10.1609/icwsm.v8i1.14550\n\n\nJurafsky, D., & Martin, J. H. (2024). Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition with language models (3rd ed.). https://web.stanford.edu/~jurafsky/slp3/\n\n\nLe, Q., & Mikolov, T. (2014). Distributed representations of sentences and documents (E. P. Xing & T. Jebara, Eds.; Vol. 32, p. 11881196). PMLR. https://proceedings.mlr.press/v32/le14.html\n\n\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., H√§ussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93‚Äì118. https://doi.org/10.1080/19312458.2018.1430754\n\n\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality (C. J. Burges, L. Bottou, M. Welling, Z. Ghahramani, & K. Q. Weinberger, Eds.; Vol. 26). Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n\n\nNicholls, T. (2019). Detecting Textual Reuse in News Stories, At Scale. International Journal of Communication, 13(0), 25. https://ijoc.org/index.php/ijoc/article/view/9904\n\n\nPennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word representation. 15321543. https://doi.org/10.3115/v1/D14-1162\n\n\nRoberts, M. E., Stewart, B. M., & Airoldi, E. M. (2016). A model of text for experimentation in the social sciences. Journal of the American Statistical Association, 111(515), 988‚Äì1003. https://doi.org/10/f88tzh\n\n\nRoberts, M. E., Stewart, B. M., & Tingley, D. (2019). stm: An R Package for Structural Topic Models. Journal of Statistical Software, 91(1), 1‚Äì40. https://doi.org/10.18637/jss.v091.i02\n\n\nRuigrok, N., & Atteveldt, W. van. (2007). Global Angling with a Local Angle: How U.S., British, and Dutch Newspapers Frame Global and Local Terrorist Attacks. Harvard International Journal of Press/Politics, 12(1), 68‚Äì90. https://doi.org/10.1177/1081180X06297436\n\n\nTausczik, Y. R., & Pennebaker, J. W. (2009). The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods. Journal of Language and Social Psychology, 29(1), 24‚Äì54. https://doi.org/10.1177/0261927x09351676\n\n\nWijffels, J. (2023). Udpipe: Tokenization, parts of speech tagging, lemmatization and dependency parsing with the ‚ÄôUDPipe‚Äô ‚ÄôNLP‚Äô toolkit. https://CRAN.R-project.org/package=udpipe"
  },
  {
    "objectID": "slides/slides-02.html#kurzes-update",
    "href": "slides/slides-02.html#kurzes-update",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Kurzes Update",
    "text": "Kurzes Update\nAllgemeine Infos zum Kurs\n\nHaben alle sich f√ºr die Pr√ºfung angemeldet? Gibt es noch Fragen zum Sonderanmeldetermin?\nHaben alle eine Benachrichtung f√ºr den Post im  StudOn-Forum bekommen?\nHaben alle die üìñ Basisliteratur gefunden? Gibt es Fragen?\nPr√§sentationsgruppe 1&2: Denkt bitte an die Zusendung des Entwurf der Pr√§sentationsfolien (bis n√§chsten Dienstag bis 12:00) und das Feedbackgespr√§ch n√§chste Woche!"
  },
  {
    "objectID": "slides/slides-02.html#finale-themenvergabe",
    "href": "slides/slides-02.html#finale-themenvergabe",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Finale Themenvergabe",
    "text": "Finale Themenvergabe\n√úberblick √ºber die Gruppenverteilung\n\n\n\n\n\n\n\n\n\nGruppe\nThema\nStudierende\n\n\n\n\n1\nMotivation der Nutzung von Twitch\nAzat, Heimst√§dt\n\n\n2\nKommunikation und Interaktion auf Twitch\nBurmeister, Fischer, Erdogmus\n\n\n3\n(Wirkungs-)Effekte der Twitch-Nutzung/Interaktion\nDierking, Reineke\n\n\n4\n(Wirkungs-)Effekte von TV-Wahldebatten\nSpickenreuther, Wolf\n\n\n5\nWechselwirkung zwischen TV-Debatten und Twitter\nGierth, Landgraf\n\n\n6\nLive-Chat(-Kommentare) in politischen Debatten\nMach, Stadler, Wei√ü"
  },
  {
    "objectID": "slides/slides-02.html#semsterplan",
    "href": "slides/slides-02.html#semsterplan",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Semsterplan",
    "text": "Semsterplan\n\n\n\n\n\n\n\n\nSession\nDatum\nTopic\nPresenter\n\n\n\n\n\nüìÇ Block 1\nIntroduction\n\n\n\n1\n23.10.2024\nKick-Off\nChristoph Adrian\n\n\n2\n30.10.2024\nDBD: Overview & Introduction\nChristoph Adrian\n\n\n3\n06.11.2024\nüî® Introduction to working with R\nChristoph Adrian\n\n\n\nüìÇ Block 2\nTheoretical Background: Twitch & TV Election Debates\n\n\n\n4\n13.11.2024\nüìö Twitch-Nutzung im Fokus\nStudent groups\n\n\n5\n20.11.2024\nüìö (Wirkungs-)Effekte von Twitch & TV-Debatten\nStudent groups\n\n\n6\n27.11.2024\nüìö Politische Debatten & Social Media\nStudent groups\n\n\n\nüìÇ Block 3\nMethod: Natural Language Processing\n\n\n\n7\n04.12.2024\nüî® Text as data I: Introduction\nChristoph Adrian\n\n\n8\n11.12.2024\nüî® Text as data I: Advanced Methods\nChristoph Adrian\n\n\n9\n18.12.2024\nüî® Advanced Method I: Topic Modeling\nChristoph Adrian\n\n\n\nNo lecture\nüéÑChristmas Break\n\n\n\n10\n08.01.2025\nüî® Advanced Method I: Machine Learning\nChristoph Adrian\n\n\n\nüìÇ Block 4\nProject Work\n\n\n\n11\n15.01.2025\nüî® Project work\nStudent groups\n\n\n12\n22.01.2025\nüî® Project work\nStudent groups\n\n\n13\n29.01.2025\nüìä Project Presentation I\nStudent groups (TBD)\n\n\n14\n05.02.2025\nüìä Project Presentation & üèÅ Evaluation\nStudentds (TBD) & Christoph Adrian"
  },
  {
    "objectID": "slides/slides-02.html#was-ist-das-eigentlich",
    "href": "slides/slides-02.html#was-ist-das-eigentlich",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Was ist das eigentlich?",
    "text": "Was ist das eigentlich?\nR√ºckblick auf einen Definitionversuch von Weller (2021)\n\n\n‚Ä¶ fasst eine Vielzahl von m√∂glichen Datenquellen zusammen, die verschiedene Arten von Aktivit√§ten aufzeichnen (h√§ufig sogar ‚Äúnur‚Äù als Nebenprodukt)\n‚Ä¶ k√∂nnen dabei helfen, Meinungen, Verhalten und Merkmale der menschlichen Nutzung digitaler Technologien zu erkennen"
  },
  {
    "objectID": "slides/slides-02.html#und-im-kontext-des-seminars",
    "href": "slides/slides-02.html#und-im-kontext-des-seminars",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Und im Kontext des Seminars?",
    "text": "Und im Kontext des Seminars?\nArbeitsdefinition & Kernbereiche (GESIS) von DBD\n\n\n\nDBD umfasst digitale Beobachtungen menschlichen und algorithmischen Verhaltens,\nwie sie z.B. von Online-Plattformen (wie Google, Facebook oder dem World Wide Web) oder\nSensoren (wie Smartphones, RFID-Sensoren, Satelliten oder Street View-Kameras) erfasst werden."
  },
  {
    "objectID": "slides/slides-02.html#die-power-von-social-sensing",
    "href": "slides/slides-02.html#die-power-von-social-sensing",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Die Power von Social Sensing",
    "text": "Die Power von Social Sensing\nForschungsdesign zur Erhebung digitaler Verhaltensdaten (Fl√∂ck & Sen, 2022)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Zukunft: Linking"
  },
  {
    "objectID": "slides/slides-02.html#mit-fokus-auf-die-platform",
    "href": "slides/slides-02.html#mit-fokus-auf-die-platform",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Mit Fokus auf die Platform",
    "text": "Mit Fokus auf die Platform\nForschungsdesign zur Erhebung digitaler Verhaltensdaten (Fl√∂ck & Sen, 2022)"
  },
  {
    "objectID": "slides/slides-02.html#online-plattformen-pr√§gen-die-gesellschaft",
    "href": "slides/slides-02.html#online-plattformen-pr√§gen-die-gesellschaft",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Online-Plattformen pr√§gen die Gesellschaft",
    "text": "Online-Plattformen pr√§gen die Gesellschaft\nGr√ºnde f√ºr den Fokus auf Onlineplattformen (Ulloa, 2021)\n\n\nvermitteln & formen menschliche Kommunikation (z.B. Tweet mit 280 Zeichen)\npolitische (Miss-)Nutzung\nGatekeeper f√ºr Informationen (z.B. ‚ÄúDr.Google‚Äù)\nt√§gliche algorithmische Empfehlungen und Werbung: Nachrichten, Produkte, Jobangebote, Bewerbungen, Versicherungen, Hotels, ‚Ä¶\n\n\nABER: Ber√ºcksichtigung der Art und Weise, wie die Daten gesammelt werden!"
  },
  {
    "objectID": "slides/slides-02.html#eine-kleine-lobeshymne-auf-dbd",
    "href": "slides/slides-02.html#eine-kleine-lobeshymne-auf-dbd",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Eine kleine Lobeshymne auf DBD",
    "text": "Eine kleine Lobeshymne auf DBD\nZwischenfazit\n\nDigitale Ger√§te oder Sensoren k√∂nnen sich an bestimmte Fakten besser ‚Äúerinnern‚Äù als das menschliche Ged√§chtnis.\nSensoren sind oft bereits in allt√§gliche Technologie eingebaut und produzieren digitale Verhaltensdaten als ein ‚ÄúNebenprodukt‚Äù.\nUnaufdringliche Erfassung als potentieller Vorteil bzw. Entlastung f√ºr Teilnehmer*Innen\nKombination mit Umfragedaten m√∂glich (und bereichernd!)\n\n\nAber: Ber√ºcksichtigung der Rahmenbedingungen!\nZur erfolgreichen Nutzung m√ºssen Forschungsziele & verf√ºgbare Daten in Einklang gebracht, m√∂gliche Biases und methodische Probleme ber√ºcksichtigt sowie die Datenqualit√§t evaluiert werden.\n\n\nBietet die Plattform Zugang zu den ben√∂tigten Daten? Wenn nicht, gibt es alternative Weg um an die Daten zu gelangen? Wenn ja, ist dies legal/ethisch?"
  },
  {
    "objectID": "slides/slides-02.html#wenn-der-vorteil-zum-nachteil-wird",
    "href": "slides/slides-02.html#wenn-der-vorteil-zum-nachteil-wird",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Wenn der Vorteil zum Nachteil wird",
    "text": "Wenn der Vorteil zum Nachteil wird\nAmbivalenz der Unaufdringlichkeit (Engel et al., 2021)\n\nUnterscheidung zwischen aufdringlichen (z.B. spezielle Research-App & Befragungen) & unaufdringlichen (z.B. Cookies, Browserplugins & APIs) erhobenen Daten\nBewertung und Erwartung an Datensammlung ist abh√§ngig vom Kontext (z.B. Amazon vs.¬†Researchgate)\n\n\nParadoxes Dilemma\nEinerseits bereitwillige (oft unwissende) Abgabe der Daten an Konzerne ohne Wissen um deren Weiterverarbeitung, andererseits h√§ufig Bedenken bez√ºglich Datenschutz & Privatsph√§re bei wissenschaftlichen Studien, die √ºber Verwendung der Daten aufkl√§ren.\n\n\nWarum? Pers√∂nlicher Nutzen?"
  },
  {
    "objectID": "slides/slides-02.html#eher-konzept-als-begriff",
    "href": "slides/slides-02.html#eher-konzept-als-begriff",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Eher Konzept als Begriff",
    "text": "Eher Konzept als Begriff\nZur Ambigutit√§t des Begriffes bias und dessen Bedeutung im Seminar\n\n\nProblem: keine klare Grenzen zwischen den eher normativen Konnotationen (z.B. confirmation bias) und der statistischen Bedeutung des Begriffs (z.B. selection bias)\nDeswegen: Bewusstsein f√ºr Ambiguit√§t des Begriffes\n\nVerwendung in vielen Disziplinen wie der Sozialwissenschaft, der kognitiven Psychologie oder dem Recht\nUntersuchung von verschiedenen Ph√§nomenen, wie kognitive Voreingenommenheiten (Croskerry, 2002) sowie systemische, diskriminierende Ergebnisse (Friedman & Nissenbaum, 1996) oder Sch√§den (Barocas & Selbst, 2016), aktuell z.B. bei der Verwendung von Machine Learning oder AI.\n\n\n\n\n\n\nVerwendung des Begriff haupts√§chlich in seiner statistischen Bedeutung, um auf Verzerrungen in sozialen Daten und deren Analysen hinzuweisen."
  },
  {
    "objectID": "slides/slides-02.html#know-your-bias",
    "href": "slides/slides-02.html#know-your-bias",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Know your bias!",
    "text": "Know your bias!\nFramework zur Minimierung von Fehlern und Problemen (Olteanu et al., 2019)\n\n\nBeschreibung:\n\nDie Analyse sozialer Daten beginnt mit bestimmten Zielen (Abschnitt 2.1), wie dem Verst√§ndnis oder der Beeinflussung von Ph√§nomenen, die f√ºr soziale Plattformen spezifisch sind (Typ I) und/oder von Ph√§nomenen, die √ºber soziale Plattformen hinausgehen (Typ II).\nDiese Ziele erfordern, dass die Forschung bestimmte Validit√§tskriterien erf√ºllt, die weiter oben beschrieben wurden (Abschnitt 2.2).\nDiese Kriterien k√∂nnen ihrerseits durch eine Reihe von allgemeinen Verzerrungen und Problemen beeintr√§chtigt werden (Abschnitt 3).\nDiese Herausforderungen k√∂nnen von den Merkmalen der einzelnen Datenplattformen (Abschnitt 4) abh√§ngen - die oft nicht unter der Kontrolle der Forschenden stehen - und von den Entscheidungen des Forschungsdesigns entlang einer Datenverarbeitungspipeline (Abschnitte 5 bis 8) - die oft unter der Kontrolle des Forschers stehen.\nPfeile zeigen an, wie sich Komponenten in unserem Rahmenwerk direkt auf andere auswirken"
  },
  {
    "objectID": "slides/slides-02.html#the-biggest-problem-of-them-all",
    "href": "slides/slides-02.html#the-biggest-problem-of-them-all",
    "title": "√úberblick & Einf√ºhrung",
    "section": "The biggest problem of them all",
    "text": "The biggest problem of them all\nPotentielle Probleme mit der Qualit√§t der Daten\n\n\n\nDefinition Data bias (Olteanu et al., 2019)\nA systematic distortion in the sampled data that compromises its representativeness.\n\n\n\n\nSparsity: H√§ufig Heavy-Tail-Verteilung, was Analyse am ‚ÄúKopf‚Äù (in Bezug auf h√§ufige Elemente oder Ph√§nomene) erleichtert, am ‚ÄúSchwanz‚Äù (wie seltene Elemente oder Ph√§nomene) jedoch erschwert (Baeza-Yates, 2013)\nNoise: Unvollst√§ndige, besch√§digte, unzuverl√§ssige oder unglaubw√ºrdige Inhalte (boyd & Crawford, 2012; Naveed et al., 2011)\n\nAber: Unterscheidung von ‚ÄúNoise‚Äù und ‚ÄúSignal‚Äù ist oft unklar und h√§ngt von der Forschungsfrage ab (Salganik, 2018)\n\nOrganische vs gemessene Daten: Fragen zur Repr√§sentativit√§t (vs.¬†Stichprobenbeschreibung), Kausalit√§t (vs.¬†Korrelation) und Vorhersageg√ºte"
  },
  {
    "objectID": "slides/slides-02.html#bias-at-the-source",
    "href": "slides/slides-02.html#bias-at-the-source",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Bias at the source",
    "text": "Bias at the source\nPotentielle Probleme mit der Datenquelle oder -herkunft\n\nBiases, die auf das Design und die M√∂glichkeiten der Plattformen zur√ºckzuf√ºhren sind (functional biases).\nVerhaltensnormen, die auf den einzelnen Plattformen bestehen oder sich herausbilden (normative biases).\nFaktoren, die au√üerhalb der sozialen Plattformen liegen, aber das Nutzerverhalten beeinflussen k√∂nnen (external biases)\nVorhandensein von nicht-individuellen Konten ein (non-individuals).\n\n\nfunctional biases:\n- Platform-specific design and features shape user behavior (z.B. Emojis) - Algorithms used for organizing and ranking content influence user behavior - Content presentation influences user behavior (z.B. UI)\nnormative biases:\n\nNorms are shaped by the attitudes and behaviors of online communities, which may be context-dependent (z.B. Partyfotos auf Instagram, aber nicht LinkedIn)\nThe awareness of being observed by others impacts user behavio (Anonymit√§t vs Klarnamen)\nSocial conformity and ‚Äúherding‚Äù happen in social platforms, and such behavioral traits shape user behavior (z.B. Ratings beinflussen eigenes Rating)\n\nexternal biase:\n\nCultural elements and social contexts are reflected in social datasets. (Zeichenlimit Japan vs.¬†Deutschland)\nMisinformation and disinformation.\nContents on different topics are treated differently.\nHigh-impact events, whether anticipated or not, are reflected on social media (z.B. Feiertage)\n\nnon-individual-accounts: Organizational accounts, Bots"
  },
  {
    "objectID": "slides/slides-02.html#gefangen-im-spannungsverh√§ltnis",
    "href": "slides/slides-02.html#gefangen-im-spannungsverh√§ltnis",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Gefangen im Spannungsverh√§ltnis",
    "text": "Gefangen im Spannungsverh√§ltnis\nForschungethik bei digitalen Daten\nHintergrund: Die Herausforderung besteht in der Kombination von zwei extremen Sichtweisen, der Betrachtung der Forschung mit sozialen Daten als ‚Äúklinische‚Äù Forschung oder als Computerforschung\n\nDie Sozialdatenforschung unterscheidet sich von klinischen Versuchen.\nEthische Entscheidungen in der Sozialdatenforschung m√ºssen gut √ºberlegt sein, da oft sind mehrere Werte betroffen, die miteinander in Konflikt stehen k√∂nnen\nDiskussion des Spannungsverh√§ltnisses am Beispiel von drei spezifischer ethischer Kriterien: Autonomie, Wohlt√§tigkeit und Gerechtigkeit\n\n\nHintergrund:\n\nDie Sozialdatenforschung √§hnelt klinischen Versuchen und anderen Experimenten am Menschen in ihrer F√§higkeit, Menschen zu schaden, und sollte daher auch als solche reguliert werden\ndie Sozialdatenforschung √§hnelt der sonstigen Computerforschung, die sich traditionell auf Methoden, Algorithmen und den Aufbau von Systemen konzentriert, mit minimalen direkten Auswirkungen auf Menschen.\n\nPunkt 2: Sch√§den, die die √ºblichen Arten der Sozialdatenforschung ( z. B. die Verletzung der Privatsph√§re oder der Anblick verst√∂render Bilder)verursachen k√∂nnen, oft nicht mit Sch√§den von klinischen Versuchen gleichzusetzen\nPunkt 3: Datenanalyse beispielsweise erforderlich sein, um wichtige Dienste bereitzustellen, und es sollten L√∂sungen erwogen werden, die ein Gleichgewicht zwischen Datenschutz und Genauigkeit herstellen (Goroff, 2015)."
  },
  {
    "objectID": "slides/slides-02.html#achtung-der-individuellen-autonomie",
    "href": "slides/slides-02.html#achtung-der-individuellen-autonomie",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Achtung der individuellen Autonomie",
    "text": "Achtung der individuellen Autonomie\nDiskussion der Informierte Zustimmung als Indikator autonomer Entscheidung\n\n\n\n\n\n\nEinwilligung nach Aufkl√§rung setzt voraus, dass\n\n\n\n\ndie Forscher*Innen den potenziellen Teilnehmenden alle relevanten Informationen offenlegen;\ndie potenziellen Teilnehmenden in der Lage sind, diese Informationen zu bewerten;\ndie potenziellen Teilnehmenden freiwillig entscheiden k√∂nnen, ob sie teilnehmen wollen oder nicht;\ndie Teilnehmenden den Forschernden ihre ausdr√ºckliche Erlaubnis erteilen, h√§ufig in schriftlicher Form; und\ndie Teilnehmende die M√∂glichkeit haben, ihre Einwilligung jederzeit zur√ºckzuziehen.\n\n\n\n\n\nPotentielle Probleme mit Blick auf DBD\n\nDie Zustimmung von Millionen von Nutzern einzuholen ist nicht praktikabel.\nDie Nutzungsbedingungen sozialer Plattformen stellen m√∂glicherweise keine informierte Zustimmung zur Forschung dar.\nDas √∂ffentliche Teilen von Inhalten im Internet bedeutet nicht unbedingt eine Zustimmung zur Forschung."
  },
  {
    "objectID": "slides/slides-02.html#no-no-yes",
    "href": "slides/slides-02.html#no-no-yes",
    "title": "√úberblick & Einf√ºhrung",
    "section": "No ‚ÄúNo‚Äù ‚â† ‚ÄúYes‚Äù!",
    "text": "No ‚ÄúNo‚Äù ‚â† ‚ÄúYes‚Äù!\nEthische Erw√§gungen bei DBD-Forschung\n\nAus √∂ffentlicher Zug√§nglich- bzw. Verf√ºgbarkeit von Daten leitet sich nicht automatisch ethische Verwertbarkeit ab (boyd & Crawford, 2012; Zimmer, 2010)\n\nVerletzung der Privatsph√§re der Nutzer (Goroff, 2015)\nErm√∂glichung von rassischem, sozio√∂konomischem oder geschlechtsspezifischem Profiling (Barocas & Selbst, 2016)\n\nNegative Beispiele\n\nFacebook contagion experiment (2012-2014): Feeds von Nutzer*Innen so manipulierten, dass sie je nach den ge√§u√üerten Emotionen mehr oder weniger von bestimmten Inhalten enthielten (Kramer et al., 2014)\nEncore-Forschungsprojekt: Messung der Internetzensur auf der ganzen Welt, bei der Webbrowser angewiesen wurden, zu versuchen, sensible Webinhalte ohne das Wissen oder die Zustimmung der Nutzer herunterzuladen (Burnett & Feamster, 2014)\n\n\n\nHintergrund:\n\nEthische Fragen bisher epistemische Bedenken (Verwendung von nicht schl√ºssigen oder fehlgeleiteten Beweisen), jetzt normativ Bedenken (Folgen der Forschung)\nForschung grunds√§tzlich in vielen L√§ndern gesetztlich geregelt\n\nNegativbeispiele:\n\nFacebook contagion experiment: Das Experiment wurde als ein Eingriff kritisiert, der den emotionalen Zustand von ahnungslosen Nutzern beeinflusste, die keine Zustimmung zur Teilnahme an der Studie gegeben hatten (Hutton und Henderson, 2015a).\nEncore-Forschngsprojekt: Menschen in einigen L√§ndern durch diese Zugriffsversuche m√∂glicherweise gef√§hrdet wurden\n\nFolgende Abschnitte:\n\nzentrales Spannungsverh√§ltnis in der Forschungsethik digitaler Daten dargestellt.\nAnschlie√üend wird die Diskussion spezifischer ethischer Probleme in der Sozialdatenforschung im Hinblick auf drei grundlegende Kriterien gegliedert, die im Belmont-Bericht (Ryan et al., 1978), einem grundlegenden Werk zur Forschungsethik, vorgebracht wurden: Autonomie (Abschnitt 9.2), Wohlt√§tigkeit (Abschnitt 9.3) und Gerechtigkeit (Abschnitt 9.4)."
  },
  {
    "objectID": "slides/slides-02.html#wohlt√§tigkeit-und-unsch√§dlichkeit-als-ziel",
    "href": "slides/slides-02.html#wohlt√§tigkeit-und-unsch√§dlichkeit-als-ziel",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Wohlt√§tigkeit und Unsch√§dlichkeit als Ziel",
    "text": "Wohlt√§tigkeit und Unsch√§dlichkeit als Ziel\nBewertung von Risken & Nutzen\nHintergrund: Nicht nur Fokus auf den Nutzen der Forschung, sondern auch auf die m√∂glichen Arten von Sch√§den, die betroffenen Gruppen und die Art und Weise, wie nachteilige Auswirkungen getestet werden k√∂nnen . (Sweeney, 2013)\n\nPotentielle Probleme\n\nDaten √ºber Einzelpersonen k√∂nnen ihnen schaden, wenn sie offengelegt werden.\nForschungsergebnisse k√∂nnen verwendet werden, um Schaden anzurichten.\n‚ÄúDual-Use‚Äù- und Sekund√§ranalysen sind in der Sozialdatenforschung immer h√§ufiger anzutreffen.\n\n\nDie Forschung zu sozialen Daten wird mit bestimmten Arten von Sch√§den in Verbindung gebracht, von denen die Verletzung der Privatsph√§re vielleicht die offensichtlichste ist (Zimmer, 2010; Crawford und Finn, 2014).\nBeispiel 1: Einige prominente Beispiele sind die Datenpanne bei Ashley Madison im Jahr 2015, bei der einer Website, die sich als Dating-Netzwerk f√ºr betr√ºgerische Ehepartner anpreist, Kontoinformationen (einschlie√ülich der vollst√§ndigen Namen der Nutzer) gestohlen und online gestellt wurden (Thomsen, 2015), sowie die j√ºngsten Datenpannen bei Facebook, bei denen Hunderte Millionen von Datens√§tzen mit Kommentaren, Likes, Reaktionen, Kontonamen, App-Passw√∂rtern und mehr √∂ffentlich gemacht wurden.\nzu 1: - Stalking, Diskriminierung, Erpressung oder Identit√§tsdiebstahl (Gross und Acquisti, 2005). - Zu lange Archivierung personenbezogener Daten oder die √∂ffentliche Freigabe schlecht anonymisierter Datens√§tze kann zu Verletzungen der Privatsph√§re f√ºhren, da diese Daten mit anderen Quellen kombiniert werden k√∂nnen, um Erkenntnisse √ºber Personen ohne deren Wissen zu gewinnen (Crawford und Finn, 2014; Goroff, 2015; Horvitz und Mulligan, 2015)\nzu 2: Abgesehen von der Tatsache, dass aus sozialen Daten gezogene R√ºckschl√ºsse in vielerlei Hinsicht falsch sein k√∂nnen, wie in dieser Studie hervorgehoben wird, k√∂nnen zu pr√§zise R√ºckschl√ºsse dazu f√ºhren, dass Menschen in immer kleinere Gruppen eingeteilt werden k√∂nnen (Barocas, 2014).\nzu 3: Daten, Instrumente und Schlussfolgerungen, die f√ºr einen bestimmten Zweck gewonnen wurden, f√ºr einen anderen Zweck verwendet werden (Hovy und Spruit, 2016; Benton et al., 2017)"
  },
  {
    "objectID": "slides/slides-02.html#faire-verteilung-von-risiken-nutzen",
    "href": "slides/slides-02.html#faire-verteilung-von-risiken-nutzen",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Faire Verteilung von Risiken & Nutzen",
    "text": "Faire Verteilung von Risiken & Nutzen\nRecht & Gerechtigkeit\nHintergrund: H√§ufig wird unterstellt bzw. angenommen, dass es von Anfang an bekannt, wer durch die Forschung belastet und wer von den Ergebnissen profitieren wird.\n\nPotentielle Probleme\n\nDie digitale Kluft kann das Forschungsdesign beeinflussen (z.B. WEIRD Samples)\nAlgorithmen und Forschungsergebnisse k√∂nnen zu Diskriminierung f√ºhren.\nForschungsergebnisse sind m√∂glicherweise nicht allgemein zug√§nglich.\nNicht alle Interessengruppen werden √ºber die Verwendung von Forschungsergebnissen konsultiert.\n\n\nzu 1: Data divide: mangelnde Verf√ºgbarkeit von hochwertigen Daten √ºber Entwicklungsl√§nder und unterprivilegierte Gemeinschaften (Cinnamon und Schuurman, 2013). WEIRD = White, Educated, Industrialized, Rich, and Democratic\nzu 3: Idealerweise sollten die Menschen Zugang zu den Forschungsergebnissen und Artefakten haben, die aus der Untersuchung ihrer pers√∂nlichen Daten entstanden sind (Gross und Acquisti, 2005; Crawford und Finn, 2014).\nzu 4: In die √úberlegungen dar√ºber, wie, f√ºr wen und wann Forschungsergebnisse umgesetzt werden, sollten diejenigen einbezogen werden, die m√∂glicherweise betroffen sind oder deren Daten verwendet werden (Costanza-Chock, 2018; Design Justice, 2018; Green, 2018)"
  },
  {
    "objectID": "slides/slides-02.html#zwei-trends-drei-fragen-vier-empfehlungen",
    "href": "slides/slides-02.html#zwei-trends-drei-fragen-vier-empfehlungen",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Zwei Trends, Drei Fragen, Vier Empfehlungen",
    "text": "Zwei Trends, Drei Fragen, Vier Empfehlungen\nZusammenfassung und Ausblick\nTrend 1: Skepsis gegen√ºber einfachen Antworten\n\n\nWie einstehen die Daten, was enthalten sie tats√§chlich und wie sind die Arbeitsdatens√§tze zusammengestellt?\nWird deutlich, was ausgewertet wird?\nWird die Verwendung von vorhandenen Datens√§tzen und Modellen des maschinellen Lernens hinterfragt?\n\n\nTrend 2: Wechsel von der Thematisierung zur Adressieung von Bedenken\n\n\nDetaillierte Dokumentation und kritische Pr√ºfung der Datensatz- und Modellerstellung\nDBD-Studien auf verschiedene Plattformen, Themen, Zeitpunkte und Teilpopulationen auszuweiten, um festzustellen, wie sich die Ergebnisse beispielsweise in verschiedenen kulturellen, demografischen und verhaltensbezogenen Kontexten unterscheiden\nTransparenzmechanismen zu schaffen, die es erm√∂glichen, Online-Plattformen zu √ºberpr√ºfen und Verzerrungen in Daten an der Quelle zu evaluieren\nForschung zu diesen Leitlinien, Standards, Methoden und Protokollen auszuweiten und ihre √úbernahme zu f√∂rdern.\n\n\n\nSchlie√ülich gibt es angesichts der Komplexit√§t der inh√§rent kontextabh√§ngigen, anwendungs- und bereichsabh√§ngigen Verzerrungen und Probleme in sozialen Daten und Analysepipelines, die in diesem Papier behandelt werden, keine Einheitsl√∂sungen - bei der Bewertung und Bek√§mpfung von Verzerrungen ist Nuancierung entscheidend."
  },
  {
    "objectID": "slides/slides-02.html#der-weg-bestimmt-das-ergebnis",
    "href": "slides/slides-02.html#der-weg-bestimmt-das-ergebnis",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Der Weg bestimmt das Ergebnis",
    "text": "Der Weg bestimmt das Ergebnis\nEinfluss der Erhebung auf die Daten(-form) (Davidson et al., 2023)"
  },
  {
    "objectID": "slides/slides-02.html#data-download-packages",
    "href": "slides/slides-02.html#data-download-packages",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Data Download Packages",
    "text": "Data Download Packages\nBeispiel f√ºr Data Donations (Driel et al., 2022)\n\n\n\nGeneral Data Protection Regulation erlaubt die von einer Plattform gespeicherten personenbezogenen Daten in einem strukturierten, allgemein gebr√§uchlichen und maschienenlesbaren Format (Data Download Package) anzufordern.\nNutzer:innen k√∂nnen Forschenden diese Daten spenden, verbunden mit der M√∂glichkeit, bestimmte Daten (heraus) zu filtern."
  },
  {
    "objectID": "slides/slides-02.html#screenomics-software",
    "href": "slides/slides-02.html#screenomics-software",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Screenomics software",
    "text": "Screenomics software\nBeispiel f√ºr Tracking (Reeves et al., 2021)\n\n\n\nErfassung: Alle f√ºnf Sekunden, in denen digitale Ger√§te aktiviert sind, werden Screenshots erstellt, verschl√ºssel, komprimiert & an einen Forschungsserver √ºbertragen\nVerarbeitung: Screenomics (App) erkennt und segmentiert Text, Gesichter, Logos und Objekte auf den Screenshots"
  },
  {
    "objectID": "slides/slides-02.html#zeeschuimer-plugin",
    "href": "slides/slides-02.html#zeeschuimer-plugin",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Zeeschuimer Plugin",
    "text": "Zeeschuimer Plugin\nBeispiel f√ºr Scraping (Peeters, 2022)\n\n\n\nBrowsererweiterung, die w√§hrend des Besuchs einer Social-Media-Website Daten √ºber die Elemente sammelt, die in der Weboberfl√§che einer Plattform zu sehen sind\nDerzeit werden die unter anderem , ,  &  unterst√ºtzt\nErg√§nzung zu 4CAT (Peeters & Hagen, 2022), einem Tool zur Analyse und Verarbeitung von Daten aus Online-Plattformen"
  },
  {
    "objectID": "slides/slides-02.html#and-now-you",
    "href": "slides/slides-02.html#and-now-you",
    "title": "√úberblick & Einf√ºhrung",
    "section": "And now ‚Ä¶ you!",
    "text": "And now ‚Ä¶ you!\nGruppenarbeit (ca. 15 Minuten) mit kurzer Ergebnisvorstellung (ca. 15 Min)\n\n\n\n\n\n\nArbeitsauftrag\n\n\nStellt euch vor, Ihr wollt eine der drei vorgestellten Methoden nutzen, um ein Forschungsprojekt durchzuf√ºhren.\n\nWas sind m√∂gliche Biases an der Quelle der Daten, die ihr bei der Methode ber√ºcksichtigen m√ºsst?\nWelche ethischen und rechtlichen Fragen ergeben sich aus der Nutzung der Methode?\n\n\n\n\n\n\n\n\n\n\nN√§chste Schritte\n\n\n\nEs gibt f√ºr jede Methode (Data Donation, Tracking & Scraping) eine Gruppe. Ihr k√∂nnt selber aussuchen, in welche Gruppe ihr m√∂chtet.\nSchreibt eure Ergebnisse in die daf√ºr bereitgestellet Folienvorlage (auf der n√§chsten Slide)."
  },
  {
    "objectID": "slides/slides-02.html#please-discuss",
    "href": "slides/slides-02.html#please-discuss",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Please discuss!",
    "text": "Please discuss!\nBitte nutzt die jeweilige Folienvorlage f√ºr die Dokumentation euerer Ergebnisse\n\n\n\n\n\n     Data Donations\n\n\n\n\n\n     Tracking\n\n\n\n\n\n     Scraping\n\n\n\n\n‚àí+\n15:00"
  },
  {
    "objectID": "slides/slides-02.html#literatur",
    "href": "slides/slides-02.html#literatur",
    "title": "√úberblick & Einf√ºhrung",
    "section": "Literatur",
    "text": "Literatur\n\n\nBaeza-Yates, R. A. (2013). Big data or right data.\n\n\nBarocas, S., & Selbst, A. D. (2016). Big Data‚Äôs Disparate Impact. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2477899\n\n\nboyd, danah m., & Crawford, K. (2012). Critical questions for big data: Provocations for a cultural, technological, and scholarly phenomenon. Information, Communication & Society, 15(5), 662‚Äì679. https://doi.org/10.1080/1369118X.2012.678878\n\n\nBurnett, S., & Feamster, N. (2014). Encore: Lightweight measurement of web censorship with cross-origin requests. https://doi.org/10.48550/ARXIV.1410.1211\n\n\nCroskerry, P. (2002). Achieving Quality in Clinical Decision Making: Cognitive Strategies and Detection of Bias. Academic Emergency Medicine, 9(11), 1184‚Äì1204. https://doi.org/10.1197/aemj.9.11.1184\n\n\nDavidson, B. I., Wischerath, D., Racek, D., Parry, D. A., Godwin, E., Hinds, J., Van Der Linden, D., Roscoe, J. F., Ayravainen, L. E. M., & Cork, A. (2023). Platform-controlled social media APIs threaten open science. https://osf.io/ps32z\n\n\nDriel, I. I. van, Giachanou, A., Pouwels, J. L., Boeschoten, L., Beyens, I., & Valkenburg, P. M. (2022). Promises and Pitfalls of Social Media Data Donations. Communication Methods and Measures, 1‚Äì17. https://doi.org/10.1080/19312458.2022.2109608\n\n\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Digital trace data (1st ed., pp. 100‚Äì118). Routledge. https://www.taylorfrancis.com/books/9781003024583/chapters/10.4324/9781003024583-8\n\n\nFl√∂ck, F., & Sen, I. (2022). Digital traces of human behaviour in online platforms  research design and error sources. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meet_the_experts_Digitaltraces_humanbehaviour.pdf\n\n\nFriedman, B., & Nissenbaum, H. (1996). Bias in computer systems. ACM Transactions on Information Systems, 14(3), 330‚Äì347. https://doi.org/10.1145/230538.230561\n\n\nGoroff, D. L. (2015). Balancing privacy versus accuracy in research protocols. Science, 347(6221), 479‚Äì480. https://doi.org/10.1126/science.aaa3483\n\n\nKramer, A. D. I., Guillory, J. E., & Hancock, J. T. (2014). Experimental evidence of massive-scale emotional contagion through social networks. Proceedings of the National Academy of Sciences, 111(24), 8788‚Äì8790. https://doi.org/10.1073/pnas.1320040111\n\n\nNaveed, N., Gottron, T., Kunegis, J., & Alhadi, A. C. (2011). the 20th ACM international conference. 183. https://doi.org/10.1145/2063576.2063607\n\n\nOlteanu, A., Castillo, C., Diaz, F., & Kƒ±cƒ±man, E. (2019). Social data: Biases, methodological pitfalls, and ethical boundaries. Frontiers in Big Data, 2, 13. https://doi.org/10.3389/fdata.2019.00013\n\n\nPeeters, S. (2022). Zeeschuimer. Zenodo. https://zenodo.org/record/6826877\n\n\nPeeters, S., & Hagen, S. (2022). The 4CAT Capture and Analysis Toolkit: A Modular Tool for Transparent and Traceable Social Media Research. Computational Communication Research, 4(2), 571‚Äì589. https://doi.org/10.5117/ccr2022.2.007.hage\n\n\nReeves, B., Ram, N., Robinson, T. N., Cummings, J. J., Giles, C. L., Pan, J., Chiatti, A., Cho, M., Roehrick, K., Yang, X., Gagneja, A., Brinberg, M., Muise, D., Lu, Y., Luo, M., Fitzgerald, A., & Yeykelis, L. (2021). Screenomics : A Framework to Capture and Analyze Personal Life Experiences and the Ways that Technology Shapes Them. HumanComputer Interaction, 36(2), 150‚Äì201. https://doi.org/10.1080/07370024.2019.1578652\n\n\nSalganik, M. J. (2018). Bit by bit: Social research in the digital age. Princeton University Press.\n\n\nSweeney, L. (2013). Discrimination in Online Ad Delivery: Google ads, black names and white names, racial discrimination, and click advertising. Queue, 11(3), 10‚Äì29. https://doi.org/10.1145/2460276.2460278\n\n\nUlloa, R. (2021). Introduction to online data acquisition. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meettheexperts_Introductioncss.pdf\n\n\nWeller, K. (2021). A short introduction to computational social science and digital behavioral data. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meettheexperts_Introductioncss.pdf\n\n\nZimmer, M. (2010). ‚ÄúBut the data is already public‚Äù: on the ethics of research in Facebook. Ethics and Information Technology, 12(4), 313‚Äì325. https://doi.org/10.1007/s10676-010-9227-5"
  },
  {
    "objectID": "slides/slides-10.html#seminarplan",
    "href": "slides/slides-10.html#seminarplan",
    "title": "üî® Sentiment Analysis",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\nSession\nDatum\nTopic\nPresenter\n\n\n\n\n\nüìÇ Block 1\nIntroduction\n\n\n\n1\n23.10.2024\nKick-Off\nChristoph Adrian\n\n\n2\n30.10.2024\nDBD: Overview & Introduction\nChristoph Adrian\n\n\n3\n06.11.2024\nüî® Introduction to working with R\nChristoph Adrian\n\n\n\nüìÇ Block 2\nTheoretical Background: Twitch & TV Election Debates\n\n\n\n4\n13.11.2024\nüìö Twitch-Nutzung im Fokus\nStudent groups\n\n\n5\n20.11.2024\nüìö (Wirkungs-)Effekte von Twitch & TV-Debatten\nStudent groups\n\n\n6\n27.11.2024\nüìö Politische Debatten & Social Media\nStudent groups\n\n\n\nüìÇ Block 3\nMethod: Natural Language Processing\n\n\n\n7\n04.12.2024\nüî® Text as data I: Introduction\nChristoph Adrian\n\n\n8\n11.12.2024\nüî® Text as data II: Advanced Methods\nChristoph Adrian\n\n\n9\n18.12.2024\nüî® Advanced Method I: Topic Modeling\nChristoph Adrian\n\n\n\nNo lecture\nüéÑChristmas Break\n\n\n\n10\n08.01.2025\nüî® Advanced Method II: Machine Learning\nChristoph Adrian\n\n\n\nüìÇ Block 4\nProject Work\n\n\n\n11\n15.01.2025\nüî® Project work\nStudent groups\n\n\n12\n22.01.2025\nüî® Project work\nStudent groups\n\n\n13\n29.01.2025\nüìä Project Presentation I\nStudent groups (TBD)\n\n\n14\n05.02.2025\nüìä Project Presentation & üèÅ Evaluation\nStudentds (TBD) & Christoph Adrian"
  },
  {
    "objectID": "slides/slides-10.html#kurze-rekapitulation",
    "href": "slides/slides-10.html#kurze-rekapitulation",
    "title": "üî® Sentiment Analysis",
    "section": "Kurze Rekapitulation",
    "text": "Kurze Rekapitulation\nGrundidee & verschiedene Umsetzungsm√∂glichkeiten einer Sentimentanalyse\n\nAnwendung von Natural Language Processing (NLP), Textanalyse und Computational Linguistics, um subjektive Informationen aus Texten zu extrahieren bzw. Meinung, Einstellung oder Emotionen zu bestimmten Themen oder Entit√§ten zu bestimmen\nVerschiede Methoden:\n\nRegelbasierte Ans√§tze (Dictionaries)\nMaschinelles Lernen & Deep Learning\nLLMs (& KI)"
  },
  {
    "objectID": "slides/slides-10.html#ein-spektrum-an-m√∂glichkeiten",
    "href": "slides/slides-10.html#ein-spektrum-an-m√∂glichkeiten",
    "title": "üî® Sentiment Analysis",
    "section": "Ein Spektrum an M√∂glichkeiten",
    "text": "Ein Spektrum an M√∂glichkeiten\nBeispiele f√ºr verschiedene Dictionaries & Pakete zur Umsetzung einer Sentimentanalyse\n\n\nDefinition eines eigenen, ‚Äúorganischen‚Äù Dictionaries vs.¬†Off-the-shelf, wie z.B.\n\nLexicoder Sentiment Dictionary (Young & Soroka, 2012) ‚ûú Das W√∂rterbuch besteht aus 2.858 ‚Äúnegativen‚Äù und 1.709 ‚Äúpositiven‚Äù Sentiment-W√∂rtern sowie 2.860 und 1.721 Negationen von negativen bzw. positiven W√∂rtern.\nAFINN (Nielsen, 2011) ‚ûú Bewertung von W√∂rtern mit Sentiment-Werten von -5 (negativ) bis +5 (positiv)\nValence Aware Dictionary and sEntiment Reasoner (Hutto & Gilbert, 2014) ‚ûú Sentiment-Tool, dass zus√§tzlich den Kontext der W√∂rter mit ber√ºcksichtigt und einen Score zwischen -1 (negativ) und +1 (positiv) berechnet\n\nPraktische Umsetzung mit quanteda (v4.1.0, Benoit et al., 2018) bzw. quanteda.sentiment [v0.31] oder vader [v0.2.1, Roehrick (n.d.)]"
  },
  {
    "objectID": "slides/slides-10.html#erweiterung-des-quantedaverse",
    "href": "slides/slides-10.html#erweiterung-des-quantedaverse",
    "title": "üî® Sentiment Analysis",
    "section": "Erweiterung des quantedaverse",
    "text": "Erweiterung des quantedaverse\nVorstellung von quanteda.sentiment bzw. enthaltenen Funktionen & Diktion√§re\n\nquanteda.sentiment erweitert das quanteda Paket um Funktionen zur Berechnung von Sentiment in Texten. Es bietet zwei Hauptfunktionen:\n\ntextstat_polarity() ‚ûú Sentiment basierend auf positiven und negativen W√∂rtern (z.B. mit Lexicoder Sentiment Dictionary).\nBeispiel in Bezug auf politische Diskussionen: ‚ÄûWar der Ton der Diskussion positiv oder negativ?‚Äú\ntextstat_valence() ‚ûú Sentiment als Durchschnitt der Valenzwerte der W√∂rter in einem Dokument (z.B. AFINN).\nBeispiel in Bezug auf politische Diskussionen: ‚ÄûWie intensiv haben die Teilnehmer:innen ihre Emotionen ausgedr√ºckt?‚Äú"
  },
  {
    "objectID": "slides/slides-10.html#unterschied-zwischen-polarit√§t-valenz",
    "href": "slides/slides-10.html#unterschied-zwischen-polarit√§t-valenz",
    "title": "üî® Sentiment Analysis",
    "section": "Unterschied zwischen Polarit√§t & Valenz",
    "text": "Unterschied zwischen Polarit√§t & Valenz\nPraktische Anwedung von quanteda.sentiment\n\n\n\nchats_polarity &lt;- corp_chats %&gt;% \n  textstat_polarity(\n    dictionary = data_dictionary_LSD2015) %&gt;% \n  rename(polarity = sentiment)\n\n\nchats_polarity %&gt;% \n    head(n = 10)\n\n                                 doc_id  polarity\n1  dc03b89a-722d-4eaa-a895-736533a68aca  0.000000\n2  6be50e12-2fd5-436f-b253-b2358b618380  0.000000\n3  f5e41904-7f01-4f03-ad6c-2c0f07d70ed0  1.098612\n4  92dc6519-eb54-4c18-abef-27201314b22f -1.098612\n5  92055088-7067-48c0-aa11-9c6103bdf4c4  0.000000\n6  03ad4706-aa67-4ddc-a1e4-6f8ca981778e  0.000000\n7  00c5dd9c-41b8-4430-8b2e-be67c5e363ac  0.000000\n8  923c7eac-d92e-4cac-876a-07d4fa45cb57  0.000000\n9  6bdfb03d-fdbd-48b6-9b81-2fc56785fd67 -1.098612\n10 7f25fc9f-b000-41fa-91b8-60672cd3e608  0.000000\n\n\n\n\nchats_valence &lt;- corp_chats %&gt;% \n  textstat_valence(\n    dictionary = data_dictionary_AFINN) %&gt;% \n  rename(valence = sentiment)\n\n\nchats_valence %&gt;% \n    head(n = 10)\n\n                                 doc_id valence\n1  dc03b89a-722d-4eaa-a895-736533a68aca       0\n2  6be50e12-2fd5-436f-b253-b2358b618380       0\n3  f5e41904-7f01-4f03-ad6c-2c0f07d70ed0       0\n4  92dc6519-eb54-4c18-abef-27201314b22f      -5\n5  92055088-7067-48c0-aa11-9c6103bdf4c4       0\n6  03ad4706-aa67-4ddc-a1e4-6f8ca981778e       0\n7  00c5dd9c-41b8-4430-8b2e-be67c5e363ac       0\n8  923c7eac-d92e-4cac-876a-07d4fa45cb57       0\n9  6bdfb03d-fdbd-48b6-9b81-2fc56785fd67       0\n10 7f25fc9f-b000-41fa-91b8-60672cd3e608       0"
  },
  {
    "objectID": "slides/slides-10.html#was-vader-anders-macht",
    "href": "slides/slides-10.html#was-vader-anders-macht",
    "title": "üî® Sentiment Analysis",
    "section": "Was VADER anders macht",
    "text": "Was VADER anders macht\nHintergrundinformationen zu VADER (Hutto & Gilbert, 2014)\n\nBer√ºcksichtigt Valenzverschiebungen mit Kontextbewusstsein\n\nNegationen (z.B. ‚Äûnicht gut‚Äú ist weniger positiv als ‚Äûgut‚Äú).\nIntensit√§tsmodifikatoren (z.B. ‚Äûsehr gut‚Äú ist positiver als ‚Äûgut‚Äú).\nKontrastierende Konjunktionen (z.B. ‚Äûaber‚Äú signalisiert einen Stimmungswechsel: ‚Äûgut, aber nicht gro√üartig‚Äú).\n\nBer√ºcksichtigt Interpunktion (z.B. ‚ÄûErstaunlich!!!‚Äú ist positiver als ‚ÄûErstaunlich‚Äú) und Gro√üschreibung (z.B. ‚ÄûERSTAUNLICH‚Äú hat ein st√§rkeres Sentiment als ‚Äûerstaunlich‚Äú)\nHandhabt Slang, Emojis und internet-spezifische Sprache (z.B. ‚ÄûLOL‚Äú, ‚Äû:)‚Äú, oder ‚Äûomg‚Äú)"
  },
  {
    "objectID": "slides/slides-10.html#mehr-als-nur-ein-score",
    "href": "slides/slides-10.html#mehr-als-nur-ein-score",
    "title": "üî® Sentiment Analysis",
    "section": "Mehr als nur ein Score",
    "text": "Mehr als nur ein Score\nVorstellung der Funktion vader [v0.2.1, Roehrick (n.d.)] inklusive Output\n\nget_vader()‚ûú Exportiert folgende Metriken:\n\nWortbezogene Sentiment-Scores: Jedes Wort erh√§lt einen Sentiment-Score, der basierend auf Faktoren wie Interpunktion und Gro√üschreibung angepasst wird.\nGesamtwert (Compound score): Ein einzelner Wert, der das Gesamtsentiment des gesamten Satzes zusammenfasst.\nPositive (pos), neutrale (neu) und negative (neg) Scores: Repr√§sentieren den Prozentsatz der W√∂rter, die in jede Sentiment-Kategorie fallen.\nBut count: Z√§hlt das Vorkommen des Wortes ‚Äûaber‚Äú, was auf m√∂gliche Stimmungswechsel innerhalb des Satzes hinweist."
  },
  {
    "objectID": "slides/slides-10.html#erstellung-transformation-des-vader-outputs",
    "href": "slides/slides-10.html#erstellung-transformation-des-vader-outputs",
    "title": "üî® Sentiment Analysis",
    "section": "Erstellung & Transformation des VADER Outputs",
    "text": "Erstellung & Transformation des VADER Outputs\nPraktische Anwedung von vader\n\nchats_vader &lt;- chats %&gt;% \n  mutate(\n    # Estimate sentiment scores\n    vader_output = map(dialogue, ~vader::get_vader(.x)), \n    # Extract word-level scores\n    word_scores = map(vader_output, ~ .x[\n        names(.x) != \"compound\" &\n        names(.x) != \"pos\" & \n        names(.x) != \"neu\" & \n        names(.x) != \"neg\" & \n        names(.x) != \"but_count\"]),  \n    compound = map_dbl(vader_output, ~ as.numeric(.x[\"compound\"])),\n    pos = map_dbl(vader_output, ~ as.numeric(.x[\"pos\"])),\n    neu = map_dbl(vader_output, ~ as.numeric(.x[\"neu\"])),\n    neg = map_dbl(vader_output, ~ as.numeric(.x[\"neg\"])),\n    but_count = map_dbl(vader_output, ~ as.numeric(.x[\"but_count\"]))\n  )"
  },
  {
    "objectID": "slides/slides-10.html#ein-blick-auf-das-ergebnis",
    "href": "slides/slides-10.html#ein-blick-auf-das-ergebnis",
    "title": "üî® Sentiment Analysis",
    "section": "Ein Blick auf das Ergebnis",
    "text": "Ein Blick auf das Ergebnis\nPraktische Anwedung von vader\n\nchats_vader %&gt;% \n    select(message_id, compound:but_count) %&gt;% \n    head(n = 20)\n\n# A tibble: 20 √ó 6\n   message_id                           compound   pos   neu   neg but_count\n   &lt;chr&gt;                                   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 dc03b89a-722d-4eaa-a895-736533a68aca    0         0 1     0             0\n 2 6be50e12-2fd5-436f-b253-b2358b618380    0         0 1     0             0\n 3 f5e41904-7f01-4f03-ad6c-2c0f07d70ed0    0         0 1     0             0\n 4 92dc6519-eb54-4c18-abef-27201314b22f   -0.586     0 0.513 0.487         0\n 5 92055088-7067-48c0-aa11-9c6103bdf4c4    0         0 1     0             0\n 6 03ad4706-aa67-4ddc-a1e4-6f8ca981778e    0         0 1     0             0\n 7 00c5dd9c-41b8-4430-8b2e-be67c5e363ac    0         0 1     0             0\n 8 923c7eac-d92e-4cac-876a-07d4fa45cb57    0         0 1     0             0\n 9 6bdfb03d-fdbd-48b6-9b81-2fc56785fd67    0         0 1     0             0\n10 7f25fc9f-b000-41fa-91b8-60672cd3e608    0         0 1     0             0\n11 a00e2ca8-2e76-4941-b360-b6b311701cba    0         0 1     0             0\n12 637e5e96-9f26-4a87-955e-74f2fb29685a    0         0 1     0             0\n13 4b0a6fbe-54d6-4d06-8d08-875112abcd92    0         0 1     0             0\n14 cf57874e-a239-4bce-a766-4bb7636847b7    0         0 1     0             0\n15 51b66d60-0f6b-43a6-a40c-cb6d51cde1a9    0         0 1     0             0\n16 08d7ae3c-1180-4e26-940e-de763fbe6f18    0         0 1     0             0\n17 72494412-fe24-44ad-9a02-de22e8e54724    0         0 1     0             0\n18 93a9da3e-63ab-4eea-bb51-73bff8dadf13    0         0 1     0             0\n19 3aa667c1-a8b1-4f18-94a6-920b8a9ee37b    0         0 1     0             0\n20 daebee85-4885-48f2-8086-9b9172285792   -0.586     0 0.513 0.487         0"
  },
  {
    "objectID": "slides/slides-10.html#kombinieren-vergleichen",
    "href": "slides/slides-10.html#kombinieren-vergleichen",
    "title": "üî® Sentiment Analysis",
    "section": "Kombinieren & vergleichen",
    "text": "Kombinieren & vergleichen\nZusammenf√ºhrung der einzelnen Dictionary-Sentiments mit den Stammdaten\n\nchats_sentiment &lt;- chats %&gt;% \n    left_join(chats_polarity, by = join_by(\"message_id\" == \"doc_id\")) %&gt;%\n    left_join(chats_valence, by = join_by(\"message_id\" == \"doc_id\")) %&gt;% \n    left_join(chats_vader %&gt;% \n        select(message_id, vader_output, word_scores, compound, pos, neu, neg, but_count), \n        by = \"message_id\")\n\n\nchats_sentiment %&gt;% \n    select(message_id, polarity, valence, compound) %&gt;%\n    datawizard::describe_distribution()\n\nVariable |  Mean |   SD | IQR |         Range | Skewness | Kurtosis |      n | n_Missing\n----------------------------------------------------------------------------------------\npolarity | -0.06 | 0.65 |   0 | [-4.44, 3.61] |    -0.18 |     1.30 | 913375 |         0\nvalence  | -0.04 | 1.38 |   0 | [-5.00, 5.00] |    -0.21 |     2.54 | 913375 |         0\ncompound |  0.01 | 0.30 |   0 | [-1.00, 1.00] |    -0.12 |     1.11 | 913170 |       205"
  },
  {
    "objectID": "slides/slides-10.html#neutralit√§t-dominiert",
    "href": "slides/slides-10.html#neutralit√§t-dominiert",
    "title": "üî® Sentiment Analysis",
    "section": "Neutralit√§t dominiert",
    "text": "Neutralit√§t dominiert\nVergleich der Verteilungsfunktionen der verschiedenen Sentiments\n\n\nExpand for full code\nchats_sentiment %&gt;%\n    pivot_longer(cols = c(polarity, valence, compound), names_to = \"sentiment_type\", values_to = \"sentiment_value\") %&gt;%\n    ggplot(aes(x = sentiment_value, fill = sentiment_type)) +\n    geom_density(alpha = 0.5) +\n    facet_wrap(~ sentiment_type, scales = \"free\") +\n    labs(\n        title = \"Density Plot of Polarity, Valence, and Compound Sentiment\"\n    ) +\n    theme_pubr() +\n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "slides/slides-10.html#sentiment-scores-einer-nachricht",
    "href": "slides/slides-10.html#sentiment-scores-einer-nachricht",
    "title": "üî® Sentiment Analysis",
    "section": "Sentiment Scores einer Nachricht",
    "text": "Sentiment Scores einer Nachricht\nPraktische Anwendung des compound scores\n\n\nExpand for full code\nchats_vader_sample &lt;- chats_vader %&gt;%\n    filter(message_length &lt; 100) %&gt;%\n    slice_sample(n = 10) \n\nchats_vader_sample %&gt;%\n    ggplot(aes(x = message_content, y = compound, fill = compound &gt; 0)) +\n        geom_bar(stat = \"identity\", width = 0.7) +\n        scale_fill_manual(values = c(\"TRUE\" = \"blue\", \"FALSE\" = \"red\"), labels = c(\"Positive\", \"Negative\")) +\n        labs(\n            title = \"Overall Compound Sentiment for Each Sentence\",\n            x = \"Sentences\",\n            y = \"Compound Sentiment\",\n            fill = \"Sentiment\") +\n        coord_flip() +  # Flip for easier readability\n        theme_minimal() +\n        theme(\n            axis.text.x = element_text(angle = 45, hjust = 1))  # Label wrapping and adjusting angle"
  },
  {
    "objectID": "slides/slides-10.html#anteil-an-positiven-neutralen-und-negativen-w√∂rtern",
    "href": "slides/slides-10.html#anteil-an-positiven-neutralen-und-negativen-w√∂rtern",
    "title": "üî® Sentiment Analysis",
    "section": "Anteil an positiven, neutralen und negativen W√∂rtern",
    "text": "Anteil an positiven, neutralen und negativen W√∂rtern\nPraktische Anwendung der Word-Level Scores\n\n\nExpand for full code\nchats_vader_sample %&gt;% \n    mutate(\n        pos_pct = pos * 100,\n        neu_pct = neu * 100,\n        neg_pct = neg * 100) %&gt;% \n  select(message_content, pos_pct, neu_pct, neg_pct) %&gt;% \n  pivot_longer(\n    cols = c(pos_pct, neu_pct, neg_pct),\n    names_to = \"sentiment\",\n    values_to = \"percentage\") %&gt;% \n  mutate(\n    sentiment = factor(\n        sentiment,\n        levels = c(\"pos_pct\", \"neu_pct\", \"neg_pct\"),\n        labels = c(\"Positive\", \"Neutral\", \"Negative\"))) %&gt;% \n  ggplot(aes(x = message_content, y = percentage, fill = sentiment)) +\n    geom_bar(stat = \"identity\", width = 0.7) +\n    scale_fill_manual(values = c(\"Positive\" = \"blue\", \"Neutral\" = \"gray\", \"Negative\" = \"red\")) +\n    labs(\n        title = \"Proportion of Positive, Neutral, and Negative Sentiment\",\n        x = \"Sentences\",\n        y = \"Percentage\",\n        fill = \"Sentiment\") +\n  coord_flip() +\n  theme_minimal()"
  },
  {
    "objectID": "slides/slides-10.html#the-power-of-machines",
    "href": "slides/slides-10.html#the-power-of-machines",
    "title": "üî® Sentiment Analysis",
    "section": "The power of machines",
    "text": "The power of machines\nAlternativen zu Dictionary-sbasierten Ans√§tzen\n\n‚ÄúTraditionelles‚Äù Machine Learning (ML), z.B. durch Feature extraction (z.B. TF-IDF) und Modellierung (z.B. Naive Bayes, Random Forest, SVM, etc.)\nDeep Learning, z.B. durch die Nutzung von Wort-Embeddings (z.B. Word2Vec, GloVe) oder kontextuellen Embeddings (z.B. BERT) und Verwendung von neuronalen Netzwerken wie LSTMs, GRUs oder CNNs.\nLarge Language Models (LLMs), z.B. vortrainierte LLMs (z.B. GPT, BERT), die f√ºr Sentiment-Aufgaben feinabgestimmt sind und Kontext und Nuancen besser verstehen als traditionelle Ans√§tze."
  },
  {
    "objectID": "slides/slides-10.html#the-way-to-use-ml-in-r",
    "href": "slides/slides-10.html#the-way-to-use-ml-in-r",
    "title": "üî® Sentiment Analysis",
    "section": "The way to use ML in R",
    "text": "The way to use ML in R\nHintergrundinformationen zu Tidymodels\n\n\nKernfunktionen und Merkmale\n\nVorverarbeitung: Umgang mit fehlenden Daten, Transformationen und Feature-Engineering.\nModellierung: Vereinfachte Modellspezifikation, -training und -abstimmung.\nBewertung: Bewertung der Modellleistung mit Metriken, Resampling und Visualisierungen.\nWorkflow: Integration von Vorverarbeitung, Modellierung und Bewertung in reproduzierbare Pipelines.\nKompatibilit√§t: Unterst√ºtzt eine Vielzahl von Modellen (z.B. Regression, Klassifikation, Clustering)."
  },
  {
    "objectID": "slides/slides-10.html#ml-the-tidy-way",
    "href": "slides/slides-10.html#ml-the-tidy-way",
    "title": "üî® Sentiment Analysis",
    "section": "ML the tidy way",
    "text": "ML the tidy way\nWas macht das tidymodels Paket besonders?\n\nIntegration mit Tidyverse: Datenorientiertes Design mit menschenlesbarer Syntax.\nModulares √ñkosystem: Spezialisierte Pakete (z.B. recipes, parsnip, rsample, tune), die nahtlos zusammenarbeiten.\nReproduzierbarkeit und Transparenz: Explizite Workflows und Abstimmungsstrategien.\nUmfassende Toolbox: Kreuzvalidierung, Bootstrapping und erweiterte Diagnosen.\nBenutzerfreundlich: Intuitiv f√ºr R-Nutzer, Balance zwischen Benutzerfreundlichkeit und fortgeschrittener Funktionalit√§t."
  },
  {
    "objectID": "slides/slides-10.html#viele-vorteile-ein-zentrales-problem",
    "href": "slides/slides-10.html#viele-vorteile-ein-zentrales-problem",
    "title": "üî® Sentiment Analysis",
    "section": "Viele Vorteile, ein zentrales Problem",
    "text": "Viele Vorteile, ein zentrales Problem\nWarum im Seminar kein Fokus auf supervised ML gelegt wird\n\nZeit: sorgf√§ltige Vorverarbeitung, (Re-)Modellierung und Fine-Tuning\nKomplexit√§t: tiefes Verst√§ndnis von Algorithmen und Modellierungstechniken erforderlich.\nFehlende Daten: besonders ‚Äúsupervised‚Äù ML ben√∂tigt gro√üe, saubere und gut annotierte Datens√§tze\n\nAber:\n\nCode f√ºr Umsetzung im Tutorial zur Sitzung enhalten\nDie Umsetzung orientiert sich an einem Blogeintrag (inklusive Screencast) von Julia Silge , der mehr Hintergrundinformationen enth√§lt"
  },
  {
    "objectID": "slides/slides-10.html#the-not-so-distant-future",
    "href": "slides/slides-10.html#the-not-so-distant-future",
    "title": "üî® Sentiment Analysis",
    "section": "The (not so distant) future",
    "text": "The (not so distant) future\nNutzung lokaler LLMs mit Ollama\n\n\n\n\n\nopen-source project that serves as a powerful and user-friendly platform for running LLMs on your local machine.\nbridge between the complexities of LLM technology and the desire for an accessible and customizable AI experience.\nprovides access to a diverse and continuously expanding library of pre-trained LLM models (e.g.Llama 3, Phi 3, Mistral, Gemma 2)"
  },
  {
    "objectID": "slides/slides-10.html#r-wrapper-f√ºr-llm-apis",
    "href": "slides/slides-10.html#r-wrapper-f√ºr-llm-apis",
    "title": "üî® Sentiment Analysis",
    "section": "R-Wrapper f√ºr LLM APIs",
    "text": "R-Wrapper f√ºr LLM APIs\nVorstellung von Pakten f√ºr die Nutzung (lokaler) LLMs in R\n\n\n\n\n\nthe goal of rollama is to wrap the Ollama API, which allows you to run different LLMs locally and create an experience similar to ChatGPT/OpenAI‚Äôs API.\n\n\n\n\n\n\n\nellmer makes it easy to use large language models (LLM) from R. It supports a wide variety of LLM providers and implements a rich set of features including streaming outputs, tool/function calling, structured data extraction, and more."
  },
  {
    "objectID": "slides/slides-10.html#chat-mit-llms-in-r",
    "href": "slides/slides-10.html#chat-mit-llms-in-r",
    "title": "üî® Sentiment Analysis",
    "section": "Chat mit LLMs in R",
    "text": "Chat mit LLMs in R\nPraktische Anwendung von ellmer [v0.0.0.9000, Wickham & Cheng (2024)]\n\n\n\n\nellmer_chat_llama &lt;- ellmer::chat_ollama(\n    model = \"llama3.2\"\n)\n\nellmer_chat_llama$chat(\"Why is the sky blue?\")\n\nThe sky appears blue because of a phenomenon called scattering, which occurs \nwhen sunlight interacts with the tiny molecules of gases in the Earth's \natmosphere.\n\nHere's a simplified explanation:\n\n1. Sunlight consists of all colors of the visible spectrum, including red, \norange, yellow, green, blue, indigo, and violet.\n2. When sunlight enters the Earth's atmosphere, it encounters tiny molecules of\ngases such as nitrogen (N2) and oxygen (O2).\n3. The shorter wavelengths of light, like blue and violet, are scattered more \nthan the longer wavelengths, like red and orange. This is known as Rayleigh \nscattering.\n4. As a result of scattering, the blue light is distributed in all directions, \nreaching our eyes from every part of the sky.\n\nThe reason why the sky appears blue on a clear day is because:\n\n* The blue light scatters in all directions, making it visible to our eyes from\nevery angle.\n* The longer wavelengths of light (like red and orange) continue to travel in \nstraight lines, allowing us to see them as the sun's rays pass through the \natmosphere.\n\nThe color we perceive is a combination of the scattered blue light and any \ndirect sunlight that reaches our eyes. On cloudy days or during sunrise/sunset \nwhen there is more scattering of shorter wavelengths, the sky can appear more \nhazy, red, or orange.\n\nKeep in mind that the exact shade of blue and other atmospheric effects can \nvary depending on factors such as:\n\n* The time of day (sun rise, peak noon, sunset)\n* Atmospheric conditions (humidity, pollution levels, weather patterns)\n* Altitude and latitude\n\nBut for most of us, the classic clear-sky blue will always be our sky's \nsignature hue!\n\n\n\n\nellmer_chat_mistral &lt;- ellmer::chat_ollama(\n    model = \"mistral\"\n)\n\nellmer_chat_mistral$chat(\"Why is the sky blue?\")\n\n The appearance of a blue sky is due to a process called Rayleigh scattering. \nIn simple terms, this is scattering of light by particles in the atmosphere \nthat are much smaller than the wavelength of light (molecules and small air \nmolecules). Blue light is scattered more because it travels in shorter, smaller\nwaves, and has more energy to spare for scattering events. This scattering of \nblue light is what we perceive as a blue sky during a clear day. If you \nremember the mnemonic \"ROYGBIV\" (Red, Orange, Yellow, Green, Blue, Indigo, \nViolet), blue occupies the part of the visible spectrum that scatters most in \nthe atmosphere, so this explains why we don't usually see red or orange hues in\nthe sky. However, as the sun sets and moves towards the horizon, the light has \nto pass through more atmosphere, which means more of shorter wavelengths like \nblue are scattered away. What is left is predominantly red and yellow light, \nwhich results in the colorful sunsets we often witness."
  },
  {
    "objectID": "slides/slides-10.html#chat-mit-llms-in-r-1",
    "href": "slides/slides-10.html#chat-mit-llms-in-r-1",
    "title": "üî® Sentiment Analysis",
    "section": "Chat mit LLMs in R",
    "text": "Chat mit LLMs in R\nPraktische Anwendung von rollama [v0.2.0, Gruber & Weber (2024)]\n\n\n\n\ndemo_2_llama3_2 &lt;- rollama::query(\n     \"What is the longest five letter word in english?\",\n    model = \"llama3.2\",\n    screen = FALSE,\n    output = \"text\"\n)\n\nglue::glue(demo_2_llama3_2)\n\nThe longest five-letter word in English is \"house\".\n\n\n\n\ndemo_2_mistral &lt;- rollama::query(\n    \"What is the longest five letter word in english?\",\n    model = \"mistral\",\n    screen = FALSE,\n    output = \"text\"\n)\n\nglue::glue(demo_2_mistral)\n\n The longest five-letter word in English that is considered a single word (without a hyphen) is \"smile\" when you consider the past tense form \"smiled\". If we stick to common, everyday words, then there are no five-letter words longer than \"movie\". Words like \"abstemsia,\" \"floccinaucinihilipilification,\" and \"derterminable\" have more letters but consist of multiple words combined."
  },
  {
    "objectID": "slides/slides-10.html#vorsicht-bei-der-auswahl-eines-modells",
    "href": "slides/slides-10.html#vorsicht-bei-der-auswahl-eines-modells",
    "title": "üî® Sentiment Analysis",
    "section": "Vorsicht bei der Auswahl eines Modells",
    "text": "Vorsicht bei der Auswahl eines Modells\nModelle unterscheiden sich in ihrer Komplexit√§t & Performance\n\n\n\n\ndemo_3_llama3_2 &lt;- rollama::query(\n    \"Is 9677 a prime number?\",\n    model = \"llama3.2\",\n    screen = FALSE,\n    output = \"text\"\n)\n\nglue::glue(demo_3_llama3_2)\n\nTo determine if 9677 is a prime number, I'll need to check its divisibility by other numbers.\n\nAfter checking, I found that 9677 can be divided by 17 and 569, which means it's not a prime number. Therefore, the answer is no, 9677 is not a prime number.\n\n\n\n\ndemo_3_mistral &lt;- rollama::query(\n    \"Is 9677 a prime number?\",\n    model = \"mistral\",\n    screen = FALSE,\n    output = \"text\"\n)\n\nglue::glue(demo_3_mistral)\n\n9677 is not a prime number. A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. In the case of 9677, it can be divided evenly by 1, 7, 1381, and 9677, so it does not meet the criteria for a prime number."
  },
  {
    "objectID": "slides/slides-10.html#sentimentscores-mit-llm",
    "href": "slides/slides-10.html#sentimentscores-mit-llm",
    "title": "üî® Sentiment Analysis",
    "section": "Sentimentscores mit LLM",
    "text": "Sentimentscores mit LLM\nPrompt-Design f√ºr einfache Sentimentanalsye via LLM in R\n\n# Erstellung einer kleinen Stichprobe\nsubsample &lt;- chats_sentiment %&gt;% \n    filter(message_length &gt; 20 & message_length &lt; 50) %&gt;%\n    slice_sample(n = 10) \n\n# Process each review using make_query\nqueries &lt;- rollama::make_query(\n    text = subsample$message_content,\n    prompt = \"Classify the sentiment of the provided text. Provide a sentiment score ranging from -1 (very negative) to 1 (very positive).\",\n    template = \"{prefix}{text}\\n{prompt}\",\n    system = \"Classify the sentiment of this text. Respond with only a numerical sentiment score.\",\n    prefix = \"Text: \"\n)\n\n# Create sentiment score for different models\nmodels &lt;- c(\"llama3.2\", \"gemma2\", \"mistral\")\nnames &lt;- c(\"llama\", \"gemma\", \"mistral\")\nfor (i in seq_along(models)) {\n  subsample[[names[i]]] &lt;- rollama::query(queries, model = models[i], screen = FALSE, output = \"text\")\n}"
  },
  {
    "objectID": "slides/slides-10.html#die-krux-mit-dem-sentiment",
    "href": "slides/slides-10.html#die-krux-mit-dem-sentiment",
    "title": "üî® Sentiment Analysis",
    "section": "Die Krux mit dem Sentiment",
    "text": "Die Krux mit dem Sentiment\nVergleich der verschiedenen Sentiment Scores\n\nsubsample %&gt;% \n  select(message_content, polarity, valence, compound, llama, gemma, mistral) %&gt;% \n  gt() \n\n\n\n\n\n\n\nmessage_content\npolarity\nvalence\ncompound\nllama\ngemma\nmistral\n\n\n\n\nAc Games never had strong combat.\n0.000000\n0.5\n-0.169\n0\n-0.8\n0.2 (slightly negative)\n\n\nIf you believe Kamala‚Äôs lies that is on you\n0.000000\n0.0\n-0.421\n0\n-0.8\n0.35 (Negative)\n\n\nWhat audience you been in?\n0.000000\n0.0\n0.000\n0\n0\n0 (Neutral)\n\n\nshe's evolving into a giga karen\n0.000000\n0.0\n0.000\n0\n-0.8\n0.6 (Moderately Negative)\n\n\n@Megaphonix, Stop one-man spamming [2x]\n0.000000\n-1.5\n-0.649\n0\n-0.8\n0.4 (Negative)\n\n\nEveryone was in bomb shelters\n0.000000\n-1.0\n-0.494\n0\n-0.8\n0.5 (Neutral to slightly negative, indicating a state of fear or concern)\n\n\nChatting \"this rarely happens\"\n0.000000\n0.0\n0.000\n0\n0\n0.3 (Mildly Positive)\n\n\nCurseLit CurseLit CurseLit CurseLit CurseLit\n0.000000\n0.0\n0.000\n0\n-1\n0 (Neutral/Negative)\n\n\nCANT BE SICK IF YOU DONT SEE DOCTOR *big brain*\n-1.098612\n-0.5\n0.501\n0\n0.5\n0.4 (Ambivalent, leaning slightly positive due to the \"big brain\" comment, but overall tone is not clearly positive or negative)\n\n\ncan you taste the 23 flavors\n0.000000\n0.0\n0.000\n0\n0.5\n0.5 (Neutral or Slightly Positive)"
  },
  {
    "objectID": "slides/slides-10.html#und-was-machen-wir-jetzt-damit",
    "href": "slides/slides-10.html#und-was-machen-wir-jetzt-damit",
    "title": "üî® Sentiment Analysis",
    "section": "Und was machen wir jetzt damit?",
    "text": "Und was machen wir jetzt damit?\n(Weiter-)Arbeit mit dem Sentiments\n\nValidierung, z.B.\n\nQualitativer Vergleich der Scores und dem Inhalt der Nachricht\n√úberpr√ºfung besonders ‚Äúpositiver‚Äù oder ‚Äúnegativer‚Äù Nachrichten\nggf. Vergleich verschiedener Sentiment Scores\n\nWeiterf√ºhrende Analysen, z.B.\n\nVerteilung der Sentiment Scores nach Streamer oder L√§nge der Nachrichten\nWichtig: Bezug zur Forschungsfrage!"
  },
  {
    "objectID": "slides/slides-10.html#unterschiedliche-emotionalit√§t-des-chats",
    "href": "slides/slides-10.html#unterschiedliche-emotionalit√§t-des-chats",
    "title": "üî® Sentiment Analysis",
    "section": "Unterschiedliche Emotionalit√§t des Chats?",
    "text": "Unterschiedliche Emotionalit√§t des Chats?\nBeispiel f√ºr weiterf√ºhrende Analyse: Sentiment Scores nach Streamer\n\nchats_sentiment %&gt;% \n    ggpubr::ggdensity(\n        x = \"compound\",\n        color = \"streamer\"\n    )"
  },
  {
    "objectID": "slides/slides-10.html#und-was-machen-wir-jetzt-damit-1",
    "href": "slides/slides-10.html#und-was-machen-wir-jetzt-damit-1",
    "title": "üî® Sentiment Analysis",
    "section": "Und was machen wir jetzt damit?",
    "text": "Und was machen wir jetzt damit?\nBeispiel f√ºr weiterf√ºhrende Analyse: Sentiment Scores nach L√§nge der Nachrichten\n\n\nExpand for full code\nchats_sentiment %&gt;% \n    mutate(message_length_fct = case_when( \n        message_length &lt;= 7 ~ \"&lt;= 7 words\",\n        message_length &gt; 7 & message_length &lt;= 34 ~ \"8 to 34 words\",\n        message_length &gt;= 34 ~ \"&gt; 34 words\")\n     ) %&gt;%\n    group_by(message_length_fct) %&gt;%\n    mutate(n = n()) %&gt;%\n    ggviolin(\n        x = \"message_length_fct\",\n        y = \"compound\", \n        fill = \"message_length_fct\"\n    ) +\n    stat_summary(\n        fun.data = function(x) data.frame(y = max(x) + 0.15, label = paste0(\"n=\", length(x))),\n        geom = \"text\",\n        size = 3,\n        color = \"black\"\n    ) +\n    labs(\n        x = \"L√§nge der Nachricht\"\n    )"
  },
  {
    "objectID": "slides/slides-10.html#validierung-validierung-validierung",
    "href": "slides/slides-10.html#validierung-validierung-validierung",
    "title": "üî® Sentiment Analysis",
    "section": "Validierung, Validierung, Validierung",
    "text": "Validierung, Validierung, Validierung\n√úberpr√ºfung besonders ‚Äúpositiver‚Äù Nachrichten\n\nchats_sentiment %&gt;% \n    filter(compound &gt;= 0.95) %&gt;% \n    arrange(desc(compound)) %&gt;% \n    select(message_content, compound) %&gt;% \n    head(n = 3) %&gt;% \n    gt() %&gt;% gtExtras::gt_theme_538()\n\n\n\n\n\n\n\nmessage_content\ncompound\n\n\n\n\nmizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3\n0.997\n\n\nhasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY\n0.996\n\n\nhasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY\n0.996"
  },
  {
    "objectID": "slides/slides-10.html#validierung-validierung-validierung-1",
    "href": "slides/slides-10.html#validierung-validierung-validierung-1",
    "title": "üî® Sentiment Analysis",
    "section": "Validierung, Validierung, Validierung",
    "text": "Validierung, Validierung, Validierung\n√úberpr√ºfung besonders ‚Äúnegativer‚Äù Nachrichten\n\nchats_sentiment %&gt;% \n    filter(compound &lt;= -0.95) %&gt;% \n    arrange(compound) %&gt;% \n    select(message_content, compound) %&gt;% \n    head(n = 3) %&gt;% \n    gt() %&gt;% gtExtras::gt_theme_538()\n\n\n\n\n\n\n\nmessage_content\ncompound\n\n\n\n\npepeMeltdown OH SHIT MY OIL FUCK FUCK FUCK pepeMeltdown OH SHIT MY OIL FUCK FUCK FUCK pepeMeltdown OH SHIT MY OIL FUCK FUCK FUCK pepeMeltdown OH SHIT MY OIL FUCK FUCK FUCK\n-0.997\n\n\nbut why kill more birdsbut why kill more birdsbut why kill more birdsbut why kill more birdsbut why kill more birdsbut why kill more birdsbut why kill more birds\n-0.996\n\n\ncry bully ass losers KEKL cry bully ass losers KEKL cry bully ass losers KEKL cry bully ass losers KEKL cry bully ass losers KEKL\n-0.996"
  },
  {
    "objectID": "slides/slides-10.html#validierung-validierung-validierung-2",
    "href": "slides/slides-10.html#validierung-validierung-validierung-2",
    "title": "üî® Sentiment Analysis",
    "section": "Validierung, Validierung, Validierung",
    "text": "Validierung, Validierung, Validierung\nWersendert besonders negative Nachrichten?\n\nchats_sentiment %&gt;% \n    filter(compound &gt;= 0.95) %&gt;% \n    sjmisc::frq(\n        user_name, \n        min.frq = 5,\n        sort.frq = \"desc\")\n\nuser_name &lt;character&gt; \n# total N=289 valid N=289 mean=85.70 sd=50.72\n\nValue                |   N | Raw % | Valid % | Cum. %\n-----------------------------------------------------\nnotilandefinitelynot |  17 |  5.88 |    5.88 |   5.88\ndirty_barn_owl       |  16 |  5.54 |    5.54 |  11.42\naliisontw1tch        |   7 |  2.42 |    2.42 |  13.84\nomnivalor            |   7 |  2.42 |    2.42 |  16.26\nx7yz42               |   6 |  2.08 |    2.08 |  18.34\nchakek1993414        |   5 |  1.73 |    1.73 |  20.07\ndoortoratworld       |   5 |  1.73 |    1.73 |  21.80\nmuon_2ms             |   5 |  1.73 |    1.73 |  23.53\nn &lt; 5                | 221 | 76.47 |   76.47 | 100.00\n&lt;NA&gt;                 |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;"
  },
  {
    "objectID": "slides/slides-10.html#was-nehmen-wir-mit",
    "href": "slides/slides-10.html#was-nehmen-wir-mit",
    "title": "üî® Sentiment Analysis",
    "section": "Was nehmen wir mit?",
    "text": "Was nehmen wir mit?\nKurze Zusammenfassung der Inhalte zur Sentimentanalyse\n\nVerschiedene M√∂glichkeiten (Modelle, Dictionaries, etc), Sentimentanalyse in R durchzuf√ºhren\nDie verschiedene M√∂glichkeiten haben unterschiedliche Vor- und Nachteile\nAllgemein gilt:\n\nDie Wahl des Modells h√§ngt von der spezifischen Forschungsfrage und den verf√ºgbaren Daten ab\nValidieren, Validieren, Validieren (& Optimieren!)\n\n\nAber: Wie sinnvoll und aussagekr√§ftig sind (unsupervised) Sentimentanalysen in der Praxis?"
  },
  {
    "objectID": "slides/slides-10.html#goodbye-theory-hello-practice",
    "href": "slides/slides-10.html#goodbye-theory-hello-practice",
    "title": "üî® Sentiment Analysis",
    "section": "Goodbye theory, hello practice!",
    "text": "Goodbye theory, hello practice!\nEin Blick auf die kommenden Sitzungen\n\nAbschluss der inhaltlichen Sitzungen ‚ûú ‚ÄúProjektphase‚Äù\nZiel: Durchf√ºhrung einer ‚ÄúMini-Studie‚Äù\n\nEntwicklung einer Forschungsfrage (auf Basis der Inhalte der Vortr√§ge) und\nAnwendung mindestens einer der behandelten Methoden\nauf bereitgestellte Datens√§tze"
  },
  {
    "objectID": "slides/slides-10.html#fokus-auf-gruppenarbeit",
    "href": "slides/slides-10.html#fokus-auf-gruppenarbeit",
    "title": "üî® Sentiment Analysis",
    "section": "Fokus auf Gruppenarbeit",
    "text": "Fokus auf Gruppenarbeit\nZum Ablauf der n√§chsten zwei Sitzungen\n\nFokus der n√§chsten Sitzungen liegt auf eigenst√§ndige Gruppenarbeit\n\nGrobe Struktur: Kurze Input-Session am Anfang (Fragerunde, Orgaupdates), danach Fokus auf Arbeit in den Gruppen\n\nNutzt die M√∂glichkeit f√ºr den Austausch oder Nachfragen\n\nTauscht euch untereinander aus, sprecht mit den Expert:innen der jeweiligen Sitzung!\nIch stehe w√§hrendessen als Ansprechpartner zur Verf√ºgung\n\nDenkt an die anstehenden Assignments (Pr√§sentationsentwurf & Peer Review)!"
  },
  {
    "objectID": "slides/slides-10.html#and-now-you",
    "href": "slides/slides-10.html#and-now-you",
    "title": "üî® Sentiment Analysis",
    "section": "üß™ And now ‚Ä¶ you!",
    "text": "üß™ And now ‚Ä¶ you!\nF√ºr den Rest der Sitzung: Grupppenarbeit am Projektpr√§sentationsentwurf\n\n\n\n\n\n\nWichtige Hinweise\n\n\n\nN√§chste Woche (15.01.) ist die Deadline f√ºr den Entwurf der ‚ÄúProjektpr√§sentation‚Äù ( = Grundlage f√ºr das Peer Review)\nAblauf wird n√§chste Woche noch detailliert besprochen!\n\n\n\n\n\n\n\n\n\n\nArbeitsauftrag\n\n\nIn euren Gruppen ‚Ä¶\n\nbeginnt die Arbeit an der Projektpr√§sentation (siehe QR-Code n√§chste Folie)\nsetzt den Schwerpunkt zun√§chst auf die Forschungsfrage, und √ºberlegt danach, wie ihr diese mit Hilfe der vorgestellten Methoden beantworten k√∂nnt"
  },
  {
    "objectID": "slides/slides-10.html#get-started",
    "href": "slides/slides-10.html#get-started",
    "title": "üî® Sentiment Analysis",
    "section": "Get started!",
    "text": "Get started!\nBitte nutzt die jeweilige Folienvorlage f√ºr die Dokumentation euerer Ergebnisse\n\n\n\n\n     Gruppe 1 \n\n\n\n\n\n     Gruppe 2 \n\n\n\n\n\n     Gruppe 3 \n\n\n\n\n\n\n     Gruppe 4 \n\n\n\n\n\n     Gruppe 5 \n\n\n\n\n\n     Gruppe 6"
  },
  {
    "objectID": "slides/slides-10.html#references",
    "href": "slides/slides-10.html#references",
    "title": "üî® Sentiment Analysis",
    "section": "References",
    "text": "References\n\n\nBenoit, K., Watanabe, K., Wang, H., Nulty, P., Obeng, A., M√ºller, S., & Matsuo, A. (2018). Quanteda: An r package for the quantitative analysis of textual data. Journal of Open Source Software, 3(30), 774. https://doi.org/10.21105/joss.00774\n\n\nGruber, J. B., & Weber, M. (2024). Rollama: Communicate with ‚Äôollama‚Äô. https://jbgruber.github.io/rollama/\n\n\nHutto, C., & Gilbert, E. (2014). VADER: A parsimonious rule-based model for sentiment analysis of social media text. Proceedings of the International AAAI Conference on Web and Social Media, 8(1), 216‚Äì225. https://doi.org/10.1609/icwsm.v8i1.14550\n\n\nNielsen, F. √Ö. (2011). A new ANEW: Evaluation of a word list for sentiment analysis in microblogs. https://doi.org/10.48550/ARXIV.1103.2903\n\n\nRoehrick, K. (n.d.). vader: Valence Aware Dictionary and sEntiment Reasoner (VADER). https://doi.org/10.32614/CRAN.package.vader\n\n\nWickham, H., & Cheng, J. (2024). Ellmer: Chat with large language models. https://ellmer.tidyverse.org\n\n\nYoung, L., & Soroka, S. (2012). Affective News: The Automated Coding of Sentiment in Political Texts. Political Communication, 29(2), 205‚Äì231. https://doi.org/10.1080/10584609.2012.671234"
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Christoph Adrian (he/him)     is a Research Assistant at the Chair of Communication Science at Friedrich-Alexander-Universit√§t (FAU) Erlangen-N√ºrnberg.\nHis work focuses on computational methods, especially Text as Data Approaches and workin with Digital behavioral data, with an emphasis on computing, reproducible research, student-centered learning, and open-source education.\n\n\n\nOffice hours\nLocation\n\n\n\n\nWednesday 15:30 - 16:30\nFG 2.031",
    "crumbs": [
      "Course information",
      "Teaching team"
    ]
  },
  {
    "objectID": "course-team.html#instructor",
    "href": "course-team.html#instructor",
    "title": "Teaching team",
    "section": "",
    "text": "Christoph Adrian (he/him)     is a Research Assistant at the Chair of Communication Science at Friedrich-Alexander-Universit√§t (FAU) Erlangen-N√ºrnberg.\nHis work focuses on computational methods, especially Text as Data Approaches and workin with Digital behavioral data, with an emphasis on computing, reproducible research, student-centered learning, and open-source education.\n\n\n\nOffice hours\nLocation\n\n\n\n\nWednesday 15:30 - 16:30\nFG 2.031",
    "crumbs": [
      "Course information",
      "Teaching team"
    ]
  },
  {
    "objectID": "data_collection/01_03-data_collection-twitch_streamer_stats.html",
    "href": "data_collection/01_03-data_collection-twitch_streamer_stats.html",
    "title": "Mining: Twitch streamer statistics",
    "section": "",
    "text": "# Create a tibble based on the provided data\nlibrary(tidyverse)\n\ndata_hasanabi &lt;- tribble(\n  ~Month, ~Avg_Viewers, ~Avg_Viewers_Gain, ~Avg_Viewers_Percent_Gain, ~Peak_Viewers, ~Hours_Streamed, ~Hours_Streamed_Gain, ~Hours_Streamed_Percent_Gain, ~Followers, ~Followers_Gain, ~Followers_Percent_Gain, ~Followers_Per_Hour,\n  \"Dec 2024\", 26265, -17031, -39.3, 37094, 18.4, -221, -92.3, 2793538, 604, NA, 32.8,\n  \"Nov 2024\", 43296, 12510, 40.6, 312431, 239, 16.8, 7.6, 2792934, 59296, 2.2, 248,\n  \"Oct 2024\", 30786, 2921, 10.5, 110889, 223, -12.9, -5.5, 2733638, 17998, 0.7, 80.9,\n  \"Sep 2024\", 27865, -8174, -22.7, 192633, 236, 3.9, 1.7, 2715640, 26341, 1, 112,\n  \"Aug 2024\", 36039, -57, -0.2, 86598, 232, 12.1, 5.5, 2689299, 23519, 0.9, 102,\n  \"Jul 2024\", 36096, 14830, 69.7, 135265, 220, 3.7, 1.7, 2665780, 37035, 1.4, 169,\n  \"Jun 2024\", 21266, -1291, -5.7, 126069, 216, -9.5, -4.2, 2628745, 4805, 0.2, 22.3,\n  \"May 2024\", 22557, 1939, 9.4, 37656, 225, -24.4, -9.8, 2623940, 61255, 2.4, 272,\n  \"Apr 2024\", 20618, 1980, 10.6, 64992, 250, 31, 14.2, 2562685, 4545, 0.2, 18.2,\n  \"Mar 2024\", 18638, -374, -2, 46536, 219, -8.5, -3.7, 2558140, -4783, -0.2, -21.9,\n  \"Feb 2024\", 19012, -714, -3.6, 34541, 227, 8.3, 3.8, 2562923, 690, NA, 3,\n  \"Jan 2024\", 19726, 1504, 8.3, 37875, 219, 31.5, 16.8, 2562233, 1035, NA, 4.7\n  ) %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(streamer = \"hasanabi\")\n\n\n# Create a tibble based on the provided data\ndata_zackrawrr &lt;- tribble(\n  ~Month, ~Avg_Viewers, ~Avg_Viewers_Gain, ~Avg_Viewers_Percent_Gain, ~Peak_Viewers, ~Hours_Streamed, ~Hours_Streamed_Gain, ~Hours_Streamed_Percent_Gain, ~Followers, ~Followers_Gain, ~Followers_Percent_Gain, ~Followers_Per_Hour,\n  \"Dec 2024\", 31240, -17585, -36, 41061, 13.5, -155, -92, 1914946, 280, NA, 20.8,\n  \"Nov 2024\", 48825, 11527, 30.9, 170847, 168, 57.9, 52.5, 1914666, 21419, 1.1, 127,\n  \"Oct 2024\", 37298, 5369, 16.8, 80561, 110, -93.5, -45.9, 1893247, 9610, 0.5, 87.2,\n  \"Sep 2024\", 31929, -1415, -4.2, 71902, 204, 8.8, 4.5, 1883637, 21098, 1.1, 104,\n  \"Aug 2024\", 33344, 1054, 3.3, 74397, 195, -5.1, -2.5, 1862539, 22585, 1.2, 116,\n  \"Jul 2024\", 32290, 4919, 18, 93783, 200, 14.4, 7.8, 1839954, 25887, 1.4, 130,\n  \"Jun 2024\", 27371, 10911, 66.3, 62836, 186, -55.8, -23.1, 1814067, 25388, 1.4, 137,\n  \"May 2024\", 16460, -1888, -10.3, 48105, 241, 39.2, 19.4, 1788679, 15092, 0.9, 62.5,\n  \"Apr 2024\", 18348, -2500, -12, 35976, 202, 12.7, 6.7, 1773587, 18650, 1.1, 92.3,\n  \"Mar 2024\", 20848, 1142, 5.8, 40612, 190, 24.4, 14.7, 1754937, 32400, 1.9, 171,\n  \"Feb 2024\", 19706, -5189, -20.8, 41302, 165, 0.8, 0.5, 1722537, 36598, 2.2, 222,\n  \"Jan 2024\", 24895, -6579, -20.9, 45829, 164, -29.2, -15.1, 1685939, 42503, 2.6, 259\n  ) %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(streamer = \"zackrawrr\")\n\n\n# Create a tibble based on the provided data\ndata_tmr &lt;- tribble(\n  ~Month, ~Avg_Viewers, ~Avg_Viewers_Gain, ~Avg_Viewers_Percent_Gain, ~Peak_Viewers, ~Hours_Streamed, ~Hours_Streamed_Gain, ~Hours_Streamed_Percent_Gain, ~Followers, ~Followers_Gain, ~Followers_Percent_Gain, ~Followers_Per_Hour,\n  \"Dec 2024\", 1005, -632, -38.6, 1971, 7.1, -55.2, -88.6, 83325, 200, 0.2, 28.1,\n  \"Nov 2024\", 1637, 588, 56.1, 4995, 62.3, -8.2, -11.6, 83125, 5639, 7.3, 90.5,\n  \"Oct 2024\", 1049, 102, 10.8, 2383, 70.5, -0.2, -0.3, 77486, 2098, 2.8, 29.8,\n  \"Sep 2024\", 947, -131, -12.2, 2144, 70.7, -1.6, -2.2, 75388, 1075, 1.4, 15.2,\n  \"Aug 2024\", 1078, 197, 22.4, 2189, 72.3, -0.7, -0.9, 74313, 2215, 3.1, 30.6,\n  \"Jul 2024\", 881, 242, 37.9, 1971, 73, 10, 15.9, 72098, 2058, 2.9, 28.2,\n  \"Jun 2024\", 639, 7, 1.1, 1230, 62.9, -3.7, -5.5, 70040, 691, 1, 11,\n  \"May 2024\", 632, 83, 15.1, 1100, 66.6, -1.5, -2.2, 69349, 715, 1, 10.7,\n  \"Apr 2024\", 549, -52, -8.7, 905, 68.1, 4.2, 6.6, 68634, 501, 0.7, 7.4,\n  \"Mar 2024\", 601, 71, 13.4, 1206, 63.9, -1.3, -2, 68133, 427, 0.6, 6.7,\n  \"Feb 2024\", 530, -22, -4, 1907, 65.2, 0.2, 0.3, 67706, 156, 0.2, 2.4,\n  \"Jan 2024\", 552, 33, 6.4, 971, 65, 9.6, 17.3, 67550, 232, 0.3, 3.6\n  ) %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(streamer = \"the_majority_report\")\n\n\ndata &lt;- bind_rows(data_hasanabi, data_zackrawrr, data_tmr) %&gt;%\n  mutate(month = lubridate::my(month))\n\n\nqs::qsave(data, file = \"local_data/twitch_streamer_stats.qs\")"
  },
  {
    "objectID": "data_collection/01_11-data_processing-chats.html",
    "href": "data_collection/01_11-data_processing-chats.html",
    "title": "Processing: Chats",
    "section": "",
    "text": "Information\n\n\n\n\nProcesses chat data from presidential and vice-presidential debates.\nImports raw data and combines it for analysis.\nRecodes variables and adds new information.\nRenames and reorders variables for clarity.\nSaves the processed data for further use.",
    "crumbs": [
      "Data collection",
      "Processing: Chats"
    ]
  },
  {
    "objectID": "data_collection/01_11-data_processing-chats.html#preparation",
    "href": "data_collection/01_11-data_processing-chats.html#preparation",
    "title": "Processing: Chats",
    "section": "Preparation",
    "text": "Preparation\n\n# Load packages\nsource(file = here::here(\n  \"data_collection/00_02-setup-session.R\"\n))\n\n\nchat &lt;- list(\n    raw = list(\n        presidential = qs::qread(here(\"local_data/chat_raw-vods_presidential_debate.qs\")),\n        vice_presidential = qs::qread(here(\"local_data/chat_raw-vods_vice_presidential_debate.qs\"))\n    )\n)",
    "crumbs": [
      "Data collection",
      "Processing: Chats"
    ]
  },
  {
    "objectID": "data_collection/01_11-data_processing-chats.html#process-data",
    "href": "data_collection/01_11-data_processing-chats.html#process-data",
    "title": "Processing: Chats",
    "section": "Process data",
    "text": "Process data\n\n\n\n\n\n\nChangelog\n\n\n\n\nCombined data from presidential and vice-presidential debates.\nDropped ‚Äúempty‚Äù meta variables: user_is_*, user_type, user_gender.\nAdded debate variable to indicate the source of the data.\nRecoded user_id to character type.\nRecoded stream_id to streamer names.\nAdded platform variable based on the URL.\nAdded message_length, message_timecode, message_time, and message_during_debate variables.\nAdded user information variables: user_has_badge, user_is_premium, user_is_subscriber, user_is_turbo, user_is_moderator, user_is_partner, user_is_subgifter, user_is_broadcaster, user_is_vip, user_is_twitchdj, user_is_founder, user_is_staff, user_is_game_dev, user_is_ambassador, user_no_audio, user_no_video.\nConverted user information variables to numeric type.\nRenamed variables: stream_id to streamer, username to user_name, display_name to user_display_name, badges to user_badges, timestamp to message_timestamp, emotes to message_emotes.\nReordered variables: moved platform and debate after url, and message_emotes after message_content.\n\n\n\n\nchat$correct &lt;- bind_rows(\n    # Combine data from presidential and vicepresidental debate\n    chat$raw$presidential %&gt;%\n        select(!c(starts_with(\"user_is_\"), user_type, user_gender)) %&gt;% # drop \"empty\" meta variables\n        mutate(debate = \"presidential\"), # add source \n    chat$raw$vice_presidential %&gt;% \n        select(!c(starts_with(\"user_is_\"), user_type, user_gender)) %&gt;% # drop \"empty\" meta variables\n        mutate(debate = \"vice presidential\")\n    ) %&gt;% \n    # Manual wrangling\n    mutate(\n        # Correction / recoding variables\n        across(user_id, ~as.character(.)), \n        across(stream_id, ~case_when(\n            str_detect(.x, \"stream_1\") ~ \"hasanabi\",\n            str_detect(.x, \"stream_2\") ~ \"zackrawrr\",\n            str_detect(.x, \"stream_3\") ~ \"the_majority_report\",\n            TRUE ~ \"unknown\")\n        ), \n        # Add information\n        platform = case_when(\n            str_detect(url, \"twitch\") ~ \"twitch\",\n            str_detect(url, \"youtube\") ~ \"youtube\",\n            TRUE ~ \"unknown\"),\n        message_length = nchar(message_content), \n        message_timecode = seconds_to_period(timestamp),\n        message_time = format(as.POSIXct(timestamp, origin = \"1970-01-01\", tz = \"UTC\"), \"%H:%M:%S\"),\n        message_during_debate = case_when(\n            # Presidential debate\n            stream_id == \"hasanabi\" & debate == \"presidential\" &\n            timestamp &gt;= hms::as_hms(\"07:00:11\") & timestamp &lt;= hms::as_hms(\"08:45:21\") |\n            stream_id == \"zackrawrr\" & debate == \"presidential\" &\n            timestamp &gt;= hms::as_hms(\"08:02:12\") & timestamp &lt;= hms::as_hms(\"09:46:15\") |\n            stream_id == \"the_majority_report\" & debate == \"presidential\" &\n            timestamp &gt;= hms::as_hms(\"00:12:53\") & timestamp &lt;= hms::as_hms(\"01:57:49\") |\n            # # Vice-Presidential debate\n            stream_id == \"hasanabi\" & debate == \"vice presidential\" &\n            timestamp &gt;= hms::as_hms(\"06:57:00\") & timestamp &lt;= hms::as_hms(\"08:43:17\") |\n            stream_id == \"zackrawrr\" & debate == \"vice presidential\" &\n            timestamp &gt;= hms::as_hms(\"07:19:26\") & timestamp &lt;= hms::as_hms(\"09:05:41\") |\n            stream_id == \"the_majority_report\" & debate == \"vice presidential\" &\n            timestamp &gt;= hms::as_hms(\"00:09:52\") & timestamp &lt;= hms::as_hms(\"01:57:07\")        \n             ~ 1,\n            TRUE ~ 0\n        ), \n        # Add user information\n        user_has_badge = map_lgl(badges, ~ length(.x) &gt; 0),\n        user_is_premium = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"premium\")), \n        user_is_subscriber = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"subscriber\")), \n        user_is_turbo = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"turbo\")),\n        user_is_moderator = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"moderator\")),\n        user_is_partner = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"partner\")),\n        user_is_subgifter = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"subgifter\")),\n        user_is_broadcaster = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"broadcaster\")),\n        user_is_vip = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"vip\")),\n        user_is_twitchdj = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"twitch_dj\")),\n        user_is_founder = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"founder\")),\n        user_is_staff = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"staff\")),\n        user_is_game_dev = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"game_developer\")),\n        user_is_ambassador = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"ambassador\")),\n        user_no_audio = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"no_audio\")),\n        user_no_video = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"no_video\")),\n        across(starts_with(\"user_is_\"), ~as.numeric(.)),\n        across(starts_with(\"user_no_\"), ~as.numeric(.)),\n        across(starts_with(\"user_has_\"), ~as.numeric(.))\n    ) %&gt;% \n    # Rename and reorder variables\n    rename(\n        streamer = stream_id,\n        user_name = username,\n        user_display_name = display_name,\n        user_badges = badges,\n        message_timestamp = timestamp,\n        message_emotes = emotes,\n    ) %&gt;% \n    relocate(platform, debate, .after = url) %&gt;% \n    relocate(message_emotes, .after = message_content)\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nstreamer\nName of the streamer.\n\n\nurl\nURL of the video.\n\n\nplatform\nPlatform where the video is hosted (e.g., Twitch, YouTube).\n\n\ndebate\nType of debate (e.g., presidential, vice presidential).\n\n\nuser_name\nUsername of the user.\n\n\nuser_id\nUnique identifier for the user.\n\n\nuser_display_name\nDisplay name of the user.\n\n\nuser_badges\nList of badges associated with the user.\n\n\nmessage_timestamp\nTimestamp of the message in seconds.\n\n\nmessage_id\nUnique identifier for the message.\n\n\nmessage_type\nType of message (e.g., text_message).\n\n\nmessage_content\nContent of the message.\n\n\nmessage_emotes\nList of emotes used in the message.\n\n\nmessage_length\nLength of the message content.\n\n\nmessage_timecode\nTimecode of the message in Period format.\n\n\nmessage_time\nTime of the message in HH:MM:SS format.\n\n\nmessage_during_debate\nIndicator if the message was sent during the debate (1 for yes, 0 for no).\n\n\nuser_has_badge\nIndicator if the user has any badge (1 for yes, 0 for no).\n\n\nuser_is_premium\nIndicator if the user is a premium member (1 for yes, 0 for no).\n\n\nuser_is_subscriber\nIndicator if the user is a subscriber (1 for yes, 0 for no).\n\n\nuser_is_turbo\nIndicator if the user is a Turbo member (1 for yes, 0 for no).\n\n\nuser_is_moderator\nIndicator if the user is a moderator (1 for yes, 0 for no).\n\n\nuser_is_partner\nIndicator if the user is a partner (1 for yes, 0 for no).\n\n\nuser_is_subgifter\nIndicator if the user is a subgifter (1 for yes, 0 for no).\n\n\nuser_is_broadcaster\nIndicator if the user is the broadcaster (1 for yes, 0 for no).\n\n\nuser_is_vip\nIndicator if the user is a VIP (1 for yes, 0 for no).\n\n\nuser_is_twitchdj\nIndicator if the user is a Twitch DJ (1 for yes, 0 for no).\n\n\nuser_is_founder\nIndicator if the user is a founder (1 for yes, 0 for no).\n\n\nuser_is_staff\nIndicator if the user is a staff member (1 for yes, 0 for no).\n\n\nuser_is_game_dev\nIndicator if the user is a game developer (1 for yes, 0 for no).\n\n\nuser_is_ambassador\nIndicator if the user is an ambassador (1 for yes, 0 for no).\n\n\nuser_no_audio\nIndicator if the user has no audio (1 for yes, 0 for no).\n\n\nuser_no_video\nIndicator if the user has no video (1 for yes, 0 for no).",
    "crumbs": [
      "Data collection",
      "Processing: Chats"
    ]
  },
  {
    "objectID": "data_collection/01_11-data_processing-chats.html#save-output",
    "href": "data_collection/01_11-data_processing-chats.html#save-output",
    "title": "Processing: Chats",
    "section": "Save output",
    "text": "Save output\n\nqs::qsave(\n    chat,\n    file = here(\"local_data/chat-debates_full.qs\")\n)\n\nqs::qsave(\n    chat$correct, \n    file = here(\"local_data/chat-debates.qs\")\n)",
    "crumbs": [
      "Data collection",
      "Processing: Chats"
    ]
  },
  {
    "objectID": "data_collection/01_12-data_processing-transcripts.html",
    "href": "data_collection/01_12-data_processing-transcripts.html",
    "title": "Processing: Transcripts",
    "section": "",
    "text": "Information\n\n\n\n\nProcesses transcripts of live-streamed debates.\nImports, cleans, and transforms the data.\nSaves the processed data for further analysis.",
    "crumbs": [
      "Data collection",
      "Processing: Transcripts"
    ]
  },
  {
    "objectID": "data_collection/01_12-data_processing-transcripts.html#preparation",
    "href": "data_collection/01_12-data_processing-transcripts.html#preparation",
    "title": "Processing: Transcripts",
    "section": "Preparation",
    "text": "Preparation\n\n# Load packages\nsource(file = here::here(\n  \"data_collection/00_02-setup-session.R\"\n))\n\n\ntranscripts &lt;- list(\n    raw = fs::dir_ls(\n        path = here(\"local_data/transcripts\"), \n        glob = \"*.txt\"\n        ) %&gt;%\n        # Read the files into the list\n        map(~ read_file(.)) %&gt;%\n        # Set the list names to the base file names (without the path)\n        set_names(~ str_extract(basename(.), \"^(.*)(?=\\\\.txt)\"))\n)",
    "crumbs": [
      "Data collection",
      "Processing: Transcripts"
    ]
  },
  {
    "objectID": "data_collection/01_12-data_processing-transcripts.html#process-data",
    "href": "data_collection/01_12-data_processing-transcripts.html#process-data",
    "title": "Processing: Transcripts",
    "section": "Process data",
    "text": "Process data\n\n\n\n\n\n\nChangelog\n\n\n\n\nAdded extraction of speaker and timestamp from each line of the transcript.\nRemoved brackets from the timestamp.\nExtracted dialogue text and calculated its length.\nFiltered out lines without speaker or dialogue.\nConverted timestamp to hms object and calculated duration between dialogues.\nMerged all processed files into a single data frame with source identifiers.\nAdded columns for debate type, streamer, and numeric streamer identifier.\nCreated unique speaker identifiers combining debate type, streamer, and speaker.\nAdded sequence_during_debate indicator based on timestamp ranges for each source.\nGenerated unique id_sequence for each speaking sequence.\nRemoved unnecessary columns (line, prefix) and reordered columns.\n\n\n\n\ntranscripts$correct &lt;- transcripts$raw %&gt;% \n    # Import and process each file\n    map(~ read_file(.) %&gt;%\n            str_split(\"\\n\") %&gt;%\n            .[[1]] %&gt;%\n            tibble(line = .) %&gt;%\n            mutate(\n                speaker = str_extract(line, \"S\\\\d+\"),  # Extract speaker \n                timestamp = str_extract(line, \"\\\\[\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\]\"),  # Extract timestamps\n                across(timestamp, ~str_remove_all(., \"[\\\\[\\\\]]\")),  # Remove the brackets from timestamp\n                dialogue = str_remove(line, \"S\\\\d+ \\\\[\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\]: \"),  # Extract dialogue text\n                dialogue_length = nchar(dialogue),\n            ) %&gt;% \n            filter( # Filter out lines without speaker or dialogue\n                !is.na(speaker) &\n                !is.na(dialogue)\n                ) %&gt;% \n            mutate(\n                timestamp = hms::as_hms(timestamp),  # Convert timestamp to hms object\n                duration = as.numeric(difftime(lead(timestamp), timestamp, , units = \"secs\"))  # Calculate duration\n            )\n    ) %&gt;%\n    bind_rows(.id = \"source\") %&gt;% \n    mutate(\n        debate = case_when(\n            str_detect(source, \"vice_presidential\") ~ \"vice_presidential\",\n            TRUE ~ \"presidential\"), \n        streamer = case_when(\n            str_detect(source, \"abc\") | str_detect(source, \"cbs\") ~ \"tv_station\",\n            str_detect(source, \"hasanabi\") ~ \"hasanabi\",\n            str_detect(source, \"zackrawrr\") ~ \"zackrawrr\",\n            str_detect(source, \"the_majority_report\") ~ \"the_majority_report\",\n            TRUE ~ \"unknown\"\n        ),\n        id_streamer = case_when(\n            streamer == \"tv_station\" ~ 1,\n            streamer == \"hasanabi\" ~ 2,\n            streamer == \"the_majority_report\" ~ 3,\n            streamer == \"zackrawrr\" ~ 4,\n            TRUE ~ 0\n        ),\n        prefix = paste0(\n            ifelse(debate == \"presidential\", \"p\", \"vp\"),\n            id_streamer, \"_\"\n        ),\n        id_speaker = paste0(prefix, tolower(speaker)),\n        sequence_during_debate = case_when(\n            # Presidential debate\n            source == \"presidential_debate-abc\" &\n            timestamp &gt;= hms::as_hms(\"00:00:00\") & timestamp &lt;= hms::as_hms(\"01:45:07\") |\n            source == \"presidential_debate-hasanabi\" & \n            timestamp &gt;= hms::as_hms(\"07:00:11\") & timestamp &lt;= hms::as_hms(\"08:45:21\") |\n            source == \"presidential_debate-zackrawrr\" & \n            timestamp &gt;= hms::as_hms(\"08:02:12\") & timestamp &lt;= hms::as_hms(\"09:46:15\") |\n            source == \"presidential_debate-the_majority_report\" & \n            timestamp &gt;= hms::as_hms(\"00:12:53\") & timestamp &lt;= hms::as_hms(\"01:57:49\") |\n            # Vice-Presidential debate\n            source == \"vice_presidential_debate-cbs\" &\n            timestamp &gt;= hms::as_hms(\"00:00:00\") & timestamp &lt;= hms::as_hms(\"01:47:48\") |\n            source == \"vice_presidential_debate-hasanabi\" & \n            timestamp &gt;= hms::as_hms(\"06:57:00\") & timestamp &lt;= hms::as_hms(\"08:43:17\") |\n            source == \"presidential_debate-zackrawrr\" & \n            timestamp &gt;= hms::as_hms(\"07:19:26\") & timestamp &lt;= hms::as_hms(\"09:05:41\") |\n            source == \"vice_presidential_debate-the_majority_report\" & \n            timestamp &gt;= hms::as_hms(\"00:09:52\") & timestamp &lt;= hms::as_hms(\"01:57:07\")        \n             ~ 1,\n            TRUE ~ 0\n        )\n    ) %&gt;% \n    group_by(prefix, id_streamer) %&gt;%\n    mutate(id_sequence = paste0(prefix, \"s\", sprintf(\"%04d\", row_number()))) %&gt;%\n    ungroup() %&gt;% \n    relocate(id_sequence) %&gt;% \n    select(-line, -prefix) \n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nid_sequence\nUnique identifier for each speaking sequence\n\n\nsource\nSource of the transcript (e.g., presidential_debate-abc)\n\n\nspeaker\nIdentifier for the speaker (e.g., S27)\n\n\ntimestamp\nTimestamp of the dialogue in HH:MM:SS format\n\n\ndialogue\nText of the dialogue\n\n\ndialogue_length\nLength of the dialogue text in characters\n\n\nduration\nDuration of the dialogue in seconds\n\n\ndebate\nType of debate (e.g., presidential, vice_presidential)\n\n\nstreamer\nSource of the stream (e.g., tv_station, hasanabi)\n\n\nid_streamer\nNumeric identifier for the streamer\n\n\nid_speaker\nUnique identifier for the speaker, combining debate type, streamer, and speaker\n\n\nsequence_during_debate\nIndicator if the sequence occurred during the debate (1 = yes, 0 = no)",
    "crumbs": [
      "Data collection",
      "Processing: Transcripts"
    ]
  },
  {
    "objectID": "data_collection/01_12-data_processing-transcripts.html#save-output",
    "href": "data_collection/01_12-data_processing-transcripts.html#save-output",
    "title": "Processing: Transcripts",
    "section": "Save output",
    "text": "Save output\n\nqs::qsave(\n    transcripts,\n    file = here(\"local_data/transcripts-debates_full.qs\")\n)\n\nqs::qsave(\n    transcripts$correct,\n    file = here(\"local_data/transcripts-debates.qs\")\n)",
    "crumbs": [
      "Data collection",
      "Processing: Transcripts"
    ]
  },
  {
    "objectID": "data_collection/01_21-corpora_chats-creation.html",
    "href": "data_collection/01_21-corpora_chats-creation.html",
    "title": "Corpus: Chats",
    "section": "",
    "text": "Information\n\n\n\nBased on the chat data, this script: - Creates a corpus, tokens, and a document-feature matrix with the quanteda package [v4.1.0, @benoit2018]. - Utilizes udpipe [v0.8.11, @wijffels2023] and spacyr [v1.3.0, @benoit2023] packages for additional linguistic processing, adding lemmatization, part-of-speech tagging, and named entity recognition.",
    "crumbs": [
      "Data collection",
      "Corpus: Chats"
    ]
  },
  {
    "objectID": "data_collection/01_21-corpora_chats-creation.html#preparation",
    "href": "data_collection/01_21-corpora_chats-creation.html#preparation",
    "title": "Corpus: Chats",
    "section": "Preparation",
    "text": "Preparation\n\n# Load packages\nsource(file = here::here(\n  \"data_collection/00_02-setup-session.R\"\n))\n\n\nchat &lt;- qs::qread(here(\"local_data/chat-debates_full.qs\"))\nchat_corpora &lt;- qs::qread(here(\"local_data/chat-corpora_full.qs\"))",
    "crumbs": [
      "Data collection",
      "Corpus: Chats"
    ]
  },
  {
    "objectID": "data_collection/01_21-corpora_chats-creation.html#process-data",
    "href": "data_collection/01_21-corpora_chats-creation.html#process-data",
    "title": "Corpus: Chats",
    "section": "Process data",
    "text": "Process data\n\nchat_corpora &lt;- list()\n\n# Create corpus\nchat_corpora$corp &lt;- chats$correct %&gt;% \n    quanteda::corpus(\n        docid_field = \"message_id\", \n        text_field = \"message_content\"\n  )\n\n# Create tokens\nchat_corpora$toks &lt;- chat_corpora$corp %&gt;% \n    quanteda::tokens() \n\n# Create Document Feature Matrix (DFM)\nchat_corpora$dfm &lt;- chat_corpora$toks %&gt;% \n    quanteda::dfm()\n\n\n# Execute on first run, to download the model \n# udmodel &lt;- udpipe::udpipe_download_model(\n#     language = \"english\",\n#     model_dir = here(\"models\"))\n\n# Load udpipe model\nudmodel_english &lt;- udpipe::udpipe_load_model(file = here(\"models/english-ewt-ud-2.5-191206.udpipe\"))\n\nchat_corpora$udpipe &lt;- chat$correct %&gt;% \n  rename(\n    doc_id = message_id,\n    text = message_content\n  ) %&gt;% \n  udpipe::udpipe(udmodel_english)\n\n\n# Define environment\nreticulate::use_virtualenv(\"r-spacyr\")\n\n# Initialize\nspacyr::spacy_download_langmodel(\"en_core_web_sm\")\nspacyr::spacy_initialize(\"en_core_web_sm\")\n\nchat_corpora$spacyr &lt;- chat_corpora$corp %&gt;% \n    spacyr::spacy_parse(.,\n        tag = TRUE,\n        pos = TRUE,\n        lemma = TRUE,\n        dependency = TRUE,\n        multithread = TRUE\n    )",
    "crumbs": [
      "Data collection",
      "Corpus: Chats"
    ]
  },
  {
    "objectID": "data_collection/01_21-corpora_chats-creation.html#save-data",
    "href": "data_collection/01_21-corpora_chats-creation.html#save-data",
    "title": "Corpus: Chats",
    "section": "Save data",
    "text": "Save data\n\n# Save complete data\nqs::qsave(\n    chat_corpora,\n    file = here(\"local_data/chat-corpora_full.qs\")\n)\n\n# Save udpipe corpus\nqs::qsave(\n    chat_corpora$udpipe, \n    file = here(\"local_data/chat-corpus_udpipe.qs\")\n)\n\n# Save spacyr corpus\nqs::qsave(\n    chat_corpora$spacyr, \n    file = here(\"local_data/chat-corpus_spacyr.qs\")\n)",
    "crumbs": [
      "Data collection",
      "Corpus: Chats"
    ]
  },
  {
    "objectID": "data_collection/02_01-qc-chat-emotes.html",
    "href": "data_collection/02_01-qc-chat-emotes.html",
    "title": "Quality Control: Chat Emotes",
    "section": "",
    "text": "Information\n\n\n\nThis document outlines the process of quality control for chat emotes in a dataset. The key steps include:\n\nExtracting and processing emote information from chat messages.\nGenerating frequency tables for emotes and the number of emotes per message.\nIdentifying and handling cases where emote names are missing.\nCreating and saving a dictionary of unique emotes for further analysis.",
    "crumbs": [
      "Data collection",
      "Quality Control: Chat Emotes"
    ]
  },
  {
    "objectID": "data_collection/02_01-qc-chat-emotes.html#preparation",
    "href": "data_collection/02_01-qc-chat-emotes.html#preparation",
    "title": "Quality Control: Chat Emotes",
    "section": "Preparation",
    "text": "Preparation\n\n# Load packages\nsource(here::here(\"data_collection/00_02-setup-session.R\"))\n\n\n# Load data\nchat &lt;- qs::qread(here(\"local_data/chat-debates_full.qs\"))",
    "crumbs": [
      "Data collection",
      "Quality Control: Chat Emotes"
    ]
  },
  {
    "objectID": "data_collection/02_01-qc-chat-emotes.html#create-list-of-emotes",
    "href": "data_collection/02_01-qc-chat-emotes.html#create-list-of-emotes",
    "title": "Quality Control: Chat Emotes",
    "section": "Create list of emotes",
    "text": "Create list of emotes\n\n\n\n\n\n\nNote\n\n\n\nFor example images of the emojis visit\n\nhttps://twitchemotes.com/ or\nhttps://www.twitchmetrics.net/emotes\n\n\n\n\nemotes &lt;- chat$correct %&gt;%\n    select(message_id, message_emotes) %&gt;%\n    unnest(message_emotes) %&gt;%\n    mutate(\n        emote_id = sapply(message_emotes, function(emote) emote$id),\n        emote_name = sapply(message_emotes, function(emote) emote$name),\n        emote_locations = sapply(message_emotes, function(emote) paste(emote$locations, collapse = \", \"))\n    ) %&gt;%\n    select(message_id, emote_id, emote_name, emote_locations)\n\n\nemotes %&gt;% \n    frq(emote_name, sort.frq = \"desc\", min.frq = 25) %&gt;% \n    data.frame() %&gt;% \n    select(val, frq, raw.prc, cum.prc) %&gt;% \n    rownames_to_column(\"rank\") %&gt;%\n    rename(\"Emote\" = val, \"n\" = frq) %&gt;%\n    gt() %&gt;% \n    gtExtras::gt_theme_538() %&gt;% \n    tab_options(table.width = pct(80))\n\n\n\n\n\n\n\nrank\nEmote\nn\nraw.prc\ncum.prc\n\n\n\n\n1\nLUL\n14364\n20.38\n20.4\n\n\n2\nhasL\n5640\n8.00\n28.4\n\n\n3\nbleedPurple\n5171\n7.34\n35.7\n\n\n4\nKappa\n4154\n5.89\n41.6\n\n\n5\n&lt;3\n1789\n2.54\n44.1\n\n\n6\nNotLikeThis\n1322\n1.88\n46.0\n\n\n7\nhasChud\n1154\n1.64\n47.7\n\n\n8\n:)\n1050\n1.49\n49.1\n\n\n9\nelbyBlom\n1001\n1.42\n50.6\n\n\n10\nhasSlam\n965\n1.37\n51.9\n\n\n11\nhasHi\n840\n1.19\n53.1\n\n\n12\nWutFace\n681\n0.97\n54.1\n\n\n13\n:(\n675\n0.96\n55.1\n\n\n14\nhasMods\n593\n0.84\n55.9\n\n\n15\nhasO\n576\n0.82\n56.7\n\n\n16\nhasBoot\n540\n0.77\n57.5\n\n\n17\n:D\n538\n0.76\n58.2\n\n\n18\nhasRaid\n456\n0.65\n58.9\n\n\n19\nhasSadge\n443\n0.63\n59.5\n\n\n20\nhasBaited\n384\n0.54\n60.1\n\n\n21\nhasKkona\n374\n0.53\n60.6\n\n\n22\nhasHmm\n368\n0.52\n61.1\n\n\n23\nPopNemo\n331\n0.47\n61.6\n\n\n24\n4Head\n305\n0.43\n62.0\n\n\n25\nhasCapital\n292\n0.41\n62.4\n\n\n26\nhas0head\n286\n0.41\n62.8\n\n\n27\nhasFlex\n283\n0.40\n63.2\n\n\n28\nSeemsGood\n271\n0.38\n63.6\n\n\n29\nDinoDance\n267\n0.38\n64.0\n\n\n30\nResidentSleeper\n261\n0.37\n64.4\n\n\n31\nTheIlluminati\n255\n0.36\n64.7\n\n\n32\nR)\n245\n0.35\n65.1\n\n\n33\nSUBtember\n234\n0.33\n65.4\n\n\n34\nTwitchConHYPE\n232\n0.33\n65.8\n\n\n35\nDoritosChip\n230\n0.33\n66.1\n\n\n36\nhasWeird\n226\n0.32\n66.4\n\n\n37\nJebaited\n217\n0.31\n66.7\n\n\n38\nhasPray\n214\n0.30\n67.0\n\n\n39\nhasPOGGIES\n213\n0.30\n67.3\n\n\n40\nhasLeft\n211\n0.30\n67.6\n\n\n41\nhasRight\n204\n0.29\n67.9\n\n\n42\nPogChamp\n204\n0.29\n68.2\n\n\n43\nhasSmol\n201\n0.29\n68.5\n\n\n44\n\n200\n0.28\n68.8\n\n\n45\nBatChest\n196\n0.28\n69.0\n\n\n46\n:face_with_tears_of_joy:\n192\n0.27\n69.3\n\n\n47\n:rolling_on_the_floor_laughing:\n178\n0.25\n69.6\n\n\n48\nMrDestructoid\n167\n0.24\n69.8\n\n\n49\nhasKapp\n152\n0.22\n70.0\n\n\n50\nBibleThump\n149\n0.21\n70.2\n\n\n51\n:coconut:\n142\n0.20\n70.4\n\n\n52\nhasPause\n137\n0.19\n70.6\n\n\n53\nDansGame\n136\n0.19\n70.8\n\n\n54\nhasSilly\n135\n0.19\n71.0\n\n\n55\nhasWut\n134\n0.19\n71.2\n\n\n56\n:O\n129\n0.18\n71.4\n\n\n57\nhasREE\n125\n0.18\n71.5\n\n\n58\nhasWhat\n124\n0.18\n71.7\n\n\n59\ndsaFP\n123\n0.17\n71.9\n\n\n60\nhasChair\n119\n0.17\n72.1\n\n\n61\nFailFish\n117\n0.17\n72.2\n\n\n62\nvioSASS\n117\n0.17\n72.4\n\n\n63\nhasNerd\n112\n0.16\n72.6\n\n\n64\nhasPains\n110\n0.16\n72.7\n\n\n65\nVoteYea\n110\n0.16\n72.9\n\n\n66\nTransgenderPride\n107\n0.15\n73.0\n\n\n67\natpRtsd\n106\n0.15\n73.2\n\n\n68\nBabyRage\n106\n0.15\n73.3\n\n\n69\nL\n106\n0.15\n73.5\n\n\n70\n:P\n104\n0.15\n73.6\n\n\n71\nPoroSad\n104\n0.15\n73.8\n\n\n72\nSMOrc\n102\n0.14\n73.9\n\n\n73\ndsaL\n100\n0.14\n74.1\n\n\n74\n:/\n96\n0.14\n74.2\n\n\n75\ndsaNODDERS\n94\n0.13\n74.3\n\n\n76\nrathboPALESTINEHEART\n94\n0.13\n74.5\n\n\n77\nhasRage\n91\n0.13\n74.6\n\n\n78\nhasPOGGERS\n88\n0.12\n74.7\n\n\n79\nHeyGuys\n84\n0.12\n74.8\n\n\n80\nCoolStoryBob\n83\n0.12\n75.0\n\n\n81\nhasRant\n83\n0.12\n75.1\n\n\n82\nhas5\n81\n0.11\n75.2\n\n\n83\ndsaKEKW\n79\n0.11\n75.3\n\n\n84\nCurseLit\n77\n0.11\n75.4\n\n\n85\nBigSad\n73\n0.10\n75.5\n\n\n86\nhasGun\n73\n0.10\n75.6\n\n\n87\nWhySoSerious\n73\n0.10\n75.7\n\n\n88\n;)\n70\n0.10\n75.8\n\n\n89\nPopCorn\n69\n0.10\n75.9\n\n\n90\nhasSpooked\n66\n0.09\n76.0\n\n\n91\nSquid2\n65\n0.09\n76.1\n\n\n92\natpCap\n63\n0.09\n76.2\n\n\n93\nSquid1\n63\n0.09\n76.3\n\n\n94\n:grinning_squinting_face:\n62\n0.09\n76.4\n\n\n95\nSquid3\n62\n0.09\n76.5\n\n\n96\nSquid4\n62\n0.09\n76.5\n\n\n97\nhasPog\n61\n0.09\n76.6\n\n\n98\nhasComfy\n60\n0.09\n76.7\n\n\n99\nhasBuff\n59\n0.08\n76.8\n\n\n100\nhasUnless\n59\n0.08\n76.9\n\n\n101\nGoldPLZ\n57\n0.08\n77.0\n\n\n102\nhasZzz\n54\n0.08\n77.0\n\n\n103\nKomodoHype\n54\n0.08\n77.1\n\n\n104\nGoatEmotey\n53\n0.08\n77.2\n\n\n105\nhasPrime\n53\n0.08\n77.3\n\n\n106\nhasStop\n53\n0.08\n77.3\n\n\n107\nShush\n51\n0.07\n77.4\n\n\n108\nCoolCat\n50\n0.07\n77.5\n\n\n109\nüáµüá∏\n49\n0.07\n77.5\n\n\n110\ndsaKnee\n49\n0.07\n77.6\n\n\n111\nhasWicked\n49\n0.07\n77.7\n\n\n112\nPowerUpR\n49\n0.07\n77.8\n\n\n113\nCaitlynS\n48\n0.07\n77.8\n\n\n114\nhasHug\n48\n0.07\n77.9\n\n\n115\nHypeLUL\n48\n0.07\n78.0\n\n\n116\nKappaPride\n48\n0.07\n78.0\n\n\n117\nhasHAAA\n47\n0.07\n78.1\n\n\n118\nironmouseLOVE\n47\n0.07\n78.2\n\n\n119\nsL\n47\n0.07\n78.2\n\n\n120\nstrug4Free\n47\n0.07\n78.3\n\n\n121\nFBCatch\n46\n0.07\n78.4\n\n\n122\nOSFrog\n46\n0.07\n78.4\n\n\n123\nPowerUpL\n46\n0.07\n78.5\n\n\n124\nGlitchCat\n45\n0.06\n78.6\n\n\n125\nhasMad\n45\n0.06\n78.6\n\n\n126\nhasSmash\n45\n0.06\n78.7\n\n\n127\nhasTruth\n45\n0.06\n78.8\n\n\n128\nMaxLOL\n45\n0.06\n78.8\n\n\n129\nScaredyCat\n44\n0.06\n78.9\n\n\n130\n:face-fuchsia-tongue-out:\n42\n0.06\n78.9\n\n\n131\nhasD\n42\n0.06\n79.0\n\n\n132\nOhMyDog\n42\n0.06\n79.1\n\n\n133\nDatSheffy\n41\n0.06\n79.1\n\n\n134\nFBtouchdown\n41\n0.06\n79.2\n\n\n135\nPJSugar\n41\n0.06\n79.2\n\n\n136\nTTours\n41\n0.06\n79.3\n\n\n137\n:face-green-smiling:\n40\n0.06\n79.3\n\n\n138\nrathboFREE\n39\n0.06\n79.4\n\n\n139\ndsaCOPIUM\n38\n0.05\n79.5\n\n\n140\nhasBOOMER\n38\n0.05\n79.5\n\n\n141\n:fire:\n37\n0.05\n79.6\n\n\n142\n:p\n37\n0.05\n79.6\n\n\n143\nBloodTrail\n37\n0.05\n79.7\n\n\n144\nEleGiggle\n37\n0.05\n79.7\n\n\n145\nFBBlock\n37\n0.05\n79.8\n\n\n146\nKeepo\n37\n0.05\n79.8\n\n\n147\nrathboEST\n37\n0.05\n79.9\n\n\n148\nrathboPAL\n37\n0.05\n79.9\n\n\n149\ngremloeFP\n36\n0.05\n80.0\n\n\n150\nhasEZ\n36\n0.05\n80.0\n\n\n151\nrathboINE\n36\n0.05\n80.1\n\n\n152\n:palm_tree:\n35\n0.05\n80.1\n\n\n153\nrathboFREEPALESTINE\n35\n0.05\n80.2\n\n\n154\nyugopnXISALUTE\n35\n0.05\n80.2\n\n\n155\nstrug4F12\n34\n0.05\n80.3\n\n\n156\nDarkMode\n32\n0.05\n80.3\n\n\n157\ndsaFacepalm\n32\n0.05\n80.4\n\n\n158\nmermai40Heart\n32\n0.05\n80.4\n\n\n159\nvioSLUDGE\n32\n0.05\n80.5\n\n\n160\nyugopnRAID\n32\n0.05\n80.5\n\n\n161\nBopBop\n31\n0.04\n80.5\n\n\n162\ndsaBrooks\n31\n0.04\n80.6\n\n\n163\nhasSammie\n31\n0.04\n80.6\n\n\n164\nPokPikachu\n31\n0.04\n80.7\n\n\n165\nPotFriend\n31\n0.04\n80.7\n\n\n166\nforsenBased\n30\n0.04\n80.8\n\n\n167\n:o\n29\n0.04\n80.8\n\n\n168\nhasKomrade\n29\n0.04\n80.8\n\n\n169\nGayPride\n28\n0.04\n80.9\n\n\n170\nironmouseSABERDANCE\n28\n0.04\n80.9\n\n\n171\nPixelBob\n28\n0.04\n81.0\n\n\n172\nVoHiYo\n28\n0.04\n81.0\n\n\n173\nüá∫üá¶\n27\n0.04\n81.0\n\n\n174\nCarlSmile\n27\n0.04\n81.1\n\n\n175\nckwanFartdance\n27\n0.04\n81.1\n\n\n176\nMiniK\n27\n0.04\n81.2\n\n\n177\nvioFP\n27\n0.04\n81.2\n\n\n178\nVoteNay\n27\n0.04\n81.2\n\n\n179\nyugopnPal\n27\n0.04\n81.3\n\n\n180\n:orange_circle:\n26\n0.04\n81.3\n\n\n181\nforsenLaughingAtYou\n26\n0.04\n81.3\n\n\n182\nFortOne\n26\n0.04\n81.4\n\n\n183\nhasFatty\n26\n0.04\n81.4\n\n\n184\nhasGachi\n26\n0.04\n81.5\n\n\n185\nKappaHD\n26\n0.04\n81.5\n\n\n186\nKEKHeim\n26\n0.04\n81.5\n\n\n187\nrhyzROT\n26\n0.04\n81.6\n\n\n188\n:loudly_crying_face:\n25\n0.04\n81.6\n\n\n189\ndsaDance\n25\n0.04\n81.7\n\n\n190\ndsaHmm\n25\n0.04\n81.7\n\n\n191\nironmouseWiggly\n25\n0.04\n81.7\n\n\n192\nPogBones\n25\n0.04\n81.8\n\n\n193\nn &lt; 25\n12861\n18.25\n100.0\n\n\n194\nNA\n0\n0.00\nNA\n\n\n\n\n\n\n\n\nemotes %&gt;% \n    group_by(message_id) %&gt;% \n    summarise( n = n()) %&gt;% \n    frq(n) %&gt;% \n    data.frame() %&gt;% \n    select(val, frq, raw.prc, cum.prc) %&gt;%\n    rename(\"Number of Emotes\" = val, \"n\" = frq) %&gt;%\n    gt() %&gt;% \n    gtExtras::gt_theme_538()    \n\n\n\n\n\n\n\nNumber of Emotes\nn\nraw.prc\ncum.prc\n\n\n\n\n1\n62534\n94.93\n94.9\n\n\n2\n2465\n3.74\n98.7\n\n\n3\n612\n0.93\n99.6\n\n\n4\n178\n0.27\n99.9\n\n\n5\n55\n0.08\n100.0\n\n\n6\n13\n0.02\n100.0\n\n\n7\n9\n0.01\n100.0\n\n\n8\n4\n0.01\n100.0\n\n\n10\n1\n0.00\n100.0\n\n\n13\n1\n0.00\n100.0\n\n\nNA\n0\n0.00\nNA",
    "crumbs": [
      "Data collection",
      "Quality Control: Chat Emotes"
    ]
  },
  {
    "objectID": "data_collection/02_01-qc-chat-emotes.html#check-empty-emotes",
    "href": "data_collection/02_01-qc-chat-emotes.html#check-empty-emotes",
    "title": "Quality Control: Chat Emotes",
    "section": "Check: ‚ÄúEmpty emotes‚Äù",
    "text": "Check: ‚ÄúEmpty emotes‚Äù\n\nemotes %&gt;% \n    group_by(emote_name, emote_id) %&gt;% \n    summarise(n = n()) %&gt;% \n    arrange(desc(n)) %&gt;% \n    filter(emote_name == \"\") %&gt;%\n    gt() %&gt;% \n    gtExtras::gt_theme_538()\n\n`summarise()` has grouped output by 'emote_name'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\nemote_id\nn\n\n\n\n\n\n\n\n425618\n53\n\n\n302140936\n29\n\n\n555555558\n21\n\n\n555555584\n21\n\n\n1\n13\n\n\n555555560\n11\n\n\n555555580\n8\n\n\n306629700\n5\n\n\n300238152\n4\n\n\n302587115\n3\n\n\n303446392\n3\n\n\nemotesv2_4f058d58458544a4971de55672468204\n3\n\n\n300238154\n2\n\n\n300756431\n2\n\n\n555555577\n2\n\n\n555555589\n2\n\n\n300238151\n1\n\n\n300238155\n1\n\n\n301079765\n1\n\n\n303433990\n1\n\n\n303706436\n1\n\n\n304192517\n1\n\n\n354\n1\n\n\n489\n1\n\n\n508650\n1\n\n\n555555557\n1\n\n\n555555563\n1\n\n\n555555585\n1\n\n\n6\n1\n\n\n626795\n1\n\n\n62835\n1\n\n\nemotesv2_1c432fe325994220960ed5720682ca63\n1\n\n\nemotesv2_819621bcb8f44566a1bd8ea63d06c58f\n1\n\n\nemotesv2_89106685bdb643c2943994b027e25556\n1\n\n\n\n\n\n\n\n\nemote_name_recode &lt;- emotes %&gt;%\n    group_by(emote_id, emote_name) %&gt;%\n    summarise(n = n(), .groups = 'drop') %&gt;%\n    arrange(desc(n)) %&gt;%\n    group_by(emote_id) %&gt;%\n    filter(n == max(n)) %&gt;%\n    slice(1) %&gt;%\n    select(emote_id, emote_name)\n\nemotes_recoded &lt;- emotes %&gt;%\n    left_join(emote_name_recode, by = \"emote_id\", suffix = c(\"\", \"_recode\")) %&gt;%\n    mutate(emote_name = ifelse(!is.na(emote_name_recode), emote_name_recode, emote_name)) %&gt;%\n    select(-emote_name_recode)",
    "crumbs": [
      "Data collection",
      "Quality Control: Chat Emotes"
    ]
  },
  {
    "objectID": "data_collection/02_01-qc-chat-emotes.html#create-emote-dictionary",
    "href": "data_collection/02_01-qc-chat-emotes.html#create-emote-dictionary",
    "title": "Quality Control: Chat Emotes",
    "section": "Create emote dictionary",
    "text": "Create emote dictionary\n\n# Extract unique emotes\nemotes_recoded_names &lt;- emotes_recoded %&gt;% \n    distinct(emote_name, .keep_all = TRUE) %&gt;% \n    arrange(emote_name) %&gt;% \n    filter(emote_name != \"\") %&gt;%\n    pull(emote_name)\n\n# Ensure the list is named\nnamed_emotes_recoded_names &lt;- setNames(\n  as.list(emotes_recoded_names), emotes_recoded_names)\n\n# Create a dictionary\ndict_emotes &lt;- dictionary(named_emotes_recoded_names)\n\n# Save the dictionary to a file\nsaveRDS(dict_emotes, file = here(project_dir, \"local_data/dictionary_chat_emotes.RDS\"))",
    "crumbs": [
      "Data collection",
      "Quality Control: Chat Emotes"
    ]
  },
  {
    "objectID": "course-presentations.html",
    "href": "course-presentations.html",
    "title": "Course presentations",
    "section": "",
    "text": "Zimmer, F., Scheibe, K., & Stock, W. G. (2018). A Model for Information Behavior Research on Social Live Streaming Services (SLSSs) (G. Meiselwitz, Ed.; pp. 429‚Äì448). Springer International Publishing. https://doi.org/10.1007/978-3-319-91485-5_33\nHou, F., Guan, Z., Li, B., & Chong, A. Y. L. (2020). Factors influencing people‚Äôs continuous watching intention and consumption intention in live streaming. Internet Research, 30(1), 141‚Äì163. https://doi.org/10.1108/INTR-04-2018-0177\nCabeza-Ram√≠rez, L. J., Fuentes-Garc√≠a, F. J., & Mu√±oz-Fernandez, G. A. (2021). Exploring the Emerging Domain of Research on Video Game Live Streaming in Web of Science: State of the Art, Changes and Trends. International Journal of Environmental Research and Public Health, 18(6), 2917. https://doi.org/10.3390/ijerph18062917\nNguyen, T. T., & Veer, E. (2024). Why people watch user-generated videos? A systematic review and meta-analysis. International Journal of Human-Computer Studies, 181, 103144. https://doi.org/10.1016/j.ijhcs.2023.103144\n\n\nXu, Y., Kapitan, S., & Phillips, M. (2023). The commercial impact of live streaming: A systematic literature review and future research agenda. International Journal of Consumer Studies, 47(6), 2495‚Äì2527. https://doi.org/10.1111/ijcs.12960\n\n\n\n\nGros, D., Hackenholt, A., Zawadzki, P., & Wanner, B. (2018). Interactions of Twitch Users and Their Usage Behavior (G. Meiselwitz, Ed.; pp. 201‚Äì213). Springer International Publishing. https://doi.org/10.1007/978-3-319-91485-5_15\nWolff, G. H., & Shen, C. (2022). Audience size, moderator activity, gender, and content diversity: Exploring user participation and financial commitment on Twitch.tv. New Media & Society, 146144482110699. https://doi.org/10.1177/14614448211069996\nGamir-R√≠os, J., Cano-Or√≥n, L., & Garc√≠a-Casas, D. (2024). Twitch‚Äôs Second Phase of Development: Analyzing Streamer Profiles and Content Trends That Boost Its Evolution into a Mass Media. Games and Culture, 15554120241257030. https://doi.org/10.1177/15554120241257030\nCastro-Agirre, I., & Mart√≠nez-Fern√°ndez, G. (2024). From gamer niche to mainstream media: Twitch‚Äôs most popular media figures and content. Communication & Society, 179‚Äì196. https://doi.org/10.15581/003.37.2.179-196"
  },
  {
    "objectID": "course-presentations.html#introduction-to-social-live-streaming-services",
    "href": "course-presentations.html#introduction-to-social-live-streaming-services",
    "title": "Course presentations",
    "section": "",
    "text": "Zimmer, F., Scheibe, K., & Stock, W. G. (2018). A Model for Information Behavior Research on Social Live Streaming Services (SLSSs) (G. Meiselwitz, Ed.; pp. 429‚Äì448). Springer International Publishing. https://doi.org/10.1007/978-3-319-91485-5_33\nHou, F., Guan, Z., Li, B., & Chong, A. Y. L. (2020). Factors influencing people‚Äôs continuous watching intention and consumption intention in live streaming. Internet Research, 30(1), 141‚Äì163. https://doi.org/10.1108/INTR-04-2018-0177\nCabeza-Ram√≠rez, L. J., Fuentes-Garc√≠a, F. J., & Mu√±oz-Fernandez, G. A. (2021). Exploring the Emerging Domain of Research on Video Game Live Streaming in Web of Science: State of the Art, Changes and Trends. International Journal of Environmental Research and Public Health, 18(6), 2917. https://doi.org/10.3390/ijerph18062917\nNguyen, T. T., & Veer, E. (2024). Why people watch user-generated videos? A systematic review and meta-analysis. International Journal of Human-Computer Studies, 181, 103144. https://doi.org/10.1016/j.ijhcs.2023.103144\n\n\nXu, Y., Kapitan, S., & Phillips, M. (2023). The commercial impact of live streaming: A systematic literature review and future research agenda. International Journal of Consumer Studies, 47(6), 2495‚Äì2527. https://doi.org/10.1111/ijcs.12960\n\n\n\n\nGros, D., Hackenholt, A., Zawadzki, P., & Wanner, B. (2018). Interactions of Twitch Users and Their Usage Behavior (G. Meiselwitz, Ed.; pp. 201‚Äì213). Springer International Publishing. https://doi.org/10.1007/978-3-319-91485-5_15\nWolff, G. H., & Shen, C. (2022). Audience size, moderator activity, gender, and content diversity: Exploring user participation and financial commitment on Twitch.tv. New Media & Society, 146144482110699. https://doi.org/10.1177/14614448211069996\nGamir-R√≠os, J., Cano-Or√≥n, L., & Garc√≠a-Casas, D. (2024). Twitch‚Äôs Second Phase of Development: Analyzing Streamer Profiles and Content Trends That Boost Its Evolution into a Mass Media. Games and Culture, 15554120241257030. https://doi.org/10.1177/15554120241257030\nCastro-Agirre, I., & Mart√≠nez-Fern√°ndez, G. (2024). From gamer niche to mainstream media: Twitch‚Äôs most popular media figures and content. Communication & Society, 179‚Äì196. https://doi.org/10.15581/003.37.2.179-196"
  },
  {
    "objectID": "course-presentations.html#motivations-for-using-slss",
    "href": "course-presentations.html#motivations-for-using-slss",
    "title": "Course presentations",
    "section": "Motivations for using SLSS",
    "text": "Motivations for using SLSS\n\nTopic 3: By streamers\nZimmer, F., & Scheibe, K. (2019). Hawaii international conference on system sciences. https://doi.org/10.24251/HICSS.2019.306\nScheibe, K., Zimmer, F., Fietkiewicz, K., & Stock, W. (2022). Interpersonal Relations and Social Actions on Live Streaming Services. A Systematic Review on Cyber-social Relations. http://hdl.handle.net/10125/79744\nYoung, A., & Wiedenfeld, G. (2022). A Motivation Analysis of Video Game Microstreamers: ‚ÄúFinding My People and Myself‚Äù on YouTube and Twitch. Journal of Broadcasting & Electronic Media, 66(2), 381‚Äì399. https://doi.org/10.1080/08838151.2022.2086549\nXi, D., Xu, W., Tang, L., & Han, B. (2024). The impact of streamer emotions on viewer gifting behavior: Evidence from entertainment live streaming. Internet Research, 34(3), 748‚Äì783. https://doi.org/10.1108/INTR-05-2022-0350\nTomlinson, C. (2024). Community Grievances, personal responsibility, and DIY protection: Frustrations and solution-seeking among marginalized Twitch streamers. Convergence: The International Journal of Research into New Media Technologies, 30(1), 358‚Äì374. https://doi.org/10.1177/13548565231184060\n\nOptional:\nChinchilla, P., & Kim, J. (2024). ‚ÄúLet‚Äôs Chill and Chat‚Äù: Exploring the Effects of Streamers‚Äô Self-Disclosure on Parasocial Interaction via Social Presence. International Journal of HumanComputer Interaction, 1‚Äì11. https://doi.org/10.1080/10447318.2024.2390263\n\n\n\nTopic 4: By users\nHilvert-Bruce, Z., Neill, J. T., Sj√∂blom, M., & Hamari, J. (2018). Social motivations of live-streaming viewer engagement on twitch. Computers in Human Behavior, 84, 58‚Äì67. https://doi.org/10.1016/j.chb.2018.02.013\nGoh, Z. H., Tandoc, E. C., & Ng, B. (2021). ‚ÄúLive‚Äù Together with You: Livestream Views Mitigate the Effects of Loneliness on Well-being. Journal of Broadcasting & Electronic Media, 65(4), 505‚Äì524. https://doi.org/10.1080/08838151.2021.1994970\nLeith, A. P., & Gheen, E. (2022). Twitch in the time of quarantine: The role of engagement in needs fulfillment. Psychology of Popular Media, 11(3), 275‚Äì280. https://doi.org/10.1037/ppm0000372\nLessel, P., Altmeyer, M., Sahner, J., & Kr√ºger, A. (2022). Streamer‚Äôs hell - investigating audience influence in live-streams beyond the game. Proc. ACM Hum.-Comput. Interact., 6(CHI PLAY), 252:1252:27. https://doi.org/10.1145/3549515\nZsila, √Å., Shabahang, R., Aruguete, M. S., B≈ëthe, B., Gregor-T√≥th, P., & Orosz, G. (2024). Exploring the association between Twitch use and well-being. Psychology of Popular Media, 13(4), 620‚Äì632. https://doi.org/10.1037/ppm0000486\nKneisel, A., & Sternadori, M. (2023). Effects of parasocial affinity and gender on live streaming fans‚Äô motivations. Convergence, 29(2), 322‚Äì341. https://doi.org/10.1177/13548565221114461"
  },
  {
    "objectID": "course-presentations.html#effects-of-slss",
    "href": "course-presentations.html#effects-of-slss",
    "title": "Course presentations",
    "section": "Effects of SLSS",
    "text": "Effects of SLSS\n\nUse specific\nBr√ºndl, S., Matt, C., Hess, T., & Engert, S. (2023). How synchronous participation affects the willingness to subscribe to social live streaming services: The role of co-interactive behavior on twitch. European Journal of Information Systems, 32(5), 800‚Äì817. https://doi.org/10.1080/0960085X.2022.2062468\nNavarro, A., & Tapiador, F. J. (2023). Twitch as a privileged locus to analyze young people‚Äôs attitudes in the climate change debate: a quantitative analysis. Humanities and Social Sciences Communications, 10(1), 1‚Äì13. https://doi.org/10.1057/s41599-023-02377-4\n\n\nContent specific\nDutt, S., & Graham, S. (2023). Video, talk and text: How do parties communicate coherently across modalities in live videostreams? Discourse, Context and Media, 55. https://doi.org/10.1016/j.dcm.2023.100726\nMao, E. (2022). How live stream content types impact viewers‚Äô support behaviors? Mediational analysis on psychological and social gratifications. Frontiers in Psychology, 13. https://doi.org/10.3389/fpsyg.2022.951055\nRiddick, S., & Shivener, R. (2022). Affective spamming on twitch: Rhetorics of an emote-only audience in a presidential inauguration livestream. Computers and Composition, 64. https://doi.org/10.1016/j.compcom.2022.102711\n\nEmotions\nLacko, D., Dufkov√°, E., & Machackova, H. (2023). Does aggressive commentary by streamers during violent video game affect state aggression in adolescents? New Media & Society, 14614448231182620. https://doi.org/10.1177/14614448231182620\nLacko, D., Mach√°ƒçkov√°, H., & Smahel, D. (n.d.). Does violence in video games impact aggression and empathy? A longitudinal study of czech adolescents to differentiate within- and between-person effects. https://doi.org/10.31234/osf.io/dns8t"
  },
  {
    "objectID": "data_collection/01_01-data_collection-presidential_debate.html",
    "href": "data_collection/01_01-data_collection-presidential_debate.html",
    "title": "Mining: Presidential Debate",
    "section": "",
    "text": "Information\n\n\n\nThis document outlines the process of collecting live chat data from Twitch and YouTube VODs of the Presidential debate held on 10.09.2024. The steps taken include:\n\nSetting up the R and Python environments, including installing necessary packages.\nAuthenticating with Twitch using the twitchr package.\nPreparing a list of VOD URLs to be processed.\nDownloading the VODs using the twitch-dl tool.\nCollecting live chat messages from the VODs using the chat_downloader Python package.\nConverting the collected chat data into a pandas DataFrame and then into an R tibble.\nExporting the collected chat data to a local file for further analysis.",
    "crumbs": [
      "Data collection",
      "Mining: Presidential Debate"
    ]
  },
  {
    "objectID": "data_collection/01_01-data_collection-presidential_debate.html#preparation",
    "href": "data_collection/01_01-data_collection-presidential_debate.html#preparation",
    "title": "Mining: Presidential Debate",
    "section": "Preparation",
    "text": "Preparation\n\n# Setup R environment\npacman::p_load(\n    here, fs, \n    twitchr,\n    tidyverse,\n    reticulate\n)\n\n\n# Setup Python environment\nimport datetime\nimport email, smtplib, ssl\nimport pandas as pd\nimport rpy2.robjects as robjects\nimport twitchdl\n\nfrom chat_downloader import ChatDownloader\nfrom chat_downloader.sites import TwitchChatDownloader \n\n\n# twitch authorization\ntwitchr::twitch_auth()",
    "crumbs": [
      "Data collection",
      "Mining: Presidential Debate"
    ]
  },
  {
    "objectID": "data_collection/01_01-data_collection-presidential_debate.html#prepare-list-of-vods",
    "href": "data_collection/01_01-data_collection-presidential_debate.html#prepare-list-of-vods",
    "title": "Mining: Presidential Debate",
    "section": "Prepare list of VODs",
    "text": "Prepare list of VODs\n\ndebate_vods_urls &lt;- c(\n    # Twitch\n    \"https://www.twitch.tv/videos/2247664726\", #hasanabi\n    \"https://www.twitch.tv/videos/2247617457\", #zackrawrr\n    # # YouTube\n    \"https://www.youtube.com/watch?v=lzobJil9Sgc\" # Majority Report Live\n)",
    "crumbs": [
      "Data collection",
      "Mining: Presidential Debate"
    ]
  },
  {
    "objectID": "data_collection/01_01-data_collection-presidential_debate.html#prepare-the-download-links",
    "href": "data_collection/01_01-data_collection-presidential_debate.html#prepare-the-download-links",
    "title": "Mining: Presidential Debate",
    "section": "Prepare the download links",
    "text": "Prepare the download links\n\n# HasanAbi\ntwitch-dl download https://www.twitch.tv/videos/2247664726 --quality 720p30\n# zackrawrr\ntwitch-dl download https://www.twitch.tv/videos/2247617457 --quality 720p60",
    "crumbs": [
      "Data collection",
      "Mining: Presidential Debate"
    ]
  },
  {
    "objectID": "data_collection/01_01-data_collection-presidential_debate.html#collect-live-chat",
    "href": "data_collection/01_01-data_collection-presidential_debate.html#collect-live-chat",
    "title": "Mining: Presidential Debate",
    "section": "Collect live chat",
    "text": "Collect live chat\n\n# Assuming url_py is already a Python list of URLs from R\nurl_py = list(robjects.globalenv['debate_vods_urls'])\n\n# Initialize the ChatDownloader\nchat_downloader = ChatDownloader()\n\n# Initialize an empty list to store message data\nmessage_list = []\n\n# Function to generate a unique stream ID (can be URL or index-based)\ndef generate_stream_id(url, index):\n    return f\"stream_{index+1}\"\n\n# Debugging: Print the list of URLs\nprint(\"URLs to process:\", url_py)\n\n# Loop through each URL and download the chat\nfor idx, url in enumerate(url_py):\n    try:\n        print(f\"Processing URL: {url}\")\n        \n        # Fetch chat\n        chat = chat_downloader.get_chat(url)\n        if not chat:\n            print(f\"No chat data found for {url}\")\n            continue  # Skip to the next URL if no chat found\n\n        stream_id = generate_stream_id(url, idx)  # Generate a unique stream ID\n        \n        print(f\"Downloading chat for {url}\")\n        \n        for message in chat:\n            # Log message info for debugging\n            \n            # Extract message details\n            message_content = message.get('message', '')\n            message_id = message.get('message_id', None)\n            message_type = message.get('message_type', 'None')\n            timestamp = message.get('time_in_seconds', None)\n\n            # Extract author details (ensure the author field exists)\n            author_info = message.get('author', {})  # Unpack dictionary with author info\n            author_id = author_info.get('id', 'NA')  # Extract author ID\n            author_name = author_info.get('name', 'NA')  # Extract author name\n            author_type = author_info.get('type', 'NA')  # Extract type of author\n            author_gender = author_info.get('gender', 'NA')  # Extract gender of the author\n            author_bot = author_info.get('is_bot', 'NA')  # True if the user is a bot, False otherwise.\n            author_poster = author_info.get('is_original_poster', 'NA')  # True if the user is the original poster, False otherwise.\n            author_verified = author_info.get('is_verified', 'NA')  # True if the user is verified, False otherwise.\n            display_name = author_info.get('display_name', author_name)  # Extract display name\n            badges = author_info.get('badges', [])  # Keep badges as a list\n            emotes = message.get('emotes', [])  # Keep emotes as a list\n\n            # Create a dictionary representing one row of the tibble\n            message_with_info = {\n                'stream_id': stream_id,  # Add the stream ID\n                'url': url,  # Add the stream URL\n                'username': author_name,  \n                'user_id': author_id,  \n                'display_name': display_name,  \n                'user_type': author_type, \n                'user_gender': author_gender,\n                'user_is_bot': author_bot,\n                'user_is_original_poster': author_poster,\n                'user_is_verified': author_verified,\n                'badges': badges,  # Add the badges as a list\n                'emotes': emotes,  # Add the emotes as a list\n                'timestamp': timestamp,  # Add the message timestamp\n                'message_id': message_id,  # Add the message ID\n                'message_type': message_type,  # Add the message type\n                'message_content': message_content  # Add the actual message text\n            }\n\n            # Append the dictionary to the list\n            message_list.append(message_with_info)\n                \n    except Exception as e:\n        print(f\"Error processing {url}: {e}\")\n\n# Print the final list of messages collected\nprint(\"Collection finished\")\n\n\n# Convert the list of dictionaries to a pandas DataFrame\nmessage_df = pd.DataFrame(message_list)\n\n\n# Access the message_df from Python\ndf &lt;- py$message_df %&gt;% \n    as_tibble()\n\n# Check the structure of the tibble\ndf %&gt;% glimpse\n\n\n# Name of subdirectory for easier path managment\nproject_dir &lt;- here::here(\"2024-nlp_of_live_stream_chat\")\n\nqs::qsave(df, file = here(project_dir, \"local_data/chat_raw-vods_presidential_debate.qs\"))",
    "crumbs": [
      "Data collection",
      "Mining: Presidential Debate"
    ]
  },
  {
    "objectID": "data_collection/01_02-data_collection-vice_presidential_debate.html",
    "href": "data_collection/01_02-data_collection-vice_presidential_debate.html",
    "title": "Mining: Vice-Presidential Debate",
    "section": "",
    "text": "Information\n\n\n\nThis document outlines the process of collecting live chat data from Twitch VODs of the Vice-Presidential debate held on 02.10.2024. The steps taken include:\n\nSetting up the R and Python environments, including installing necessary packages.\nAuthenticating with Twitch using the twitchr package.\nPreparing a list of VOD URLs to be processed.\nDownloading the VODs using the twitch-dl tool.\nCollecting live chat messages from the VODs using the chat_downloader Python package.\nConverting the collected chat data into a pandas DataFrame and then into an R tibble.\nExporting the collected chat data to a local file for further analysis.",
    "crumbs": [
      "Data collection",
      "Mining: Vice-Presidential Debate"
    ]
  },
  {
    "objectID": "data_collection/01_02-data_collection-vice_presidential_debate.html#preparation",
    "href": "data_collection/01_02-data_collection-vice_presidential_debate.html#preparation",
    "title": "Mining: Vice-Presidential Debate",
    "section": "Preparation",
    "text": "Preparation\n\n# Setup R environment\npacman::p_load(\n    here, fs, \n    twitchr,\n    tidyverse,\n    reticulate\n)\n\n\n# Setup Python environment\nimport datetime\nimport email, smtplib, ssl\nimport pandas as pd\nimport rpy2.robjects as robjects\nimport twitchdl\n\nfrom chat_downloader import ChatDownloader\nfrom chat_downloader.sites import TwitchChatDownloader \n\n\n# twitch authorization\ntwitchr::twitch_auth()",
    "crumbs": [
      "Data collection",
      "Mining: Vice-Presidential Debate"
    ]
  },
  {
    "objectID": "data_collection/01_02-data_collection-vice_presidential_debate.html#prepare-list-of-vods",
    "href": "data_collection/01_02-data_collection-vice_presidential_debate.html#prepare-list-of-vods",
    "title": "Mining: Vice-Presidential Debate",
    "section": "Prepare list of VODs",
    "text": "Prepare list of VODs\n\ndebate_vods_urls &lt;- c(\n    # Twitch\n    \"https://www.twitch.tv/videos/2265091277\", #hasanabi\n    \"https://www.twitch.tv/videos/2265091311\", #zackrawrr\n    \"https://www.twitch.tv/videos/2265413840\" # Majority Report Live\n)\n\ns ## Prepare the download links\n\n# HasanAbi\ntwitch-dl download https://www.twitch.tv/videos/2265091277 --quality 720p30\n# zackrawrr\ntwitch-dl download https://www.twitch.tv/videos/2265091311 --quality 720p60\n# The Majority Report\ntwitch-dl download https://www.twitch.tv/videos/2265413840 --quality 720p30",
    "crumbs": [
      "Data collection",
      "Mining: Vice-Presidential Debate"
    ]
  },
  {
    "objectID": "data_collection/01_02-data_collection-vice_presidential_debate.html#collect-live-chat",
    "href": "data_collection/01_02-data_collection-vice_presidential_debate.html#collect-live-chat",
    "title": "Mining: Vice-Presidential Debate",
    "section": "Collect live chat",
    "text": "Collect live chat\n\n# Assuming url_py is already a Python list of URLs from R\nurl_py = list(robjects.globalenv['debate_vods_urls'])\n\n# Initialize the ChatDownloader\nchat_downloader = ChatDownloader()\n\n# Initialize an empty list to store message data\nmessage_list = []\n\n# Function to generate a unique stream ID (can be URL or index-based)\ndef generate_stream_id(url, index):\n    return f\"stream_{index+1}\"\n\n# Debugging: Print the list of URLs\nprint(\"URLs to process:\", url_py)\n\n# Loop through each URL and download the chat\nfor idx, url in enumerate(url_py):\n    try:\n        print(f\"Processing URL: {url}\")\n        \n        # Fetch chat\n        chat = chat_downloader.get_chat(url)\n        if not chat:\n            print(f\"No chat data found for {url}\")\n            continue  # Skip to the next URL if no chat found\n\n        stream_id = generate_stream_id(url, idx)  # Generate a unique stream ID\n        \n        print(f\"Downloading chat for {url}\")\n        \n        for message in chat:\n            # Log message info for debugging\n            \n            # Extract message details\n            message_content = message.get('message', '')\n            message_id = message.get('message_id', None)\n            message_type = message.get('message_type', 'None')\n            timestamp = message.get('time_in_seconds', None)\n\n            # Extract author details (ensure the author field exists)\n            author_info = message.get('author', {})  # Unpack dictionary with author info\n            author_id = author_info.get('id', 'NA')  # Extract author ID\n            author_name = author_info.get('name', 'NA')  # Extract author name\n            author_type = author_info.get('type', 'NA')  # Extract type of author\n            author_gender = author_info.get('gender', 'NA')  # Extract gender of the author\n            author_bot = author_info.get('is_bot', 'NA')  # True if the user is a bot, False otherwise.\n            author_poster = author_info.get('is_original_poster', 'NA')  # True if the user is the original poster, False otherwise.\n            author_verified = author_info.get('is_verified', 'NA')  # True if the user is verified, False otherwise.\n            author_moderator = author_info.get('is_moderator', 'NA')  # True if the user is a moderator, False otherwise.\n            author_subscriber = author_info.get('is_subscriber', 'NA')  # True if the user is a subscriber, False otherwise.\n            display_name = author_info.get('display_name', author_name)  # Extract display name\n            badges = author_info.get('badges', [])  # Keep badges as a list\n            emotes = message.get('emotes', [])  # Keep emotes as a list\n\n            # Create a dictionary representing one row of the tibble\n            message_with_info = {\n                'stream_id': stream_id,  # Add the stream ID\n                'url': url,  # Add the stream URL\n                'username': author_name,  \n                'user_id': author_id,  \n                'display_name': display_name,  \n                'user_type': author_type, \n                'user_gender': author_gender,\n                'user_is_bot': author_bot,\n                'user_is_original_poster': author_poster,\n                'user_is_verified': author_verified,\n                'user_is_moderator': author_moderator,\n                'user_is_subscriber': author_subscriber,\n                'badges': badges,  # Add the badges as a list\n                'emotes': emotes,  # Add the emotes as a list\n                'timestamp': timestamp,  # Add the message timestamp\n                'message_id': message_id,  # Add the message ID\n                'message_type': message_type,  # Add the message type\n                'message_content': message_content  # Add the actual message text\n            }\n\n            # Append the dictionary to the list\n            message_list.append(message_with_info)\n                \n    except Exception as e:\n        print(f\"Error processing {url}: {e}\")\n\n# Print the final list of messages collected\nprint(\"Collection finished\")\n\n\n# Convert the list of dictionaries to a pandas DataFrame\nmessage_df = pd.DataFrame(message_list)\n\n\n# Access the message_df from Python\ndf &lt;- py$message_df %&gt;% \n    as_tibble()\n\n# Check the structure of the tibble\ndf %&gt;% glimpse\n\n\n# Name of subdirectory for easier path managment\nproject_dir &lt;- here::here(\"2024-nlp_of_live_stream_chat\")\n\nqs::qsave(df, file = here(project_dir, \"local_data/chat_raw-vods_vice_presidential_debate.qs\"))",
    "crumbs": [
      "Data collection",
      "Mining: Vice-Presidential Debate"
    ]
  },
  {
    "objectID": "data_collection/01_22-corpora_transcripts-creation.html",
    "href": "data_collection/01_22-corpora_transcripts-creation.html",
    "title": "Corpus: Transcripts",
    "section": "",
    "text": "Information\n\n\n\nBased on the transcript data, this script:\n\nCreates a corpus, tokens, and a document-feature matrix with the quanteda package (v4.1.0, Benoit et al. 2018).\nUtilizes udpipe (v0.8.11, Wijffels 2023) and spacyr (v1.3.0, Benoit and Matsuo 2023) packages for additional linguistic processing, adding lemmatization, part-of-speech tagging, and named entity recognition.",
    "crumbs": [
      "Data collection",
      "Corpus: Transcripts"
    ]
  },
  {
    "objectID": "data_collection/01_22-corpora_transcripts-creation.html#preparation",
    "href": "data_collection/01_22-corpora_transcripts-creation.html#preparation",
    "title": "Corpus: Transcripts",
    "section": "Preparation",
    "text": "Preparation\n\n# Load packages\nsource(file = here::here(\n  \"data_collection/00_02-setup-session.R\"\n))\n\n\ntranscripts &lt;- qs::qread(here(\"local_data/transcripts-debates_full.qs\"))",
    "crumbs": [
      "Data collection",
      "Corpus: Transcripts"
    ]
  },
  {
    "objectID": "data_collection/01_22-corpora_transcripts-creation.html#process-data",
    "href": "data_collection/01_22-corpora_transcripts-creation.html#process-data",
    "title": "Corpus: Transcripts",
    "section": "Process data",
    "text": "Process data\n\ntranscripts_corpora &lt;- list()\n\n# Create corpus\ntranscripts_corpora$corp &lt;- transcripts$correct %&gt;% \n    quanteda::corpus(\n        docid_field = \"id_sequence\", \n        text_field = \"dialogue\"\n  )\n\n# Create tokens\ntranscripts_corpora$toks &lt;- transcripts_corpora$corp %&gt;% \n    quanteda::tokens(\n        remove_punct = TRUE, \n        remove_symbols = TRUE,\n        remove_numbers = TRUE,\n        remove_url = TRUE, \n        split_hyphens = FALSE,\n        split_tags = FALSE\n        ) %&gt;% \n    quanteda::tokens_remove(\n        pattern = quanteda::stopwords(\"en\")\n    )\n\n# Create Document Feature Matrix (DFM)\ntranscripts_corpora$dfm &lt;- transcripts_corpora$toks %&gt;% \n    quanteda::dfm()\n\n\n# Execute on first run, to download the model \n# udmodel &lt;- udpipe::udpipe_download_model(\n#     language = \"english\",\n#     model_dir = here(\"models\"))\n\n# Load udpipe model\nudmodel_english &lt;- udpipe::udpipe_load_model(file = here(\"models/english-ewt-ud-2.5-191206.udpipe\"))\n\ntranscripts_corpora$udpipe &lt;- transcripts$correct %&gt;% \n  rename(\n    doc_id = id_sequence,\n    text = dialogue\n  ) %&gt;% \n  udpipe::udpipe(udmodel_english)\n\n\n# Define environment\nreticulate::use_virtualenv(\"r-spacyr\")\n\n# Initialize\n# spacyr::spacy_download_langmodel(\"en_core_web_sm\", force = TRUE)\nspacyr::spacy_initialize(\"en_core_web_sm\")\n\n# Parse text\ntranscripts_corpora$spacyr &lt;- transcripts_corpora$corp %&gt;% \n    spacyr::spacy_parse(.,\n        tag = TRUE,\n        pos = TRUE,\n        lemma = TRUE,\n        entity = TRUE,\n        dependency = TRUE,\n        nounphrase = TRUE,\n        multithread = TRUE,\n        additional_attributes = c(\n          \"is_punct\"\n        )\n    )",
    "crumbs": [
      "Data collection",
      "Corpus: Transcripts"
    ]
  },
  {
    "objectID": "data_collection/01_22-corpora_transcripts-creation.html#save-data",
    "href": "data_collection/01_22-corpora_transcripts-creation.html#save-data",
    "title": "Corpus: Transcripts",
    "section": "Save data",
    "text": "Save data\n\n# Save complete data\nqs::qsave(\n    transcripts_corpora,\n    file = here(\"local_data/transcripts-corpora_full.qs\")\n)\n\n# Save udpipe corpus\nqs::qsave(\n    transcripts_corpora$udpipe, \n    file = here(\"local_data/transcripts-corpus_udpipe.qs\")\n)\n\n# Save spacyr corpus\nqs::qsave(\n    transcripts_corpora$spacyr, \n    file = here(\"local_data/transcripts-corpus_spacyr.qs\")\n)",
    "crumbs": [
      "Data collection",
      "Corpus: Transcripts"
    ]
  },
  {
    "objectID": "data_collection/02_02-qc-chat-badges.html",
    "href": "data_collection/02_02-qc-chat-badges.html",
    "title": "Quality Control: Chat Badges",
    "section": "",
    "text": "Information\n\n\n\n\nThe document sets up the session by sourcing a setup script.\nIt imports chat data from a local file.\nIt creates a list of user badges from the chat data.\nIt provides a link to example images of the badges.\nIt analyzes the frequency and distribution of badges per message.",
    "crumbs": [
      "Data collection",
      "Quality Control: Chat Badges"
    ]
  },
  {
    "objectID": "data_collection/02_02-qc-chat-badges.html#preparation",
    "href": "data_collection/02_02-qc-chat-badges.html#preparation",
    "title": "Quality Control: Chat Badges",
    "section": "Preparation",
    "text": "Preparation\n\nsource(here::here(\"data_collection/00_02-setup-session.R\"))\n\n\n# Load data\nchat &lt;- qs::qread(here(\"local_data/chat-debates_full.qs\"))",
    "crumbs": [
      "Data collection",
      "Quality Control: Chat Badges"
    ]
  },
  {
    "objectID": "data_collection/02_02-qc-chat-badges.html#create-list-of-badges",
    "href": "data_collection/02_02-qc-chat-badges.html#create-list-of-badges",
    "title": "Quality Control: Chat Badges",
    "section": "Create list of badges",
    "text": "Create list of badges\n\nbadges &lt;- chat$correct %&gt;%\n    select(message_id, user_badges) %&gt;%\n    unnest(user_badges) %&gt;%\n    mutate(\n        badge_name = sapply(user_badges, function(badge) badge$name),\n    ) %&gt;%\n    unnest(badge_name)\n\n\n\n\n\n\n\nNote\n\n\n\nFor example images of the emojis visit https://twitchinsights.net/badges\n\n\n\nbadges %&gt;% \n    frq(badge_name)\n\nbadge_name &lt;character&gt; \n# total N=677804 valid N=677804 mean=34.00 sd=8.99\n\nValue                                        |      N | Raw % | Valid % | Cum. %\n--------------------------------------------------------------------------------\nambassador                                   |      9 |  0.00 |    0.00 |   0.00\nanomaly_warzone_earth_1                      |     19 |  0.00 |    0.00 |   0.00\nbattlerite_1                                 |    259 |  0.04 |    0.04 |   0.04\nbits                                         |  11124 |  1.64 |    1.64 |   1.68\nbits_charity                                 |   5246 |  0.77 |    0.77 |   2.46\nbits_leader                                  |     41 |  0.01 |    0.01 |   2.46\nbroadcaster                                  |    724 |  0.11 |    0.11 |   2.57\nchatter_cs_go_2022                           |   1443 |  0.21 |    0.21 |   2.78\ncreator_cs_go_2022                           |      1 |  0.00 |    0.00 |   2.78\ncuphead_1                                    |     10 |  0.00 |    0.00 |   2.78\ndestiny_2_final_shape_raid_race              |   7713 |  1.14 |    1.14 |   3.92\ndestiny_2_the_final_shape_streamer           |    376 |  0.06 |    0.06 |   3.98\ndreamcon_2024                                |   1026 |  0.15 |    0.15 |   4.13\neso_1                                        |     25 |  0.00 |    0.00 |   4.13\nfirewatch_1                                  |      5 |  0.00 |    0.00 |   4.13\nfounder                                      |    356 |  0.05 |    0.05 |   4.19\ngame_developer                               |     96 |  0.01 |    0.01 |   4.20\nglhf_pledge                                  |   8441 |  1.25 |    1.25 |   5.45\nglitchcon2020                                |  19012 |  2.80 |    2.80 |   8.25\ngold_pixel_heart                             |   5194 |  0.77 |    0.77 |   9.02\nH1Z1_1                                       |    107 |  0.02 |    0.02 |   9.03\nhype_train                                   |   4200 |  0.62 |    0.62 |   9.65\nla_velada_iv                                 |   2628 |  0.39 |    0.39 |  10.04\nminecraft_15th_anniversary_celebration       |   2804 |  0.41 |    0.41 |  10.45\nmoderator                                    |  19200 |  2.83 |    2.83 |  13.29\nno_audio                                     |  18780 |  2.77 |    2.77 |  16.06\nno_video                                     |  13104 |  1.93 |    1.93 |  17.99\noverwatch_league_insider_1                   |    376 |  0.06 |    0.06 |  18.05\noverwatch_league_insider_2018B               |    226 |  0.03 |    0.03 |  18.08\noverwatch_league_insider_2019A               |   1451 |  0.21 |    0.21 |  18.29\npartner                                      |  15617 |  2.30 |    2.30 |  20.60\npredictions                                  |  11701 |  1.73 |    1.73 |  22.32\npremium                                      | 193824 | 28.60 |   28.60 |  50.92\nraging_wolf_helm                             |  88083 | 13.00 |   13.00 |  63.92\nrplace_2023                                  |   7954 |  1.17 |    1.17 |  65.09\nstaff                                        |    158 |  0.02 |    0.02 |  65.11\nstreamer_awards_2024                         |   2321 |  0.34 |    0.34 |  65.45\nsub_gift_leader                              |    262 |  0.04 |    0.04 |  65.49\nsub_gifter                                   |  15331 |  2.26 |    2.26 |  67.76\nsubscriber                                   | 106973 | 15.78 |   15.78 |  83.54\nsubtember_2024                               |   7010 |  1.03 |    1.03 |  84.57\nsuperultracombo_2023                         |  15703 |  2.32 |    2.32 |  86.89\nthe_game_awards_2023                         |    242 |  0.04 |    0.04 |  86.92\nthe_golden_predictor_of_the_game_awards_2023 |      4 |  0.00 |    0.00 |  86.92\nturbo                                        |  27778 |  4.10 |    4.10 |  91.02\ntwitch_dj                                    |    365 |  0.05 |    0.05 |  91.08\ntwitch_recap_2023                            |  53608 |  7.91 |    7.91 |  98.99\ntwitchcon_2024___rotterdam                   |    289 |  0.04 |    0.04 |  99.03\ntwitchcon_2024___san_diego                   |   1802 |  0.27 |    0.27 |  99.29\ntwitchcon2017                                |    304 |  0.04 |    0.04 |  99.34\ntwitchcon2018                                |    353 |  0.05 |    0.05 |  99.39\ntwitchconAmsterdam2020                       |    122 |  0.02 |    0.02 |  99.41\ntwitchconEU2019                              |    129 |  0.02 |    0.02 |  99.43\ntwitchconEU2022                              |    197 |  0.03 |    0.03 |  99.46\ntwitchconEU2023                              |     62 |  0.01 |    0.01 |  99.47\ntwitchconNA2019                              |    282 |  0.04 |    0.04 |  99.51\ntwitchconNA2022                              |    997 |  0.15 |    0.15 |  99.66\ntwitchconNA2023                              |    363 |  0.05 |    0.05 |  99.71\nvip                                          |    723 |  0.11 |    0.11 |  99.82\nzevent_2024                                  |   1251 |  0.18 |    0.18 | 100.00\n&lt;NA&gt;                                         |      0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;",
    "crumbs": [
      "Data collection",
      "Quality Control: Chat Badges"
    ]
  },
  {
    "objectID": "data_collection/02_02-qc-chat-badges.html#distribution-of-badges-per-message",
    "href": "data_collection/02_02-qc-chat-badges.html#distribution-of-badges-per-message",
    "title": "Quality Control: Chat Badges",
    "section": "Distribution of badges per message",
    "text": "Distribution of badges per message\n\nbadges %&gt;% \n    group_by(message_id) %&gt;% \n    summarise( n = n()) %&gt;% \n    describe_distribution()\n\nVariable | Mean |   SD | IQR |        Range | Skewness | Kurtosis |      n | n_Missing\n--------------------------------------------------------------------------------------\nn        | 1.16 | 0.38 |   0 | [1.00, 3.00] |     2.05 |     2.94 | 582419 |         0",
    "crumbs": [
      "Data collection",
      "Quality Control: Chat Badges"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Digital Behavioral Data",
    "section": "",
    "text": "Note\n\n\n\nThis page provides an outline of the topics, contents, and assignments for the semester. Please note that the contents of the course sessions ( background information,  slides,  exercises and  tutorials) will be updated continuously throughout the semester, with all changes being documented here.",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "index.html#copyright",
    "href": "index.html#copyright",
    "title": "Digital Behavioral Data",
    "section": "Copyright",
    "text": "Copyright\nThis content is licensed under a GPL-3.0 License.",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "slides/slides-07.html#seminarplan",
    "href": "slides/slides-07.html#seminarplan",
    "title": "üî® Text as data in R",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\nSession\nDatum\nTopic\nPresenter\n\n\n\n\n\nüìÇ Block 1\nIntroduction\n\n\n\n1\n23.10.2024\nKick-Off\nChristoph Adrian\n\n\n2\n30.10.2024\nDBD: Overview & Introduction\nChristoph Adrian\n\n\n3\n06.11.2024\nüî® Introduction to working with R\nChristoph Adrian\n\n\n\nüìÇ Block 2\nTheoretical Background: Twitch & TV Election Debates\n\n\n\n4\n13.11.2024\nüìö Twitch-Nutzung im Fokus\nStudent groups\n\n\n5\n20.11.2024\nüìö (Wirkungs-)Effekte von Twitch & TV-Debatten\nStudent groups\n\n\n6\n27.11.2024\nüìö Politische Debatten & Social Media\nStudent groups\n\n\n\nüìÇ Block 3\nMethod: Natural Language Processing\n\n\n\n7\n04.12.2024\nüî® Text as data I: Introduction\nChristoph Adrian\n\n\n8\n11.12.2024\nüî® Text as data II: Advanced Methods\nChristoph Adrian\n\n\n9\n18.12.2024\nüî® Advanced Method I: Topic Modeling\nChristoph Adrian\n\n\n\nNo lecture\nüéÑChristmas Break\n\n\n\n10\n08.01.2025\nüî® Advanced Method II: Machine Learning\nChristoph Adrian\n\n\n\nüìÇ Block 4\nProject Work\n\n\n\n11\n15.01.2025\nüî® Project work\nStudent groups\n\n\n12\n22.01.2025\nüî® Project work\nStudent groups\n\n\n13\n29.01.2025\nüìä Project Presentation I\nStudent groups (TBD)\n\n\n14\n05.02.2025\nüìä Project Presentation & üèÅ Evaluation\nStudentds (TBD) & Christoph Adrian"
  },
  {
    "objectID": "slides/slides-07.html#wahldebatte-am-digitalen-lagerfeuer",
    "href": "slides/slides-07.html#wahldebatte-am-digitalen-lagerfeuer",
    "title": "üî® Text as data in R",
    "section": "Wahldebatte am digitalen Lagerfeuer",
    "text": "Wahldebatte am digitalen Lagerfeuer\nWas wir (bisher) aus der Literatur gelernt haben\n\nWahldebatten sind ein spezifischer Teil des politischen Diskurses und haben Einfluss auf Emotionen, Einstellungen und Handlungen von Menschen\nSoziale Medien haben die Kommunikation und Interaktion zwischen Politikern und B√ºrgern ver√§ndert\n Twitch ist eine Social Networking Site (SNS) mit besonderen Eigenschaften (z.B. ‚ÄúLive‚Äù-Aspekt & die Bedeutung der community) & zunehmend Ort f√ºr politische Diskurse"
  },
  {
    "objectID": "slides/slides-07.html#who-are-we-looking-at",
    "href": "slides/slides-07.html#who-are-we-looking-at",
    "title": "üî® Text as data in R",
    "section": "Who are we looking at?",
    "text": "Who are we looking at?\n√úberblick √ºber verschiedenen Statistiken der betrachteten Streamer\n\n\nExpand for full code\nstreamer_stats &lt;- qs::qread(here(\"local_data/twitch_streamer_stats.qs\"))\n\nstreamer_stats %&gt;% \n  pivot_longer(cols = c(avg_viewers, followers, hours_streamed), names_to = \"statistic\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = month, y = value, fill = streamer)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_grid(statistic ~ ., scales = \"free_y\", labeller = as_labeller(c(\n    avg_viewers = \"Average Viewers\",\n    followers = \"Followers\",\n    hours_streamed = \"Hours Streamed\"))) +\n  theme_minimal() +\n  labs(\n    x = \"Month\",\n    y = \"\",\n    title = \"Streamer Statistics Over Time\", \n    fill = \"Streamer\") +\n  scale_y_continuous(labels = scales::comma) +\n  ggsci::scale_fill_cosmic()"
  },
  {
    "objectID": "slides/slides-07.html#wie-wurden-die-daten-erhoben",
    "href": "slides/slides-07.html#wie-wurden-die-daten-erhoben",
    "title": "üî® Text as data in R",
    "section": "Wie wurden die Daten erhoben?",
    "text": "Wie wurden die Daten erhoben?\n√úberblick √ºber den Prozess der Datenerhebung\n\nErhebung des Live-Stream-Chats mit  Paket chat_downloader\nDownload der Twitch & TV VoDs mit dem  Paket twitch-dl\nTranskription der Streams & der Debatte mit AI-based Transkriptionstool NoScribe\n\nHerausforderungen\n\nEingeschr√§nkte  Twitch API ‚ûú nicht alle (urspr√ºnglich verf√ºgbaren) Informationen abrufbar\nLimitierte Verf√ºgbarkeit der VoDs auf  Twitch ‚ûú Re-Upload auf unserem  YouTube-Kanal (ungelistet)\nQualit√§t der Transkription ist gut, Identifkation der sprechenden Person(en) ausbauf√§hig"
  },
  {
    "objectID": "slides/slides-07.html#welche-daten-stehen-zur-verf√ºgung",
    "href": "slides/slides-07.html#welche-daten-stehen-zur-verf√ºgung",
    "title": "üî® Text as data in R",
    "section": "Welche Daten stehen zur Verf√ºgung?",
    "text": "Welche Daten stehen zur Verf√ºgung?\n√úberblick √ºber die Daten\n\n\nLinks zu den (Uploads der) VoDs auf StudOn im Ordner Kursmaterialien/VODs\nDatensatz chats.qs (& Dokumentation) mit den Chatnachrichten aller Live-Streams ( hasanabi,  zackrawrr und | TheMajorityReport)\nDatensatz transcripts.qs (& Dokumentation) mit den Transkripten der TV-Debatten (Presidential auf ABC, Vice Presidential auf CBS) & aller Live-Streams ( hasanabi,  zackrawrr und | TheMajorityReport)\nDictionary dictionary_chat_emotes.RDS mit den Emojis und Emotes, die in den hier analyisierten  Twitch &  YouTube-Chats verwendet wurden"
  },
  {
    "objectID": "slides/slides-07.html#chats-als-rohform-von-dbd",
    "href": "slides/slides-07.html#chats-als-rohform-von-dbd",
    "title": "üî® Text as data in R",
    "section": "Chats als Rohform von DBD",
    "text": "Chats als Rohform von DBD\nKurzer √úberblick √ºber den chats-Datensatz\n\nchats %&gt;% glimpse \n\nRows: 913,375\nColumns: 33\n$ streamer              &lt;chr&gt; \"hasanabi\", \"hasanabi\", \"hasanabi\", \"hasanabi\", ‚Ä¶\n$ url                   &lt;chr&gt; \"https://www.twitch.tv/videos/2247664726\", \"http‚Ä¶\n$ platform              &lt;chr&gt; \"twitch\", \"twitch\", \"twitch\", \"twitch\", \"twitch\"‚Ä¶\n$ debate                &lt;chr&gt; \"presidential\", \"presidential\", \"presidential\", ‚Ä¶\n$ user_name             &lt;chr&gt; \"bendaspur\", \"spackle_pirate\", \"texaschollima\", ‚Ä¶\n$ user_id               &lt;chr&gt; \"54058406\", \"182041182\", \"185502300\", \"159018462‚Ä¶\n$ user_display_name     &lt;chr&gt; \"BenDaSpur\", \"spackle_pirate\", \"TexasChollima\", ‚Ä¶\n$ user_badges           &lt;list&gt; [], [], [], [[\"twitch_recap_2023\", 1, \"Twitch R‚Ä¶\n$ message_timestamp     &lt;dbl&gt; 19, 19, 20, 20, 21, 21, 22, 22, 24, 25, 25, 25, ‚Ä¶\n$ message_id            &lt;chr&gt; \"dc03b89a-722d-4eaa-a895-736533a68aca\", \"6be50e1‚Ä¶\n$ message_type          &lt;chr&gt; \"text_message\", \"text_message\", \"text_message\", ‚Ä¶\n$ message_content       &lt;chr&gt; \"60fps LETSGO 60fps LETSGO 60fps LETSGO 60fps LE‚Ä¶\n$ message_emotes        &lt;list&gt; [], [], [], [], [], [], [], [], [], [], [], [[\"‚Ä¶\n$ message_length        &lt;int&gt; 51, 17, 20, 27, 35, 14, 20, 5, 10, 9, 106, 97, 3‚Ä¶\n$ message_timecode      &lt;Period&gt; 19S, 19S, 20S, 20S, 21S, 21S, 22S, 22S, 24S, ‚Ä¶\n$ message_time          &lt;chr&gt; \"00:00:19\", \"00:00:19\", \"00:00:20\", \"00:00:20\", ‚Ä¶\n$ message_during_debate &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_has_badge        &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, ‚Ä¶\n$ user_is_premium       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, ‚Ä¶\n$ user_is_subscriber    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_turbo         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_moderator     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ‚Ä¶\n$ user_is_partner       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ‚Ä¶\n$ user_is_subgifter     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_broadcaster   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_vip           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_twitchdj      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_founder       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_staff         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_game_dev      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_ambassador    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_no_audio         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_no_video         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶"
  },
  {
    "objectID": "slides/slides-07.html#m√∂glichkeite-zum-data-linking-mit-transkripten",
    "href": "slides/slides-07.html#m√∂glichkeite-zum-data-linking-mit-transkripten",
    "title": "üî® Text as data in R",
    "section": "M√∂glichkeite zum Data Linking mit Transkripten",
    "text": "M√∂glichkeite zum Data Linking mit Transkripten\nKurzer √úberblick √ºber den transcripts-Datensatz\n\ntranscripts %&gt;% glimpse \n\nRows: 5,861\nColumns: 12\n$ id_sequence            &lt;chr&gt; \"p1_s0001\", \"p1_s0002\", \"p1_s0003\", \"p1_s0004\",‚Ä¶\n$ source                 &lt;chr&gt; \"presidential_debate-abc\", \"presidential_debate‚Ä¶\n$ speaker                &lt;chr&gt; \"S27\", \"S35\", \"S27\", \"S55\", \"S61\", \"S55\", \"S43\"‚Ä¶\n$ timestamp              &lt;time&gt; 00:00:00, 00:00:11, 00:00:20, 00:00:34, 00:00:‚Ä¶\n$ dialogue               &lt;chr&gt; \"Tonight, the high-stakes showdown here in Phil‚Ä¶\n$ dialogue_length        &lt;int&gt; 229, 148, 245, 91, 31, 13, 37, 102, 316, 409, 6‚Ä¶\n$ duration               &lt;dbl&gt; 11, 9, 14, 6, 4, 1, 4, 10, 17, 21, 28, 8, 13, 4‚Ä¶\n$ debate                 &lt;chr&gt; \"presidential\", \"presidential\", \"presidential\",‚Ä¶\n$ streamer               &lt;chr&gt; \"tv_station\", \"tv_station\", \"tv_station\", \"tv_s‚Ä¶\n$ id_streamer            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n$ id_speaker             &lt;chr&gt; \"p1_s27\", \"p1_s35\", \"p1_s27\", \"p1_s55\", \"p1_s61‚Ä¶\n$ sequence_during_debate &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶"
  },
  {
    "objectID": "slides/slides-07.html#where-the-magic-happends",
    "href": "slides/slides-07.html#where-the-magic-happends",
    "title": "üî® Text as data in R",
    "section": "Where the ‚Äúmagic‚Äù happends",
    "text": "Where the ‚Äúmagic‚Äù happends\nAutomatisierte Inhaltsanalyse: Definition und Ablauf\n\nAutomatisierte Inhaltsanalyse beschreibt die automatisierte (z. B. via Programmierskript) Analyse von Inhalten (z. B. Text, Bilder). Dabei unterst√ºtzen Forschende/manuelle Codierer:innen, etwa durch die Validierung von Ergebnissen. (Hase, 2023)\n\nAber:\n\nAutomatisierte Methoden ‚Äúaugment humans, not replace them‚Äù (Grimmer & Stewart, 2013, p. S.270)\n‚ÄûEnglish before everything‚Äú (Baden et al., 2022, p. S.9)\n(Systematische) Fehler: ‚ÄúAll quantitative models of language are wrong ‚Äî but some are useful‚Äù (Grimmer & Stewart, 2013, p. S.269)"
  },
  {
    "objectID": "slides/slides-07.html#workflow-demystified",
    "href": "slides/slides-07.html#workflow-demystified",
    "title": "üî® Text as data in R",
    "section": "Workflow, demystified",
    "text": "Workflow, demystified\nTypische Schritte der automatisierten Inhaltsanalyse"
  },
  {
    "objectID": "slides/slides-07.html#building-a-shared-vocabulary",
    "href": "slides/slides-07.html#building-a-shared-vocabulary",
    "title": "üî® Text as data in R",
    "section": "Building a shared vocabulary",
    "text": "Building a shared vocabulary\nGrundbegriffe und Terminologien I\n\n\nToken: A token is a string with a known meaning, and a token may be a word, number or just characters like punctuation. ‚ÄúHello‚Äù, ‚Äú123‚Äù, and ‚Äú-‚Äù are some examples of tokens.\nSentence: A sentence is a group of tokens that is complete in meaning. ‚ÄúThe weather looks good‚Äù is an example of a sentence, and the tokens of the sentence are [‚ÄúThe‚Äù, ‚Äúweather‚Äù, ‚Äúlooks‚Äù, ‚Äúgood].\nParagraph: A paragraph is a collection of sentences or phrases, and a sentence can alternatively be viewed as a token of a paragraph.\nDocuments: A document might be a sentence, a paragraph, or a set of paragraphs. A text message sent to an individual is an example of a document.\nCorpus: A corpus is typically an extensive collection of documents as a Bag-of-words. A corpus comprises each word‚Äôs id and frequency count in each record. An example of a corpus is a collection of emails or text messages sent to a particular person."
  },
  {
    "objectID": "slides/slides-07.html#building-a-shared-vocabulary-1",
    "href": "slides/slides-07.html#building-a-shared-vocabulary-1",
    "title": "üî® Text as data in R",
    "section": "Building a shared vocabulary",
    "text": "Building a shared vocabulary\nGrundbegriffe und Terminologien II\n\nLemma: Die Grundform eines Wortes. Zum Beispiel ist ‚Äúrun‚Äù‚Äù das Lemma von ‚Äúrunning‚Äù‚Äú.\nStoppw√∂rter: W√∂rter, die in der Regel keine inhaltliche Bedeutung haben und daher aus dem Text entfernt werden, z.B. ‚Äúand‚Äù, ‚Äúor‚Äù & ‚Äúthe‚Äù.\nParts of speech (POS): Linguistische Marker, die die allgemeine Kategorie einer sprachlichen Eigenschaft eines Wortes anzeigen, z.B. Nomen, Verb, Adjektiv usw.\nNamed entities: Ein reales Objekt, wie Personen, Orte, Organisationen, Produkte usw., das mit einem Eigennamen bezeichnet werden kann, z.B. ‚ÄúDonald Trump‚Äú oder ‚ÄûVereinigte Staaten‚Äù.\nMulti-word expressions: Wortfolgen, die ein einzelnes Konzept bezeichnen (und im Deutschen w√§ren), z.B. ‚ÄúMehrwertsteuer‚Äù (im Englischen: ‚Äúvalue added tax‚Äù)."
  },
  {
    "objectID": "slides/slides-07.html#was-ist-preprocessing-und-warum-ist-es-wichtig",
    "href": "slides/slides-07.html#was-ist-preprocessing-und-warum-ist-es-wichtig",
    "title": "üî® Text as data in R",
    "section": "Was ist Preprocessing und warum ist es wichtig?",
    "text": "Was ist Preprocessing und warum ist es wichtig?\nTypische Schritte des Preprocessings\n\nReduziert die Komplexit√§t von Textdaten, ohne deren substanzielle Bedeutung zu minimieren\nUmfasst die Bereinigung (Reduzierung von systematischen Fehlern, z.B. Encoding) & Normalisierung (Texte √ºber Dokumente, Sprache, Plattformen, etc. vergleichbar machen)\nTypische Bestandteile der Normalisierung sind Tokenisierung, Kleinschreibung, Entfernung von Stoppw√∂rtern, Satz- &Sonderzeichen, Lemmatisierung/Stemming und ‚ÄúPruning‚Äù (h√§ufige/seltene Features entfernen)"
  },
  {
    "objectID": "slides/slides-07.html#abw√§gungen-beim-preprocessing",
    "href": "slides/slides-07.html#abw√§gungen-beim-preprocessing",
    "title": "üî® Text as data in R",
    "section": "Abw√§gungen beim Preprocessing",
    "text": "Abw√§gungen beim Preprocessing\nHerausforderungen und Konsequenzen der Entscheidungen beim Preprocessing\n\nOft ver√§ndert Kleinschreibung die Bedeutung von Features nicht, aber es gibt Ausnahmen (‚ÄúBild‚Äù vs.¬†BILD-Zeitung)\nOft sind Sonderzeichen (z.B. Satzzeichen) nicht von substanzieller Bedeutung, aber es gibt Ausnahmen (z.B. #metoo, G7, Emojis)\nStoppw√∂rter sind stark kontextabh√§ngig! ‚ûú oft ist es sinnvoll, eigene ‚Äúorganische‚Äù Stoppwortlisten zu erstellen\nLemmatisierung (‚Äúrunning‚Äù ‚Äúran‚Äù ‚ûú ‚Äúrun‚Äù) h√§ufig ‚Äúbesser‚Äù, Stemming (‚Äúrunning‚Äù ‚Äúran‚Äù ‚ûú ‚Äúrun‚Äù ‚Äúran‚Äù) h√§ufig schneller\nReihenfolge des Preprocessings kann Ergebnisse beeinflussen (z.B. Entfernung von Stoppw√∂rtern vor oder nach Lemmatisierung)"
  },
  {
    "objectID": "slides/slides-07.html#wenn-aus-w√∂rtern-zahlen-werden",
    "href": "slides/slides-07.html#wenn-aus-w√∂rtern-zahlen-werden",
    "title": "üî® Text as data in R",
    "section": "Wenn aus W√∂rtern Zahlen werden",
    "text": "Wenn aus W√∂rtern Zahlen werden\nDie einfachste Form der Textrepr√§sentation: bag-of-words\n\nDamit Computer Text verstehen bzw. verarbeiten k√∂nnen, muss der Text in ein numerisches Format umgewandelt werden\nEine einfache und weit verbreitete Methode zur Textrepr√§sentation in der nat√ºrlichen Sprachverarbeitung (NLP) ist das bag-of-words Modell\n\nrepr√§sentiert einen Text (z.B. einen Satz oder ein Dokument) als eine Sammlung von W√∂rtern, ohne Ber√ºcksichtigung der Reihenfolge oder Grammatik\nAnnahme: Reihenfolge und Kontext von W√∂rtern haben keinen Einfluss auf Ihre Bedeutung"
  },
  {
    "objectID": "slides/slides-07.html#ugly-but-efficient",
    "href": "slides/slides-07.html#ugly-but-efficient",
    "title": "üî® Text as data in R",
    "section": "Ugly, but efficient",
    "text": "Ugly, but efficient\nWarum die bag-of-words Annahme problematisch ist ‚Ä¶\n\nPolysemie: Fliege (Tier & Kleidungsst√ºck), ‚ÄúMaus‚Äù (Tier & Computerzubeh√∂r)\nVerneinung: ‚ÄúNicht schlecht!‚Äù\nNamed entities: ‚ÄúOlaf Scholz‚Äù, ‚ÄúVereinigte Staaten‚Äù\nW√∂rter mit √§hnlichen Bedeutungen: ‚ÄúGem√ºse‚Äù & ‚ÄúGr√ºnzeug‚Äù\n\n‚Ä¶ und doch so h√§ufig verwendet wird\n\nschnell, resourcenschonend und ‚Äúrobust‚Äù\nleichte Anpassung bzw. Erweiterung steigern Aussagekraft"
  },
  {
    "objectID": "slides/slides-07.html#beyond-bag-of-words",
    "href": "slides/slides-07.html#beyond-bag-of-words",
    "title": "üî® Text as data in R",
    "section": "Beyond ‚Äúbag-of-words‚Äù",
    "text": "Beyond ‚Äúbag-of-words‚Äù\nText-as-Data Repr√§sentationen, die Reihenfolge und Kontext ber√ºcksichtigen\n\nNgram-basierte Repr√§sentation (z. B. Collocations & Keywords-in-Context)\nSyntax-basierte Repr√§sentation (z. B. Part-of-Speech Tagging & Dependency Parsing)\nVektor-basierte Repr√§sentation in semantischen, n-dimensionalen R√§umen (z. B. Word Embeddings)"
  },
  {
    "objectID": "slides/slides-07.html#flexible-for-power-users-simple-for-beginners",
    "href": "slides/slides-07.html#flexible-for-power-users-simple-for-beginners",
    "title": "üî® Text as data in R",
    "section": "Flexible for power users, simple for beginners",
    "text": "Flexible for power users, simple for beginners\nHintergrund zu Paket & Projekt quanteda (Benoit et al., 2018)\n\nquanteda ist ein umfassendes R-Paket f√ºr die Textverarbeitung und Textanalyse\nSehr aktives Open-Source-Projekt mit umfangreicher Dokumentation und Community-Support\nBritische gemeinn√ºtzige Organisation, die sich der F√∂rderung von Open-Source-Software f√ºr die Textanalyse widmet\nAlternative: tidytext(Silge & Robinson, 2017)"
  },
  {
    "objectID": "slides/slides-07.html#grundlage-ist-immer-das-korpus",
    "href": "slides/slides-07.html#grundlage-ist-immer-das-korpus",
    "title": "üî® Text as data in R",
    "section": "Grundlage ist immer das Korpus",
    "text": "Grundlage ist immer das Korpus\nArbeiten mit quanteda: corpus\n\n\n# Create corpus\ncorp_transcripts &lt;- transcripts %&gt;% \n  quanteda::corpus(\n    docid_field = \"id_sequence\", \n    text_field = \"dialogue\"\n  )\n\n# Output\ncorp_transcripts\n\n\nCorpus consisting of 5,861 documents and 10 docvars.\np1_s0001 :\n\"Tonight, the high-stakes showdown here in Philadelphia betwe...\"\n\np1_s0002 :\n\"A historic race for president upended just weeks ago, Presid...\"\n\np1_s0003 :\n\"The candidates separated by the smallest of margins, essenti...\"\n\np1_s0004 :\n\"This is an ABC News special. The most consequential moment o...\"\n\np1_s0005 :\n\"Together, we'll chart a... (..)\"\n\np1_s0006 :\n\"Donald Trump.\"\n\n[ reached max_ndoc ... 5,855 more documents ]"
  },
  {
    "objectID": "slides/slides-07.html#einfache-tokenisierung",
    "href": "slides/slides-07.html#einfache-tokenisierung",
    "title": "üî® Text as data in R",
    "section": "Einfache Tokenisierung ‚Ä¶",
    "text": "Einfache Tokenisierung ‚Ä¶\nEinfluss der Preporcessing-Schritte am Beispiel (I)\n\n# Tokenize corpus\ntoks_simple &lt;- corp_transcripts %&gt;% \n  quanteda::tokens() \n\n# Output\nhead(toks_simple[[1]], 100)\n\n [1] \"Tonight\"      \",\"            \"the\"          \"high-stakes\"  \"showdown\"    \n [6] \"here\"         \"in\"           \"Philadelphia\" \"between\"      \"Vice\"        \n[11] \"President\"    \"Kamala\"       \"Harris\"       \"and\"          \"former\"      \n[16] \"President\"    \"Donald\"       \"Trump\"        \".\"            \"Their\"       \n[21] \"first\"        \"face-to-face\" \"meeting\"      \"in\"           \"this\"        \n[26] \"presidential\" \"election\"     \",\"            \"their\"        \"first\"       \n[31] \"face-to-face\" \"meeting\"      \"ever\"         \".\""
  },
  {
    "objectID": "slides/slides-07.html#mit-entfernung-von-satz--und-sonderzeichen",
    "href": "slides/slides-07.html#mit-entfernung-von-satz--und-sonderzeichen",
    "title": "üî® Text as data in R",
    "section": "‚Ä¶ mit Entfernung von Satz- und Sonderzeichen ‚Ä¶",
    "text": "‚Ä¶ mit Entfernung von Satz- und Sonderzeichen ‚Ä¶\nEinfluss der Preporcessing-Schritte am Beispiel (II)\n\ntoks_nopunct &lt;- corp_transcripts %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  )\n\nhead(toks_nopunct[[1]], 100)\n\n [1] \"Tonight\"      \"the\"          \"high-stakes\"  \"showdown\"     \"here\"        \n [6] \"in\"           \"Philadelphia\" \"between\"      \"Vice\"         \"President\"   \n[11] \"Kamala\"       \"Harris\"       \"and\"          \"former\"       \"President\"   \n[16] \"Donald\"       \"Trump\"        \"Their\"        \"first\"        \"face-to-face\"\n[21] \"meeting\"      \"in\"           \"this\"         \"presidential\" \"election\"    \n[26] \"their\"        \"first\"        \"face-to-face\" \"meeting\"      \"ever\""
  },
  {
    "objectID": "slides/slides-07.html#und-ohne-stopw√∂rter",
    "href": "slides/slides-07.html#und-ohne-stopw√∂rter",
    "title": "üî® Text as data in R",
    "section": "‚Ä¶ und ohne Stopw√∂rter",
    "text": "‚Ä¶ und ohne Stopw√∂rter\nEinfluss der Preporcessing-Schritte am Beispiel (III)\n\ntoks_nostopw &lt;- corp_transcripts %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  ) %&gt;% \n  quanteda::tokens_remove(\n    pattern = quanteda::stopwords(\"en\")\n  )\n\nhead(toks_nostopw[[1]], 100)\n\n [1] \"Tonight\"      \"high-stakes\"  \"showdown\"     \"Philadelphia\" \"Vice\"        \n [6] \"President\"    \"Kamala\"       \"Harris\"       \"former\"       \"President\"   \n[11] \"Donald\"       \"Trump\"        \"first\"        \"face-to-face\" \"meeting\"     \n[16] \"presidential\" \"election\"     \"first\"        \"face-to-face\" \"meeting\"     \n[21] \"ever\""
  },
  {
    "objectID": "slides/slides-07.html#direkter-vergleich",
    "href": "slides/slides-07.html#direkter-vergleich",
    "title": "üî® Text as data in R",
    "section": "Direkter Vergleich",
    "text": "Direkter Vergleich\nEinfluss der Preporcessing-Schritte am Beispiel (IV)\n\nhead(toks_simple[[1]], 100)\n\n [1] \"Tonight\"      \",\"            \"the\"          \"high-stakes\"  \"showdown\"    \n [6] \"here\"         \"in\"           \"Philadelphia\" \"between\"      \"Vice\"        \n[11] \"President\"    \"Kamala\"       \"Harris\"       \"and\"          \"former\"      \n[16] \"President\"    \"Donald\"       \"Trump\"        \".\"            \"Their\"       \n[21] \"first\"        \"face-to-face\" \"meeting\"      \"in\"           \"this\"        \n[26] \"presidential\" \"election\"     \",\"            \"their\"        \"first\"       \n[31] \"face-to-face\" \"meeting\"      \"ever\"         \".\"           \n\nhead(toks_nopunct[[1]], 100)\n\n [1] \"Tonight\"      \"the\"          \"high-stakes\"  \"showdown\"     \"here\"        \n [6] \"in\"           \"Philadelphia\" \"between\"      \"Vice\"         \"President\"   \n[11] \"Kamala\"       \"Harris\"       \"and\"          \"former\"       \"President\"   \n[16] \"Donald\"       \"Trump\"        \"Their\"        \"first\"        \"face-to-face\"\n[21] \"meeting\"      \"in\"           \"this\"         \"presidential\" \"election\"    \n[26] \"their\"        \"first\"        \"face-to-face\" \"meeting\"      \"ever\"        \n\nhead(toks_nostopw[[1]], 100)\n\n [1] \"Tonight\"      \"high-stakes\"  \"showdown\"     \"Philadelphia\" \"Vice\"        \n [6] \"President\"    \"Kamala\"       \"Harris\"       \"former\"       \"President\"   \n[11] \"Donald\"       \"Trump\"        \"first\"        \"face-to-face\" \"meeting\"     \n[16] \"presidential\" \"election\"     \"first\"        \"face-to-face\" \"meeting\"     \n[21] \"ever\""
  },
  {
    "objectID": "slides/slides-07.html#ngrams-f√ºr-mehr-kontext",
    "href": "slides/slides-07.html#ngrams-f√ºr-mehr-kontext",
    "title": "üî® Text as data in R",
    "section": "Ngrams f√ºr mehr Kontext",
    "text": "Ngrams f√ºr mehr Kontext\nTokenisierung von Bi & Skipgrams\n\n# Bigrams\ntoks_nostopw %&gt;% \n  tokens_ngrams(n = 2) %&gt;% \n  .[[1]]\n\n [1] \"Tonight_high-stakes\"   \"high-stakes_showdown\"  \"showdown_Philadelphia\"\n [4] \"Philadelphia_Vice\"     \"Vice_President\"        \"President_Kamala\"     \n [7] \"Kamala_Harris\"         \"Harris_former\"         \"former_President\"     \n[10] \"President_Donald\"      \"Donald_Trump\"          \"Trump_first\"          \n[13] \"first_face-to-face\"    \"face-to-face_meeting\"  \"meeting_presidential\" \n[16] \"presidential_election\" \"election_first\"        \"first_face-to-face\"   \n[19] \"face-to-face_meeting\"  \"meeting_ever\"         \n\n\n\n# Skipgrams\ntoks_nostopw %&gt;% \n  tokens_ngrams(n = 2, skip = 0:1) %&gt;% \n  .[[1]]\n\n [1] \"Tonight_high-stakes\"       \"Tonight_showdown\"         \n [3] \"high-stakes_showdown\"      \"high-stakes_Philadelphia\" \n [5] \"showdown_Philadelphia\"     \"showdown_Vice\"            \n [7] \"Philadelphia_Vice\"         \"Philadelphia_President\"   \n [9] \"Vice_President\"            \"Vice_Kamala\"              \n[11] \"President_Kamala\"          \"President_Harris\"         \n[13] \"Kamala_Harris\"             \"Kamala_former\"            \n[15] \"Harris_former\"             \"Harris_President\"         \n[17] \"former_President\"          \"former_Donald\"            \n[19] \"President_Donald\"          \"President_Trump\"          \n[21] \"Donald_Trump\"              \"Donald_first\"             \n[23] \"Trump_first\"               \"Trump_face-to-face\"       \n[25] \"first_face-to-face\"        \"first_meeting\"            \n[27] \"face-to-face_meeting\"      \"face-to-face_presidential\"\n[29] \"meeting_presidential\"      \"meeting_election\"         \n[31] \"presidential_election\"     \"presidential_first\"       \n[33] \"election_first\"            \"election_face-to-face\"    \n[35] \"first_face-to-face\"        \"first_meeting\"            \n[37] \"face-to-face_meeting\"      \"face-to-face_ever\"        \n[39] \"meeting_ever\""
  },
  {
    "objectID": "slides/slides-07.html#welche-features-treten-h√§ufig-nacheinander-auf",
    "href": "slides/slides-07.html#welche-features-treten-h√§ufig-nacheinander-auf",
    "title": "üî® Text as data in R",
    "section": "Welche Features treten h√§ufig nacheinander auf?",
    "text": "Welche Features treten h√§ufig nacheinander auf?\nKollokationen f√ºr Identifkation prominenter Bigramme\n\n\ntoks_nostopw %&gt;% \n  quanteda.textstats::textstat_collocations(\n    size = 2, \n    min_count = 5\n  ) %&gt;% \n  head(25)\n\n\n        collocation count count_nested length    lambda        z\n1         know know  1337            0      2  3.890787 98.31370\n2        saying bad   558            0      2  6.503305 81.19215\n3        bad saying   553            0      2  6.481795 80.98625\n4         going say   666            0      2  4.555737 80.92246\n5         say going   661            0      2  4.562963 80.82524\n6      donald trump   755            0      2  7.422847 75.19267\n7     kamala harris   494            0      2  7.873933 68.88003\n8    vice president   429            0      2  7.258104 56.72753\n9             oh oh   229            0      2  4.679549 54.95066\n10        right now   269            0      2  3.996328 53.27167\n11    senator vance   129            0      2  6.583164 49.48173\n12       little bit   132            0      2  8.268099 45.80914\n13 president harris   186            0      2  4.027681 45.69845\n14           oh god   154            0      2  6.175930 44.94423\n15        years ago   102            0      2  6.436807 43.06424\n16         tim walz    90            0      2  7.588398 43.05596\n17  president trump   203            0      2  3.468589 42.70406\n18       four years   100            0      2  6.381600 42.53221\n19      health care   136            0      2  7.400206 41.83123\n20      white house    85            0      2  8.071701 41.52827\n21   donald trump's   132            0      2  6.408658 41.05938\n22 former president   141            0      2  5.790480 41.04486\n23  curious curious   373            0      2 11.836611 40.77202\n24    governor walz    77            0      2  6.512020 39.65499\n25      two minutes    84            0      2  6.490910 39.39074"
  },
  {
    "objectID": "slides/slides-07.html#von-tokens-zur-dfm",
    "href": "slides/slides-07.html#von-tokens-zur-dfm",
    "title": "üî® Text as data in R",
    "section": "Von Tokens zur DFM",
    "text": "Von Tokens zur DFM\nErkl√§rung der Dokument-Feature-Matrix (DFM)\n\n\nSehr h√§ufig genutzten Stukturen der ‚Äúklassischen‚Äù Textverarbeitung mit folgende Merkmale:\n\njede Zeile ein Dokument (wie z.B. eine Chatnachricht oder eine Sprecher:innensequenz),\njede Spalte repr√§sentiert einen Begriff, und\njeder Wert enth√§lt (typischerweise) die Anzahl der H√§ufigkeit dieses Begriffs in diesem Dokument.\n\n\n\n\n\n\n\n\n(Zheng & Casari, 2018)"
  },
  {
    "objectID": "slides/slides-07.html#√ºberpr√ºfung-h√§ufigster-token",
    "href": "slides/slides-07.html#√ºberpr√ºfung-h√§ufigster-token",
    "title": "üî® Text as data in R",
    "section": "√úberpr√ºfung h√§ufigster Token",
    "text": "√úberpr√ºfung h√§ufigster Token\nAnwendung der DFM\n\n\n# Check top 25 features\ntoks_nostopw %&gt;%\n  quanteda::dfm() %&gt;% \n  quanteda.textstats::textstat_frequency(\n    n = 25) \n\n\n     feature frequency rank docfreq group\n1       like      6350    1    1617   all\n2       know      3872    2    1325   all\n3      think      2932    3    1202   all\n4     people      2710    4    1158   all\n5       yeah      2551    5    1107   all\n6      going      2364    6     956   all\n7       just      2145    7    1322   all\n8        say      1632    8     715   all\n9      right      1556    9     930   all\n10     trump      1492   10     859   all\n11       one      1461   11     958   all\n12 president      1372   12     787   all\n13       get      1320   13     850   all\n14      said      1262   14     822   all\n15       now      1222   15     883   all\n16      want      1207   16     755   all\n17       can      1137   17     723   all\n18    really      1137   17     620   all\n19        uh      1134   19     421   all\n20   fucking      1074   20     522   all\n21       lot      1049   21     632   all\n22    saying      1042   22     376   all\n23        oh      1003   23     546   all\n24      well       974   24     740   all\n25       bad       963   25     251   all"
  },
  {
    "objectID": "slides/slides-07.html#corpus-tokens-dfm",
    "href": "slides/slides-07.html#corpus-tokens-dfm",
    "title": "üî® Text as data in R",
    "section": "Corpus ‚ûû ( Tokens ‚ûû DFM ) ‚ü≥",
    "text": "Corpus ‚ûû ( Tokens ‚ûû DFM ) ‚ü≥\nBeispiel f√ºr den Loop des (Pre-)Processing\n\n\n# Customize stopwords\ncustom_stopwords &lt;- c(\"uh\", \"oh\")\n\n# Remove custom stopwords\ntoks_no_custom_stopw &lt;- toks_nostopw %&gt;% \n  quanteda::tokens_remove(\n    pattern = custom_stopwords\n  )\n\n# Check top 25 features\ntoks_no_custom_stopw %&gt;%\n  quanteda::dfm() %&gt;% \n  quanteda.textstats::textstat_frequency(\n    n = 25) \n\n\n     feature frequency rank docfreq group\n1       like      6350    1    1617   all\n2       know      3872    2    1325   all\n3      think      2932    3    1202   all\n4     people      2710    4    1158   all\n5       yeah      2551    5    1107   all\n6      going      2364    6     956   all\n7       just      2145    7    1322   all\n8        say      1632    8     715   all\n9      right      1556    9     930   all\n10     trump      1492   10     859   all\n11       one      1461   11     958   all\n12 president      1372   12     787   all\n13       get      1320   13     850   all\n14      said      1262   14     822   all\n15       now      1222   15     883   all\n16      want      1207   16     755   all\n17       can      1137   17     723   all\n18    really      1137   17     620   all\n19   fucking      1074   19     522   all\n20       lot      1049   20     632   all\n21    saying      1042   21     376   all\n22      well       974   22     740   all\n23       bad       963   23     251   all\n24      mean       935   24     557   all\n25       way       905   25     572   all"
  },
  {
    "objectID": "slides/slides-07.html#welche-user-werden-am-h√§ufigsten-erw√§hnt",
    "href": "slides/slides-07.html#welche-user-werden-am-h√§ufigsten-erw√§hnt",
    "title": "üî® Text as data in R",
    "section": "Welche User werden am h√§ufigsten erw√§hnt?",
    "text": "Welche User werden am h√§ufigsten erw√§hnt?\nBeispiele f√ºr Analysen auf Basis der DFM: Auswahl bestimmter Muster\n\n\n# Create corpus\ncorp_chats &lt;- chats %&gt;% \n  quanteda::corpus(\n    docid_field = \"message_id\", \n    text_field = \"message_content\"\n  )\n\n# Create DFM\ndfm_chats &lt;- corp_chats %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  ) %&gt;% \n  quanteda::dfm() \n\n# Output\ndfm_chats %&gt;% \n  quanteda::dfm_select(pattern = \"@*\") %&gt;% \n  quanteda.textstats::textstat_frequency(\n    n = 25) \n\n\n            feature frequency rank docfreq group\n1         @hasanabi     29173    1   28371   all\n2        @zackrawrr     11430    2   11381   all\n3       @gizmomacks       246    3     243   all\n4         @toxicsjw       167    4     167   all\n5       @beteljuice       158    5     158   all\n6       @megaphonix       154    6     150   all\n7            @hasan        76    7      76   all\n8  @depressedaether        68    8      68   all\n9     @nicebathroom        68    8      68   all\n10    @littlebear36        64   10      61   all\n11   @david_leonard        61   11      61   all\n12     @hasandpiker        58   12      58   all\n13      @sambarty2k        58   12      58   all\n14         @tiamani        55   14      55   all\n15     @matefeedart        50   15      50   all\n16           @wihby        47   16      47   all\n17      @freejam013        44   17      44   all\n18      @austinshow        42   18      42   all\n19         @mf_jewm        41   19      41   all\n20            @chat        41   19      33   all\n21    @lakemcgroove        37   21      37   all\n22            @mhud        37   21      37   all\n23  @thistwitchname        36   23      36   all\n24     @mangobreezy        35   24      35   all\n25        @mijnboot        35   24      35   all"
  },
  {
    "objectID": "slides/slides-07.html#gezielte-suche-nach-spezifischen-worten",
    "href": "slides/slides-07.html#gezielte-suche-nach-spezifischen-worten",
    "title": "üî® Text as data in R",
    "section": "Gezielte Suche nach spezifischen Worten",
    "text": "Gezielte Suche nach spezifischen Worten\nHintergrund und Anwendung von Diktion√§ren\n\nListen von Features, die ein bestimmtes Konstrukt (z.B. Emotionalisierung) beschreiben.\nIn der (klassischen) Diktion√§r-Analysen wird gez√§hlt, wie h√§ufig manifeste Features vorkommen, um darauf zu schliessen, inwiefern ein latentes Konstrukt vorkommt.\nVerschiedene Unterscheidungen:\n\nOff-the-shelf (z.B. LIWC, ANEW) vs.¬†organische (eigene, dom√§nenspezifische) Diktion√§re\nBreite (m√∂glichst umfassende Wortliste) vs.¬†spezifische (m√∂glichst spezifische Wortliste) Diktion√§re"
  },
  {
    "objectID": "slides/slides-07.html#organisch-aber-datenbasiert",
    "href": "slides/slides-07.html#organisch-aber-datenbasiert",
    "title": "üî® Text as data in R",
    "section": "Organisch, aber datenbasiert",
    "text": "Organisch, aber datenbasiert\nVorstellung des (erstellen) Emoji-Diktion√§rs\n\n# Load custom emoji-dictionary\ndict_chat_emotes &lt;- readRDS(here(\"local_data/dictionary_chat_emotes.RDS\"))\n\n# Output\ndict_chat_emotes\n\nDictionary object with 5546 key entries.\n- [0Unroll]:\n  - 0unroll\n- [1]:\n  - 1\n- [2020Celebrate]:\n  - 2020celebrate\n- [2020Forward]:\n  - 2020forward\n- [2020Glitchy]:\n  - 2020glitchy\n- [2020Pajamas]:\n  - 2020pajamas\n[ reached max_nkey ... 5,540 more keys ]"
  },
  {
    "objectID": "slides/slides-07.html#welche-emojis-werden-am-h√§ufigsten-verwendet",
    "href": "slides/slides-07.html#welche-emojis-werden-am-h√§ufigsten-verwendet",
    "title": "üî® Text as data in R",
    "section": "Welche emojis werden am h√§ufigsten verwendet?",
    "text": "Welche emojis werden am h√§ufigsten verwendet?\nBeispiele f√ºr Analysen auf Basis der DFM: Dictionary\n\n\n# Lookup emojis in DFM of chats\ndfm_emotes &lt;- dfm_chats %&gt;% \n  quanteda::dfm_lookup(\n    dictionary = dict_chat_emotes)\n\n# Output frequency of emojis\ndfm_emotes %&gt;% \n  quanteda.textstats::textstat_frequency(\n    n = 25) \n\n\n         feature frequency rank docfreq group\n1            LUL     20194    1   14967   all\n2           hasL     12455    2    5856   all\n3    bleedPurple      5188    3    5174   all\n4          Kappa      4971    4    4240   all\n5        hasSlam      2989    5    1002   all\n6    NotLikeThis      2341    6    1354   all\n7             üáµüá∏      1968    7     780   all\n8        hasChud      1792    8    1206   all\n9          hasHi      1401    9     851   all\n10          hasO      1375   10     609   all\n11       hasBoot      1209   11     551   all\n12       hasRaid      1092   12     470   all\n13      elbyBlom      1001   13    1001   all\n14       WutFace       964   14     687   all\n15     hasBaited       901   15     396   all\n16       hasMods       853   16     604   all\n17          Guns       755   17     728   all\n18      hasKkona       727   18     384   all\n19     DinoDance       721   19     269   all\n20       PopNemo       709   20     334   all\n21 TwitchConHYPE       630   21     236   all\n22      hasSadge       601   22     481   all\n23      has0head       599   23     301   all\n24       hasFlex       569   24     294   all\n25             e       563   25     496   all"
  },
  {
    "objectID": "slides/slides-07.html#and-now-you",
    "href": "slides/slides-07.html#and-now-you",
    "title": "üî® Text as data in R",
    "section": "And now ‚Ä¶ you!",
    "text": "And now ‚Ä¶ you!\nGruppenarbeit (ca. 15 Minuten) mit kurzer Ergebnisdiskussion (ca. 15 Minuten)\n\n\n\n\n\n\nArbeitsauftrag\n\n\n\n√úberlegt zusammen mit eurer/m Pr√§sentationspartner:in (ca. 5 Minuten), welche Bereinigungschritte f√ºr die jeweiligen Daten (Chats, Transkripte und Korpus) im Kontext euers Projekts notwendig sind.\nDiskutiert eure Ergebnisse mit einer anderen Pr√§sentationsgruppe (ca. 5 Minuten).\nDokumentiert euer Fazit (inklusive der konkreten Schritte) auf einer der Folienvorlagen (siehe n√§chste Slide).\n\n\n\n\n\n\n\n\n‚àí+\n05:00"
  },
  {
    "objectID": "slides/slides-07.html#please-discuss",
    "href": "slides/slides-07.html#please-discuss",
    "title": "üî® Text as data in R",
    "section": "Please discuss!",
    "text": "Please discuss!\nBitte nutzt die jeweilige Folienvorlage f√ºr die Dokumentation euerer Ergebnisse\n\n\n\n\n\n     Gruppe A\n\n\n\n\n\n     Gruppe B\n\n\n\n\n\n     Gruppe C\n\n\n\n\n‚àí+\n10:00"
  },
  {
    "objectID": "slides/slides-07.html#references",
    "href": "slides/slides-07.html#references",
    "title": "üî® Text as data in R",
    "section": "References",
    "text": "References\n\n\nBaden, C., Pipal, C., Schoonvelde, M., & Velden, M. A. C. G. van der. (2022). Three Gaps in Computational Text Analysis Methods for Social Sciences: A Research Agenda. Communication Methods and Measures, 16(1), 1‚Äì18. https://doi.org/10.1080/19312458.2021.2015574\n\n\nBenoit, K., Watanabe, K., Wang, H., Nulty, P., Obeng, A., M√ºller, S., & Matsuo, A. (2018). Quanteda: An r package for the quantitative analysis of textual data. Journal of Open Source Software, 3(30), 774. https://doi.org/10.21105/joss.00774\n\n\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267‚Äì297. https://doi.org/10/f458q9\n\n\nHase, V. (2023). Automated Content Analysis (F. Oehmer-Pedrazzi, S. H. Kessler, E. Humprecht, K. Sommer, & L. Castro, Eds.; pp. 23‚Äì36). Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-36179-2_3\n\n\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O‚ÄôReilly.\n\n\nZheng, A., & Casari, A. (2018). Feature engineering for machine learning: Principles and techniques for data scientists (First edition). O‚ÄôReilly."
  },
  {
    "objectID": "slides/slides-01.html#und-nun-zu-ihnen",
    "href": "slides/slides-01.html#und-nun-zu-ihnen",
    "title": "Kick-Off",
    "section": "Und nun zu Ihnen!",
    "text": "Und nun zu Ihnen!\nVorstellungsrunde\n\nWie hei√üen Sie?\nWas studieren Sie aktuell?\nWas und wo haben Sie im Bachelor studiert?\nWelches soziale Netzwerk/Medium haben Sie letzte Woche am meisten genutzt und warum?\nBeteiligen Sie sich an Online-Diskussionen und wenn ja, wo und warum?\n\n\n\nHintegrund und Vorwissen\nMediennutzung"
  },
  {
    "objectID": "slides/slides-01.html#mehr-daten-durch-fortschreitende-digitalisierung",
    "href": "slides/slides-01.html#mehr-daten-durch-fortschreitende-digitalisierung",
    "title": "Kick-Off",
    "section": "Mehr Daten durch fortschreitende Digitalisierung",
    "text": "Mehr Daten durch fortschreitende Digitalisierung\nBeispiel: Wachsenden Anzahl eingebauter Smartphone-Sensoren\n\nGraphik aus Struminskaya et al. (2020)"
  },
  {
    "objectID": "slides/slides-01.html#verlagerung-in-den-digitalen-raum",
    "href": "slides/slides-01.html#verlagerung-in-den-digitalen-raum",
    "title": "Kick-Off",
    "section": "Verlagerung in den digitalen Raum",
    "text": "Verlagerung in den digitalen Raum\nTrend der Digitalisierung von Verhaltensweisen und Aktivit√§ten\n\n\n\n\n\nQuelle: Engel et al. (2021)\n\n\n\n\nEinschr√§nkungen\n\nSelektive Nutzung von bestimmten digitalen Ger√§ten bzw. Funktionen\nKategorisierung ist Momentaufnahme und nicht √ºberschneidungsfrei\n\n\n\n\nEinige inh√§rent digitale Verhalten (z.B. Web Searches) bei zunehmender Digitalisierung von analogen Verhalten (z.B. Collaborative Work)\nFehlen digitaler Spurendaten in all diesen Quadranten f√ºr bestimmte Personen und bestimmte Verhaltensweisen durch selektive Nutzung digitaler Ger√§te."
  },
  {
    "objectID": "slides/slides-01.html#ein-definitionsversuch-von-dbd",
    "href": "slides/slides-01.html#ein-definitionsversuch-von-dbd",
    "title": "Kick-Off",
    "section": "Ein Definitionsversuch von DBD",
    "text": "Ein Definitionsversuch von DBD\nnach Weller (2021)\n\n\n‚Ä¶ fasst eine Vielzahl von m√∂glichen Datenquellen zusammen, die verschiedene Arten von Aktivit√§ten aufzeichnen\n‚Ä¶ k√∂nnen dabei helfen, Meinungen, Verhalten und Merkmale der menschlichen Nutzung digitaler Technologien zu erkennen"
  },
  {
    "objectID": "slides/slides-01.html#lernziele",
    "href": "slides/slides-01.html#lernziele",
    "title": "Kick-Off",
    "section": "Lernziele",
    "text": "Lernziele\nDie Studierenden werden ‚Ä¶\n\neinen √úberblick √ºber die zentralen M√∂glichkeiten von DBD und die damit verbundenen Herausforderungen bei der Datenerhebung und -aufbereitung bekommen\nlernen die St√§rken und Schw√§chen verschiedener Methoden zur Erhebung von DBD bewerten\nzentrale Anforderungen an Datenschutz, Forschungsethik und Datenqualit√§t kennen und verstehen lernen\nzentrale sozialwissenschaftliche Methoden zur Analyse von DBD kennenlernen\ndas Wissen √ºber DBD, Statistik und Datenanalyse in eigenen kleinen Projekten zu √ºben und anzuwenden"
  },
  {
    "objectID": "slides/slides-01.html#themen-blocks",
    "href": "slides/slides-01.html#themen-blocks",
    "title": "Kick-Off",
    "section": "4 (Themen-)Blocks",
    "text": "4 (Themen-)Blocks\nStruktur und Aufbau des Seminars"
  },
  {
    "objectID": "slides/slides-01.html#twitch-meets-politics-meets-nlp",
    "href": "slides/slides-01.html#twitch-meets-politics-meets-nlp",
    "title": "Kick-Off",
    "section": "Twitch meets Politics meets NLP",
    "text": "Twitch meets Politics meets NLP\nAuswertung von Live-Stream(-Chats) zur US-Presidential Debates\n\n\nEntwicklung eigener kleiner Forschungsprojekte mit\n\nDBD in Form von Live-Chats & Transcripten\nNatrual Language Processing (NLP) f√ºr die Analyse von\nLive-(Reactions)-Streams von  Twitch-Streamer:innen\nauf die (Vice-)Presidential Debates 2024"
  },
  {
    "objectID": "slides/slides-01.html#warum-twitch",
    "href": "slides/slides-01.html#warum-twitch",
    "title": "Kick-Off",
    "section": "Warum  Twitch?",
    "text": "Warum  Twitch?\nPlattform f√ºr Live-Streaming von User-generated content\n\nf√ºhrende Live-Streaming-Plattform, haupts√§chlich f√ºr Videospiele, zunehmend aber andere Kategorien (z.B. ‚ÄúJust Chatting‚Äù)\nerm√∂glicht Streamern:innen Echtzeit-Interaktionen mit Community via Chat\nbietet Monetarisierungsm√∂glichkeiten durch Abonnements, Spenden und Werbung\nin Zahlen: 2023 hat Twitch $3 Milliarden Umsatz generiert, 7,1 Millionen aktive Streamer:innen und 2,41 Millionen aktive Zuschauer:innen, die ingesamt 21,4 Milliarden Stunden an Content konsumiert haben\n\nTrend: Plattform f√ºr politische Diskussionen und Debatten"
  },
  {
    "objectID": "slides/slides-01.html#nlp-angewendet-auf-chat-logs-und-transkripte",
    "href": "slides/slides-01.html#nlp-angewendet-auf-chat-logs-und-transkripte",
    "title": "Kick-Off",
    "section": "NLP angewendet auf Chat-Logs und Transkripte",
    "text": "NLP angewendet auf Chat-Logs und Transkripte\nInformationen zur Datenanalyse & potentielle Analysestrategien\n\nTranskripte & Chats der Live-Streams von  hasanabi und  zackrawrr und | TheMajorityReport zu der Presidential (Harris vs.¬†Trump) und Vice-Presidential (Vance vs.¬†Walz) Debates 2024\n\nDownload der VODs mit dem  Paket twitch-dl\nDownload & Stream des Live-Chats mit dem  Paket chat_downloader\nTranskription der Streams & der Debatte mit AI-based Transkriptionstool NoScribe\n\nAuswertung mit NLP-Methoden (z.B. Topic Modeling, Sentimentanalyse etc.)"
  },
  {
    "objectID": "slides/slides-01.html#kurzer-exkurs-zur-auswertung-analyse",
    "href": "slides/slides-01.html#kurzer-exkurs-zur-auswertung-analyse",
    "title": "Kick-Off",
    "section": "Kurzer Exkurs zur Auswertung & Analyse",
    "text": "Kurzer Exkurs zur Auswertung & Analyse\nH√§ufig gestellte Fragen zu notwendigen Methodenvorkenntnissen\n\nWelchen Vorkenntnisse sind f√ºr den Kurs vorausgesetzt? Interesse an sozialwissenschaftlichen Perspektiven auf Medien, Kommunikation und digitale Technologien & Grundkenntnisse in der Arbeit mit Statistikprogrammen (z.B. R, Python, Stata, SPSS)\nWerden wir praktisch mit Statistikprogrammen arbeiten? Ja. Dazu werden wir R bzw. RStudio nutzen.\n\nDeswegen: Bitte üíª mitbringen!\n\nWerden wir die mathematische Grundlagen der vorgestellten Methoden lernen? Ja und Nein. Der Kurs konzentriert sich in erster Linie auf die Anwendung; einige mathematische Parameter der vorgestellten Methoden werden jedoch f√ºr die Anwendung ben√∂tigt und deswegen kurz er√∂rtert."
  },
  {
    "objectID": "slides/slides-01.html#vorl√§ufiger-seminarplan",
    "href": "slides/slides-01.html#vorl√§ufiger-seminarplan",
    "title": "Kick-Off",
    "section": "(Vorl√§ufiger) Seminarplan",
    "text": "(Vorl√§ufiger) Seminarplan\n\n\n\n\n\n\n\n\nSession\nDatum\nTopic\nPresenter\n\n\n\n\n\nüìÇ Block 1\nIntroduction\n\n\n\n1\n23.10.2024\nKick-Off\nChristoph Adrian\n\n\n2\n30.10.2024\nDBD: Overview & Introduction\nChristoph Adrian\n\n\n3\n06.11.2024\nüî® Introduction to working with R\nChristoph Adrian\n\n\n\nüìÇ Block 2\nTheoretical Background: Twitch & TV Election Debates\n\n\n\n4\n13.11.2024\nüìö Twitch-Nutzung im Fokus\nStudent groups\n\n\n5\n20.11.2024\nüìö (Wirkungs-)Effekte von Twitch & TV-Debatten\nStudent groups\n\n\n6\n27.11.2024\nüìö Politische Debatten & Social Media\nStudent groups\n\n\n\nüìÇ Block 3\nMethod: Natural Language Processing\n\n\n\n7\n04.12.2024\nüî® Text as data I: Introduction\nChristoph Adrian\n\n\n8\n11.12.2024\nüî® Text as data II: Advanced Methods\nChristoph Adrian\n\n\n9\n18.12.2024\nüî® Advanced Method I: Topic Modeling\nChristoph Adrian\n\n\n\nNo lecture\nüéÑChristmas Break\n\n\n\n10\n08.01.2025\nüî® Advanced Method II: Machine Learning\nChristoph Adrian\n\n\n\nüìÇ Block 4\nProject Work\n\n\n\n11\n15.01.2025\nüî® Project work\nStudent groups\n\n\n12\n22.01.2025\nüî® Project work\nStudent groups\n\n\n13\n29.01.2025\nüìä Project Presentation I\nStudent groups (TBD)\n\n\n14\n05.02.2025\nüìä Project Presentation & üèÅ Evaluation\nStudentds (TBD) & Christoph Adrian"
  },
  {
    "objectID": "slides/slides-01.html#studon-github",
    "href": "slides/slides-01.html#studon-github",
    "title": "Kick-Off",
    "section": "StudOn & Github",
    "text": "StudOn & Github\nMaterialien und Kommunikation im Kurs\n\nInformationen zu Kurs (Semesterplan, Syllabus, Pr√ºfungleistungen etc.) & den einzelnen Sitzungen (Slides, Literatur und ggf. √úbungsmaterial) finden sich auf der Github-Kursseite\nPflichtliteratur der Pr√§sentationen & Beispieldatens√§tze werden auf StudOn bereitgestellt\nF√ºr die Kommunikation gilt\n\nAnk√ºndigungen werden √ºber den StudOn-Verteiler versendet\nf√ºr Fragen, allgemein zum Kurs oder spezifische zu R nutzten Sie bitte das Forum\nKontakt √ºber E-Mail bitte nur bei pers√∂nlichen Anliegen, die nicht f√ºr die Gruppe relvevant sind.\n\n\n\n\nKurze Vorf√ºhrung der Webseite\nZulip Frage/Probleme bei Registrierung?\nOptional: Github/OSF"
  },
  {
    "objectID": "slides/slides-01.html#what-is-expected",
    "href": "slides/slides-01.html#what-is-expected",
    "title": "Kick-Off",
    "section": "What is expected",
    "text": "What is expected\nLeistungsanforderungen & Pr√ºfungsleistungen\n\nRegelm√§√üig aktive Teilnahme an Sitzungen\n\nmax. zwei unentschuldigte Fehltermine (Kulanzregelung), bei Krankheit z√§hlen die Fehltermine mit\n\nBearbeitung von vier Assignments im Rahmen eines Portfolios:\n\nüë• Gruppenpr√§sentation zu wissenschaftlichen Grundlagen üìö (30 Pkt.)\nüë• Gruppenpr√§sentation zum Forschungsprojekt üìä (15 Pkt.)\nüë§ Peer Review (15 Pkt.)\nüë• Projektbericht (40 Pkt.)"
  },
  {
    "objectID": "slides/slides-01.html#die-grundlagenpr√§sentation",
    "href": "slides/slides-01.html#die-grundlagenpr√§sentation",
    "title": "Kick-Off",
    "section": "Die ‚ÄúGrundlagenpr√§sentation‚Äù",
    "text": "Die ‚ÄúGrundlagenpr√§sentation‚Äù\nInformationen zur Gruppenpr√§sentation zu wissenschaftlichen Grundlagen üìö\n\nliefert √úberblick √ºber das zentrale Thema der Pflichtlekt√ºre, z.B. zentrale Begriffe, Definitionen, Datengrundlage & - aufbereitung sowie methodische Vorgehensweise (inklusive Analyse)\nvermittelt theoretische Grundlage f√ºr Forschungsprojekt\nalle bereitgestellten Pflichtexte m√ºssen in der Pr√§sentation ber√ºcksichtigt werden, die Gruppe kann aber eigene Schwerpunkte setzen und eigene Quellen hinzuf√ºgen\nzwischen 20 und 30 Minuten, danach Zeit f√ºr Fragen und Diskussion\n\nBesonderheit: vorher verpflichtendes Feedbackgespr√§ch"
  },
  {
    "objectID": "slides/slides-01.html#feedback-dann-wenn-es-am-meisten-hilft",
    "href": "slides/slides-01.html#feedback-dann-wenn-es-am-meisten-hilft",
    "title": "Kick-Off",
    "section": "Feedback dann, wenn es am meisten hilft",
    "text": "Feedback dann, wenn es am meisten hilft\nInformationen zum verpflichtenden Feedbackgespr√§ch\n\n\nStudierende erhalten Feedback und Tipps, wie sie ihre Pr√§sentation √ºberarbeiten bzw. optimieren k√∂nnen\n30-min√ºtiges Gespr√§ch in der Sprechstunde (mittwochs, 15:30 bis 16:30 Uhr), eine Woche vor der Pr√§sentation\n\nBuchung des Termins √ºber StudOn\n\nAbgabe des ersten vollst√§ndigen Pr√§sentationsentwurfs als PowerPoint- oder PDF-Datei sp√§testens bis 12:00 Uhr am Vortag des Feedback-Gespr√§chs"
  },
  {
    "objectID": "slides/slides-01.html#die-projektpr√§sentation",
    "href": "slides/slides-01.html#die-projektpr√§sentation",
    "title": "Kick-Off",
    "section": "Die ‚ÄúProjektpr√§sentation‚Äù",
    "text": "Die ‚ÄúProjektpr√§sentation‚Äù\nInformationen zur Gruppenpr√§sentation zum Forschungsprojekt üìä\n\nPr√§sentation des (aktuellen Stand des) Projekts, inklusive Vorstellung des theoretischen Hintergrundes, Forschungsfrage Datenaufbereitung, Analyse und ersten Ergebnisse\nzwischen 15 bis 20 Minuten (inklusive mind. 5 Minuten f√ºr Nachfragen), maximal 10 Folien\nbietet M√∂glichkeit f√ºr Problembesprechung, Feedback und Diskussion\nzur Vereinfachung der Koordination werden Google Slides Vorlagen zur Verf√ºgung gestellt\n\nBesonderheit: Grundlage f√ºr das Peer Review"
  },
  {
    "objectID": "slides/slides-01.html#feedback-geben-lernen",
    "href": "slides/slides-01.html#feedback-geben-lernen",
    "title": "Kick-Off",
    "section": "Feedback geben (lernen)",
    "text": "Feedback geben (lernen)\nInformationen zum Peer Review der Gruppenpr√§sentationen\n\nPeer-Review-Formularen (Ratingskalen sowie offenen Fragen) f√ºr Pr√§sentationen anderer Gruppen ausf√ºllen\nlernen, andere Projekte zu bewerten und konstruktives Feedback zu geben\nGruppen erhalten zus√§tzliches Feedback zum eigenen Projekt, dass f√ºr die Pr√§sentation bzw. den Projektbericht verwendet werden kann\n\nBesonderheit: Individuelles Assigment!"
  },
  {
    "objectID": "slides/slides-01.html#zusammenf√ºhrung-aller-assignments",
    "href": "slides/slides-01.html#zusammenf√ºhrung-aller-assignments",
    "title": "Kick-Off",
    "section": "Zusammenf√ºhrung aller Assignments",
    "text": "Zusammenf√ºhrung aller Assignments\nInformationen zum Projektberichtch\n\nProjektbericht ist die schriftliche Ausarbeitung des Forschungsprojekts und f√ºhrt damit die Arbeit aus den Pr√§sentationen und dem (Feedback der) Peer Reviews zusammen\nmindestens eine der vorgestellten Methoden auf ein selbstgew√§hlte Stichprobe der bereitgestellten Daten anwenden, um ein Thema Ihrer Wahl zu erforschen.\n750 bis 1000 W√∂rter pro Personn, bei einem Gruppenbericht skaliert die Anzal der W√∂rter mit einem Faktor von 0,8 pro Person (z.B. sollte eine Zweiergruppe 1200 bis 1600 W√∂rter schreiben, eine Dreiergruppe 1800 bis 2400 W√∂rter)\nAbgabe als Quarto-Dokument (& PDF), d.h. der Bericht sowie alle Komponenten (z.B. Tabellen, Grafiken) werden in RStudio erstellt und m√ºssen komplett reproduzierbar sein"
  },
  {
    "objectID": "slides/slides-01.html#short-summary",
    "href": "slides/slides-01.html#short-summary",
    "title": "Kick-Off",
    "section": "Short summary",
    "text": "Short summary\nFahrplan f√ºr die Pr√ºfungsleistungen\n\n\n\n\n\n\n\n\n\nZeitpunkte\nLeistung\n\n\n\n\nBlock II (13./20./27.11)\n‚ÄúThemenpr√§sentation‚Äù üìö\n\n\nbis zum 4.12.\nR-Tutorials abschlie√üen (Grundlage f√ºr Methodenblock)\n\n\nbis zum 15.01.\nErster vollst√§ndiger Entwurf ‚ÄúProjektpr√§sentation‚Äù üìä\n\n\nvom 16.01. bis zum 21.01.\nAusf√ºllen der Peer-Review-Formulare\n\n\nvom 22.01 bis 29.01/05.02\nEinarbeitung des Feedback in ‚ÄúProjektpr√§sentation‚Äù üìä\n\n\n29.01./05.02\n‚ÄúProjektpr√§sentation‚Äù üìä\n\n\nbis zum 02.03.25\nAbgabe des Projektberichts"
  },
  {
    "objectID": "slides/slides-01.html#warum",
    "href": "slides/slides-01.html#warum",
    "title": "Kick-Off",
    "section": "Warum ?",
    "text": "Warum ?\nDer Einsatz von R bzw. RStudio im Kurs\n\nKostenlose Software mit vielen n√ºtzlichen und beginner-friendly Tutorials\n or ? Both!\n\nIm Kurs:\n\nBestehende R-Kenntnisse sind f√∂rderlich, aber nicht zwigend notwendig, wichtiger sind praktische Erfahrung im syntaxbasierten Arbeiten\nLearn to code by example: Code von Sitzungen & Beispielen wird bereitgestellt (ggf. durch Tutorials)\nPflicht: Tutorials auf Kurshomepage"
  },
  {
    "objectID": "slides/slides-01.html#building-a-common-knowledge-base",
    "href": "slides/slides-01.html#building-a-common-knowledge-base",
    "title": "Kick-Off",
    "section": "Building a common knowledge base",
    "text": "Building a common knowledge base\nInformationen zu den -Tutorials\n\nVermittlung des notwendigen Basiswissens f√ºr die Arbeit mit R, RStudio und Quarto mit Hilfe von zwei (Video-)Tutorials:\n\nEinf√ºhrung in R, RStudio und Quarto von Andy Field f√ºr die Vermittlung der Grundlagen & den allgemeinen Umgang mit den Programmen\nPraktisches Arbeiten mit R des CCS Amsterdam legt Schwerpunkt auf das ‚Äûinhaltliche‚Äú Arbeiten mit R (Vorstellung von Verwendung von wichtigen Funktionen)\n\nWeitere n√ºtzliche Quellen finden Sie in dem Bereich Working with R auf der Kursseite"
  },
  {
    "objectID": "slides/slides-01.html#sonderanmeldetermin-f√ºr-pr√ºfung",
    "href": "slides/slides-01.html#sonderanmeldetermin-f√ºr-pr√ºfung",
    "title": "Kick-Off",
    "section": "Sonderanmeldetermin f√ºr Pr√ºfung",
    "text": "Sonderanmeldetermin f√ºr Pr√ºfung\nWichtige Informationen zur Pr√ºfungsanmeldung\n\n√Ñrgerlicherweise gibt es immer wieder Studierende, die sich anmelden und betreuen lassen, aber dann einfach irgendwann (teilweise sehr kurzfristig) ‚Äúverschwinden‚Äù.\nDeshalb: Nutzung des WiSo-Sonderanmeldetermin f√ºr Pr√ºfung am 28./29.10.\n\nBitte ber√ºcksichtigen Sie unbedingt:\n\n‚ö†Ô∏è Nehmen Sie bitte kein Thema an, wenn absehbar ist, dass Sie nicht teilnehmen werden.\n‚ö†Ô∏è Wir behalten uns vor, bei R√ºckzug trotz abgeschlossener Themenvergabe, Sie trotzdem f√ºr die Pr√ºfung zu melden"
  },
  {
    "objectID": "slides/slides-01.html#please-state-your-preference",
    "href": "slides/slides-01.html#please-state-your-preference",
    "title": "Kick-Off",
    "section": "Please state your preference",
    "text": "Please state your preference\nVergabe der Pr√§sentationsthemen mit SimpleAssign\n\n\n\n\nBitte scannen Sie den QR-Code oder nutzen Sie folgenden Link und geben Sie Ihre Themenpr√§ferenz an:\n\nhttps://simpleassign.net/poll/-O9lSn9X3pAIOdHyNQNu\n\n\n\n\n\n\n\n    \n\n\n\n‚àí+\n02:00"
  },
  {
    "objectID": "slides/slides-01.html#before-we-meet-again",
    "href": "slides/slides-01.html#before-we-meet-again",
    "title": "Kick-Off",
    "section": "Before we meet again",
    "text": "Before we meet again\nHinweise und offene Fragen\n\nLernen Sie die Kursseite kennen! Und checken Sie die Infos () zur n√§chten Sitzung.\nVerschaffen Sie sich einen √úberblick √ºber die R-Tutorials\n\nEin paar Fragen an Sie:\n\nWhy no English? ü§∑\nWarum das gro√üe Interesse an Zeitreihenanalyse?\nWelche Erwartung an ‚ÄúMachine Learning‚Äù?"
  },
  {
    "objectID": "slides/slides-01.html#literatur",
    "href": "slides/slides-01.html#literatur",
    "title": "Kick-Off",
    "section": "Literatur",
    "text": "Literatur\n\n\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Digital trace data (1st ed., pp. 100‚Äì118). Routledge. https://www.taylorfrancis.com/books/9781003024583/chapters/10.4324/9781003024583-8\n\n\nStruminskaya, B., Lugtig, P., Keusch, F., & H√∂hne, J. K. (2020). Augmenting Surveys With Data From Sensors and Apps: Opportunities and Challenges. Social Science Computer Review, 089443932097995. https://doi.org/10.1177/0894439320979951\n\n\nWeller, K. (2021). A short introduction to computational social science and digital behavioral data. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meettheexperts_Introductioncss.pdf"
  },
  {
    "objectID": "slides/slides-03.html#seminarplan",
    "href": "slides/slides-03.html#seminarplan",
    "title": "üî® Working with R",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\nSession\nDatum\nTopic\nPresenter\n\n\n\n\n\nüìÇ Block 1\nIntroduction\n\n\n\n1\n23.10.2024\nKick-Off\nChristoph Adrian\n\n\n2\n30.10.2024\nDBD: Overview & Introduction\nChristoph Adrian\n\n\n3\n06.11.2024\nüî® Introduction to working with R\nChristoph Adrian\n\n\n\nüìÇ Block 2\nTheoretical Background: Twitch & TV Election Debates\n\n\n\n4\n13.11.2024\nüìö Twitch-Nutzung im Fokus\nStudent groups\n\n\n5\n20.11.2024\nüìö (Wirkungs-)Effekte von Twitch & TV-Debatten\nStudent groups\n\n\n6\n27.11.2024\nüìö Politische Debatten & Social Media\nStudent groups\n\n\n\nüìÇ Block 3\nMethod: Natural Language Processing\n\n\n\n7\n04.12.2024\nüî® Text as data I: Introduction\nChristoph Adrian\n\n\n8\n11.12.2024\nüî® Text as data II: Advanced Methods\nChristoph Adrian\n\n\n9\n18.12.2024\nüî® Advanced Method I: Topic Modeling\nChristoph Adrian\n\n\n\nNo lecture\nüéÑChristmas Break\n\n\n\n10\n08.01.2025\nüî® Advanced Method II: Machine Learning\nChristoph Adrian\n\n\n\nüìÇ Block 4\nProject Work\n\n\n\n11\n15.01.2025\nüî® Project work\nStudent groups\n\n\n12\n22.01.2025\nüî® Project work\nStudent groups\n\n\n13\n29.01.2025\nüìä Project Presentation I\nStudent groups (TBD)\n\n\n14\n05.02.2025\nüìä Project Presentation & üèÅ Evaluation\nStudentds (TBD) & Christoph Adrian"
  },
  {
    "objectID": "slides/slides-03.html#kurzes-organisatorische-update",
    "href": "slides/slides-03.html#kurzes-organisatorische-update",
    "title": "üî® Working with R",
    "section": "Kurzes organisatorische Update",
    "text": "Kurzes organisatorische Update\nInformationen zu Kursdetails und Pr√ºfungsleistungen\n\nInfos zur Zuteilung der Feedbackgespr√§che\n\nKeine Anmeldung f√ºr Feedback notwendig, stattdessen Reihenfolge wie bei Pr√§sentation\nBei Terminproblemen k√∂nnt ihr gerne auf mich zukommen\n\nüó£Ô∏è 2. Pr√§sentationsgruppe: Bitte denkt\n\nan die Zusendung des Entwurf der Pr√§sentationsfolien bis sp√§testens n√§chste Woche Dienstag 12:00!\ndas Feedbackgespr√§ch am Mittwoch im Anschluss an das Seminar."
  },
  {
    "objectID": "slides/slides-03.html#building-best-practice",
    "href": "slides/slides-03.html#building-best-practice",
    "title": "üî® Working with R",
    "section": "Building best practice",
    "text": "Building best practice\nWillkommen (zur√ºck) zu \n\n\n\nHow most academics learn R:\n\n\n\n\n\n\n\nHow you should learn R:\n\nR nicht systematisch lernen, sondern spezifisch anwenden.\nOrganisation der Arbeit mit RStudio-Projekten\nLesbaren und nachvollziehbaren Code schreiben!\n(Nach-)Fragen!"
  },
  {
    "objectID": "slides/slides-03.html#ein-repository-voller-daten",
    "href": "slides/slides-03.html#ein-repository-voller-daten",
    "title": "üî® Working with R",
    "section": "Ein Repository voller Daten",
    "text": "Ein Repository voller Daten\nBeispiel f√ºr √úbung durch Anwendung:  tidytuesday (social data project)\n\n\n\nData is posted to social media every Monday morning.\nExplore the data, watching out for interesting relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "slides/slides-03.html#beispiele-f√ºr-tidytuesday",
    "href": "slides/slides-03.html#beispiele-f√ºr-tidytuesday",
    "title": "üî® Working with R",
    "section": "Beispiele f√ºr #tidytuesday",
    "text": "Beispiele f√ºr #tidytuesday"
  },
  {
    "objectID": "slides/slides-03.html#everything-you-need-in-one-place",
    "href": "slides/slides-03.html#everything-you-need-in-one-place",
    "title": "üî® Working with R",
    "section": "Everything you need in one place",
    "text": "Everything you need in one place\nOrganisation der Arbeit mit RStudio-Projekten\n\n\n\n\n\n\n\n\nEmpfehlungen:\n\nF√ºr jedes Projekt ein RStudio-Projekt.\nSicherung und Organisation von Daten, Skripte und Ouput an einem Ort, z.B. mit Unterst√ºtzung durch R-Pakete wie z.B. prodigenr\nVerwendung von relative, keine absoluten Pfade. Empfehlung: here R-Paket"
  },
  {
    "objectID": "slides/slides-03.html#versionskontrolle-als-k√ºr",
    "href": "slides/slides-03.html#versionskontrolle-als-k√ºr",
    "title": "üî® Working with R",
    "section": "Versionskontrolle als K√ºr",
    "text": "Versionskontrolle als K√ºr\nCrashkurs zu Git(Hub)\n\n MalikaIhle\nVersionkontrolle f√ºr Code, gesichert in der Cloud\nVollst√§ndige R√ºckverfolgbarkeit von (gesicherten) √Ñnderungen\nGreat effort, great return."
  },
  {
    "objectID": "slides/slides-03.html#run-chunks-not-whole-scripts",
    "href": "slides/slides-03.html#run-chunks-not-whole-scripts",
    "title": "üî® Working with R",
    "section": "Run chunks, not (whole) scripts",
    "text": "Run chunks, not (whole) scripts\nOutputorientiertes Coding mit Quarto"
  },
  {
    "objectID": "slides/slides-03.html#quarto-rmarkdown-rscript",
    "href": "slides/slides-03.html#quarto-rmarkdown-rscript",
    "title": "üî® Working with R",
    "section": "Quarto ‚â• RMarkdown ‚â• RScript",
    "text": "Quarto ‚â• RMarkdown ‚â• RScript\nDer Weg vom Code zum Output\n\n\nGrundidee von Quarto : ein Quelldokument kann in eine Vielzahl von Ausgabeformaten umgewandelt werden\nMarkdown-Syntax f√ºr Text, verschiedene Programmiersprachen (wie z.B. R und Python) in einem Dokument"
  },
  {
    "objectID": "slides/slides-03.html#develop-your-style",
    "href": "slides/slides-03.html#develop-your-style",
    "title": "üî® Working with R",
    "section": "Develop your style",
    "text": "Develop your style\nWichtigkeit der Codeformatierung und -dokumentierung\n\n# Strive for \nshort_flights &lt;- flights |&gt; filter(air_time &lt; 60)\n\n# Avoid:\nSHTFTS &lt;- flights |&gt; filter(air_time &lt; 60)\n\n\n# Strive for \nflights |&gt;  \n  filter(!is.na(arr_delay), !is.na(tailnum)) |&gt; \n  count(dest)\n\n# Avoid\nflights|&gt;filter(!is.na(arr_delay), !is.na(tailnum))|&gt;count(dest)\n\n\n\nDie Entwicklung (oder Aneignung) eines Codestils ist wichtig!\nWas sich zun√§chst willk√ºrlich anf√ºhlt, hilft mit der Zeit sehr\nUnterst√ºtzung durch den tidyverse style guide bzw. die Pakete styler oder lintr"
  },
  {
    "objectID": "slides/slides-03.html#empfehlung-tidyverse-is-your-friend",
    "href": "slides/slides-03.html#empfehlung-tidyverse-is-your-friend",
    "title": "üî® Working with R",
    "section": "Empfehlung: tidyverse is your friend!",
    "text": "Empfehlung: tidyverse is your friend!\nVerschiedenen Paketen f√ºr alle Schritte eines Projektes\n\nQuelle: RStudio"
  },
  {
    "objectID": "slides/slides-03.html#the-friend-of-your-friend-easystats",
    "href": "slides/slides-03.html#the-friend-of-your-friend-easystats",
    "title": "üî® Working with R",
    "section": "The friend of your friend: easystats",
    "text": "The friend of your friend: easystats\nFokus auf die Analyse\n\nQuelle: L√ºdecke et al. (2022)"
  },
  {
    "objectID": "slides/slides-03.html#am-anfang-steht-die-theorie",
    "href": "slides/slides-03.html#am-anfang-steht-die-theorie",
    "title": "üî® Working with R",
    "section": "Am Anfang steht die Theorie",
    "text": "Am Anfang steht die Theorie\nTypischer ‚Äúdata science process‚Äù als Kontext der Sitzung\n\nQuelle: Wickham et al. (2023)\n\nAdditional steps (add if necessary):"
  },
  {
    "objectID": "slides/slides-03.html#age-difference-in-years-between-move-love-interests",
    "href": "slides/slides-03.html#age-difference-in-years-between-move-love-interests",
    "title": "üî® Working with R",
    "section": "Age difference in years between move love interests",
    "text": "Age difference in years between move love interests\nDatengrundlage f√ºr die Beispiele: Hollywood Age Gap ( |  )\n\n\n\n\n\n\n\n\n\n\n‚ÄúAn informational site showing the age gap between movie love interests.‚Äù\nCommunity-Projekt\n\nGuidlines for participation/submission:\n\nThe two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)\nThe youngest of the two actors is at least 17 years old\nNot animated characters"
  },
  {
    "objectID": "slides/slides-03.html#explore-adapt-repeat",
    "href": "slides/slides-03.html#explore-adapt-repeat",
    "title": "üî® Working with R",
    "section": "Explore ‚ûû Adapt ‚ûû Repeat ‚ü≥",
    "text": "Explore ‚ûû Adapt ‚ûû Repeat ‚ü≥\nProzess der Datenaufbereitung\n\n\nnimmt in der Regel den Gro√üteil der Zeit der Datenanalyse in Anspruch\nh√§ufig bedarf es der mehrfachen Wiederholung dreier Schritte:\n\nder (explorativen) Erkundung,\nder Standartdisierung und\nder (erneuten) Bereinung der Daten"
  },
  {
    "objectID": "slides/slides-03.html#drei-stufen-der-datenqualit√§t",
    "href": "slides/slides-03.html#drei-stufen-der-datenqualit√§t",
    "title": "üî® Working with R",
    "section": "Drei Stufen der Datenqualit√§t",
    "text": "Drei Stufen der Datenqualit√§t\nTypische Strategien zur Datenbereinigung nach Pearson (2018)\n\n\n\n\n\n\nQuelle: Jonge & Loo (2013)\n\n\n\n\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\nWie viele F√§lle sind enthalten? Wie viele Variablen?\nWie lauten die Variablennamen? Sind sie sinnvoll?\nWelchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\nWie viele eindeutige Werte hat jede Variable?\nWelcher Wert tritt am h√§ufigsten auf, und wie oft kommt er vor?\nGibt es fehlende Werte? Wenn ja, wie h√§ufig ist dies der Fall?\n\nUntersuchung deskriptiver Statistiken f√ºr jede Variable;\nExplorative Visualisierung;\nVerschiedene Verfahren zur Suche nach Anomalien in den Daten;\nUntersuchung der Beziehungen zwischen Schl√ºsselvariablen mit Hilfe von Scatterplots/Boxplots/Mosaic-Plots;\nDokumentation des Vorgehens und der Ergebnisse (z.B. mit .rmd-Dokument). Dient als Grundlage f√ºr die anschlie√üende Analyse und Erl√§uterung der Ergebnisse."
  },
  {
    "objectID": "slides/slides-03.html#direkter-download-via-url",
    "href": "slides/slides-03.html#direkter-download-via-url",
    "title": "üî® Working with R",
    "section": "Direkter Download via URL",
    "text": "Direkter Download via URL\nDatenimport und -preview\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\nüîç Wie viele F√§lle sind enthalten? Wie viele Variablen?\nüîç Wie lauten die Variablennamen? Sind sie sinnvoll?\nWelchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\nWie viele eindeutige Werte hat jede Variable?\nWelcher Wert tritt am h√§ufigsten auf, und wie oft kommt er vor?\nGibt es fehlende Werte? Wenn ja, wie h√§ufig ist dies der Fall?\n\n\n\n\n\nage_gaps &lt;- read_csv(\"http://hollywoodagegap.com/movies.csv\") \nage_gaps \n\n# A tibble: 1,199 √ó 12\n   `Movie Name`       `Release Year` Director    `Age Difference` `Actor 1 Name`\n   &lt;chr&gt;                       &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;         \n 1 Harold and Maude             1971 Hal Ashby                 52 Bud Cort      \n 2 Venus                        2006 Roger Mich‚Ä¶               50 Peter O'Toole \n 3 The Quiet American           2002 Phillip No‚Ä¶               49 Michael Caine \n 4 Solitary Man                 2009 Brian Kopp‚Ä¶               45 Michael Dougl‚Ä¶\n 5 The Big Lebowski             1998 Joel Coen                 45 David Huddles‚Ä¶\n 6 Beginners                    2010 Mike Mills                43 Christopher P‚Ä¶\n 7 Poison Ivy                   1992 Katt Shea                 42 Tom Skerritt  \n 8 Dirty Grandpa                2016 Dan Mazer                 41 Robert De Niro\n 9 Whatever Works               2009 Woody Allen               40 Larry David   \n10 Entrapment                   1999 Jon Amiel                 39 Sean Connery  \n# ‚Ñπ 1,189 more rows\n# ‚Ñπ 7 more variables: `Actor 1 Gender` &lt;chr&gt;, `Actor 1 Birthdate` &lt;date&gt;,\n#   `Actor 1 Age` &lt;dbl&gt;, `Actor 2 Name` &lt;chr&gt;, `Actor 2 Gender` &lt;chr&gt;,\n#   `Actor 2 Birthdate` &lt;chr&gt;, `Actor 2 Age` &lt;dbl&gt;"
  },
  {
    "objectID": "slides/slides-03.html#let-the-cleaning-beginn",
    "href": "slides/slides-03.html#let-the-cleaning-beginn",
    "title": "üî® Working with R",
    "section": "Let the cleaning beginn",
    "text": "Let the cleaning beginn\nErste Schritte der Datenbereinigung\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\n‚úÖ Wie viele F√§lle sind enthalten? Wie viele Variablen?\n‚úÖ Wie lauten die Variablennamen? Sind sie sinnvoll?\nüîç Welchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\nWie viele eindeutige Werte hat jede Variable?\nWelcher Wert tritt am h√§ufigsten auf, und wie oft kommt er vor?\nGibt es fehlende Werte? Wenn ja, wie h√§ufig ist dies der Fall?\n\n\n\n\n\nage_gaps %&lt;&gt;% janitor::clean_names()\nage_gaps %&gt;% glimpse()\n\nRows: 1,199\nColumns: 12\n$ movie_name        &lt;chr&gt; \"Harold and Maude\", \"Venus\", \"The Quiet American\", \"‚Ä¶\n$ release_year      &lt;dbl&gt; 1971, 2006, 2002, 2009, 1998, 2010, 1992, 2016, 2009‚Ä¶\n$ director          &lt;chr&gt; \"Hal Ashby\", \"Roger Michell\", \"Phillip Noyce\", \"Bria‚Ä¶\n$ age_difference    &lt;dbl&gt; 52, 50, 49, 45, 45, 43, 42, 41, 40, 39, 38, 38, 36, ‚Ä¶\n$ actor_1_name      &lt;chr&gt; \"Bud Cort\", \"Peter O'Toole\", \"Michael Caine\", \"Micha‚Ä¶\n$ actor_1_gender    &lt;chr&gt; \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"ma‚Ä¶\n$ actor_1_birthdate &lt;date&gt; 1948-03-29, 1932-08-02, 1933-03-14, 1944-09-25, 193‚Ä¶\n$ actor_1_age       &lt;dbl&gt; 23, 74, 69, 65, 68, 81, 59, 73, 62, 69, 57, 77, 59, ‚Ä¶\n$ actor_2_name      &lt;chr&gt; \"Ruth Gordon\", \"Jodie Whittaker\", \"Do Thi Hai Yen\", ‚Ä¶\n$ actor_2_gender    &lt;chr&gt; \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"man\", ‚Ä¶\n$ actor_2_birthdate &lt;chr&gt; \"1896-10-30\", \"1982-06-03\", \"1982-10-01\", \"1989-01-0‚Ä¶\n$ actor_2_age       &lt;dbl&gt; 75, 24, 20, 20, 23, 38, 17, 32, 22, 30, 19, 39, 23, ‚Ä¶"
  },
  {
    "objectID": "slides/slides-03.html#building-the-habits",
    "href": "slides/slides-03.html#building-the-habits",
    "title": "üî® Working with R",
    "section": "Building the habits!",
    "text": "Building the habits!\nErste Schritte der Datenbereinigung\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\n‚úÖ Wie viele F√§lle sind enthalten? Wie viele Variablen?\n‚úÖ Wie lauten die Variablennamen? Sind sie sinnvoll?\n‚úÖ Welchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\nWie viele eindeutige Werte hat jede Variable?\nWelcher Wert tritt am h√§ufigsten auf, und wie oft kommt er vor?\nGibt es fehlende Werte? Wenn ja, wie h√§ufig ist dies der Fall?\n\n\n\n\n\nage_gaps_correct &lt;- age_gaps %&gt;% \n  mutate(\n    across(ends_with(\"_birthdate\"), ~as.Date(.)) # set dates to dates\n  )\n\n\n\nReminder\n\nVer√§nderungen nicht im selben Datensatz speichern\nVerst√§ndliche Benennung & Kommentierung der Daten\nBearbeitungsschritte kommentieren"
  },
  {
    "objectID": "slides/slides-03.html#kontrolle-der-lageparameter",
    "href": "slides/slides-03.html#kontrolle-der-lageparameter",
    "title": "üî® Working with R",
    "section": "Kontrolle der Lageparameter",
    "text": "Kontrolle der Lageparameter\nErste Schritte der Datenbereinigung\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\n‚úÖ Wie viele F√§lle sind enthalten? Wie viele Variablen?\n‚úÖ Wie lauten die Variablennamen? Sind sie sinnvoll?\n‚úÖ Welchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\nüîç Wie viele eindeutige Werte hat jede Variable?\nüîç Welcher Wert tritt am h√§ufigsten auf, und wie oft kommt er vor?\nüîç Gibt es fehlende Werte? Wenn ja, wie h√§ufig ist dies der Fall?\n\n\n\n\n\nage_gaps_correct %&gt;% \n    datawizard::describe_distribution() %&gt;% \n    print_html()\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nMin\nMax\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nrelease_year\n2000.53\n17.07\n15\n1935\n2024\n-1.62\n2.52\n1199\n0\n\n\nage_difference\n10.62\n8.62\n12\n0\n52\n1.19\n1.54\n1199\n0\n\n\nactor_1_age\n40.07\n10.93\n15\n17\n81\n0.54\n0.22\n1199\n0\n\n\nactor_2_age\n31.22\n8.47\n10\n17\n81\n1.39\n3.71\n1199\n0"
  },
  {
    "objectID": "slides/slides-03.html#lets-start-exploring",
    "href": "slides/slides-03.html#lets-start-exploring",
    "title": "üî® Working with R",
    "section": "Let‚Äôs start exploring!",
    "text": "Let‚Äôs start exploring!\nWie sind die Altersunterschiede verteilt?\n\nage_gaps_correct %&gt;% \n    ggplot(aes(age_difference)) +\n    geom_bar() +\n    theme_pubr()"
  },
  {
    "objectID": "slides/slides-03.html#a-recent-past",
    "href": "slides/slides-03.html#a-recent-past",
    "title": "üî® Working with R",
    "section": "A recent past ‚Ä¶",
    "text": "A recent past ‚Ä¶\nIn welchen Filmen ist der Altersunterschied am h√∂chsten?\n\nage_gaps_correct %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(movie_name, age_difference, release_year) \n\n# A tibble: 1,199 √ó 3\n   movie_name         age_difference release_year\n   &lt;chr&gt;                       &lt;dbl&gt;        &lt;dbl&gt;\n 1 Harold and Maude               52         1971\n 2 Venus                          50         2006\n 3 The Quiet American             49         2002\n 4 Solitary Man                   45         2009\n 5 The Big Lebowski               45         1998\n 6 Beginners                      43         2010\n 7 Poison Ivy                     42         1992\n 8 Dirty Grandpa                  41         2016\n 9 Whatever Works                 40         2009\n10 Entrapment                     39         1999\n# ‚Ñπ 1,189 more rows"
  },
  {
    "objectID": "slides/slides-03.html#or-still-present",
    "href": "slides/slides-03.html#or-still-present",
    "title": "üî® Working with R",
    "section": "‚Ä¶ or still present?",
    "text": "‚Ä¶ or still present?\nIn welchen Filmen ist der Altersunterschied am h√∂chsten?\n\nage_gaps_correct %&gt;% \n    filter(release_year &gt;= 2022) %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(\n        movie_name, age_difference, release_year, \n        actor_1_name, actor_2_name)\n\n# A tibble: 19 √ó 5\n   movie_name              age_difference release_year actor_1_name actor_2_name\n   &lt;chr&gt;                            &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       \n 1 Poor Things                         21         2023 Mark Ruffalo Emma Stone  \n 2 The Bubble                          21         2022 Pedro Pascal Maria Bakal‚Ä¶\n 3 Oppenheimer                         20         2023 Cillian Mur‚Ä¶ Florence Pu‚Ä¶\n 4 The Northman                        20         2022 Alexander S‚Ä¶ Anya Taylor‚Ä¶\n 5 Spaceman                            19         2024 Adam Sandler Carey Mulli‚Ä¶\n 6 The Lost City                       16         2022 Channing Ta‚Ä¶ Sandra Bull‚Ä¶\n 7 We Live in Time                     13         2024 Andrew Garf‚Ä¶ Florence Pu‚Ä¶\n 8 The Idea of You                     12         2024 Nicholas Ga‚Ä¶ Anne Hathaw‚Ä¶\n 9 Barbie                              10         2023 Ryan Gosling Margot Robb‚Ä¶\n10 Twisters                            10         2024 Glen Powell  Daisy Edgar‚Ä¶\n11 Anyone but You                       9         2023 Glen Powell  Sydney Swee‚Ä¶\n12 Everything Everywhere ‚Ä¶              9         2022 Ke Huy Quan  Michelle Ye‚Ä¶\n13 Top Gun: Maverick                    8         2022 Tom Cruise   Jennifer Co‚Ä¶\n14 Oppenheimer                          7         2023 Cillian Mur‚Ä¶ Emily Blunt \n15 Your Place or Mine                   7         2023 Ashton Kutc‚Ä¶ Zo√´ Chao    \n16 Your Place or Mine                   5         2023 Jesse Willi‚Ä¶ Reese Withe‚Ä¶\n17 Poor Things                          2         2023 Christopher‚Ä¶ Emma Stone  \n18 Your Place or Mine                   2         2023 Ashton Kutc‚Ä¶ Reese Withe‚Ä¶\n19 You People                           1         2023 Jonah Hill   Lauren Lond‚Ä¶"
  },
  {
    "objectID": "slides/slides-03.html#durchschnitts-unterschied-nach-jahren",
    "href": "slides/slides-03.html#durchschnitts-unterschied-nach-jahren",
    "title": "üî® Working with R",
    "section": "(Durchschnitts-)Unterschied nach Jahren",
    "text": "(Durchschnitts-)Unterschied nach Jahren\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\nage_gaps_correct %&gt;% \n    group_by(release_year) %&gt;% \n    summarise(age_difference_mean = mean(age_difference)) %&gt;% \n    ggplot(aes(release_year, age_difference_mean)) +\n    geom_col() +\n    theme_pubr()"
  },
  {
    "objectID": "slides/slides-03.html#verteilung-nach-jahren",
    "href": "slides/slides-03.html#verteilung-nach-jahren",
    "title": "üî® Working with R",
    "section": "Verteilung nach Jahren",
    "text": "Verteilung nach Jahren\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\n\nggpubr::ggboxplot(\n    data = age_gaps_correct, \n    x = \"release_year\", \n    y = \"age_difference\"\n  ) +\n  # Rotate x-axis labels by 90 degrees\n  theme(\n    axis.text.x = element_text(\n        angle = 90,\n        vjust = 0.5,\n         hjust=1))"
  },
  {
    "objectID": "slides/slides-03.html#ein-blick-auf-die-korrelation",
    "href": "slides/slides-03.html#ein-blick-auf-die-korrelation",
    "title": "üî® Working with R",
    "section": "Ein Blick auf die Korrelation",
    "text": "Ein Blick auf die Korrelation\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\nage_gaps %&gt;%\n  select(release_year, age_difference) %&gt;% \n  correlation::correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1   |     Parameter2 |     r |         95% CI | t(1197) |         p\n----------------------------------------------------------------------------\nrelease_year | age_difference | -0.22 | [-0.27, -0.17] |   -7.83 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 1199"
  },
  {
    "objectID": "slides/slides-03.html#mit-kanonen-auf-spatzen-schie√üen",
    "href": "slides/slides-03.html#mit-kanonen-auf-spatzen-schie√üen",
    "title": "üî® Working with R",
    "section": "Mit Kanonen auf Spatzen schie√üen",
    "text": "Mit Kanonen auf Spatzen schie√üen\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\n# Sch√§tzung des Models\nmdl &lt;- lm(age_difference ~ release_year, data = age_gaps_correct)\n\n\n\nmdl %&gt;% parameters::parameters()\n\nParameter    | Coefficient |    SE |           95% CI | t(1197) |      p\n------------------------------------------------------------------------\n(Intercept)  |      233.69 | 28.48 | [177.82, 289.57] |    8.21 | &lt; .001\nrelease year |       -0.11 |  0.01 | [ -0.14,  -0.08] |   -7.83 | &lt; .001\n\n\n\n\nmdl %&gt;% performance::model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n8512.891 | 8512.911 | 8528.159 | 0.049 |     0.048 | 8.403 | 8.410"
  },
  {
    "objectID": "slides/slides-03.html#convenience-wrapper",
    "href": "slides/slides-03.html#convenience-wrapper",
    "title": "üî® Working with R",
    "section": "Convenience wrapper",
    "text": "Convenience wrapper\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\nmdl %&gt;% report::report()\n\nWe fitted a linear model (estimated using OLS) to predict age_difference with\nrelease_year (formula: age_difference ~ release_year). The model explains a\nstatistically significant and weak proportion of variance (R2 = 0.05, F(1,\n1197) = 61.35, p &lt; .001, adj. R2 = 0.05). The model's intercept, corresponding\nto release_year = 0, is at 233.69 (95% CI [177.82, 289.57], t(1197) = 8.21, p &lt;\n.001). Within this model:\n\n  - The effect of release year is statistically significant and negative (beta =\n-0.11, 95% CI [-0.14, -0.08], t(1197) = -7.83, p &lt; .001; Std. beta = -0.22, 95%\nCI [-0.28, -0.17])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "slides/slides-03.html#try---fail---repeat",
    "href": "slides/slides-03.html#try---fail---repeat",
    "title": "üî® Working with R",
    "section": "Try - Fail - Repeat",
    "text": "Try - Fail - Repeat\nKurzes Fazit der heutigen Sitzung\n\n\n\n\nWenn R, dann mit RStudio + Quarto!\nAnschauen - nachmachen - ausprobieren\nKeep it tidy\n(Gute) Routinen bilden\n‚ÄúThere is almost always a package for that ‚Ä¶‚Äù"
  },
  {
    "objectID": "slides/slides-03.html#literatur",
    "href": "slides/slides-03.html#literatur",
    "title": "üî® Working with R",
    "section": "Literatur",
    "text": "Literatur\n\n\nJonge, E. de, & Loo, M. van der. (2013). An introduction to data cleaning with R.\n\n\nL√ºdecke, D., Ben-Shachar, M. S., Patil, I., Wiernik, B. M., Bacher, E., Th√©riault, R., & Makowski, D. (2022). Easystats: Framework for easy statistical modeling, visualization, and reporting. CRAN. https://easystats.github.io/easystats/\n\n\nPearson, R. K. (2018). Exploratory data analysis using r. CRC Press/Taylor & Francis Group.\n\n\nWickham, H., √áetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: import, tidy, transform, visualize, and model data (2nd edition). O‚ÄôReilly."
  },
  {
    "objectID": "slides/slides-09.html#seminarplan",
    "href": "slides/slides-09.html#seminarplan",
    "title": "üî® Topic Modeling",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\nSession\nDatum\nTopic\nPresenter\n\n\n\n\n\nüìÇ Block 1\nIntroduction\n\n\n\n1\n23.10.2024\nKick-Off\nChristoph Adrian\n\n\n2\n30.10.2024\nDBD: Overview & Introduction\nChristoph Adrian\n\n\n3\n06.11.2024\nüî® Introduction to working with R\nChristoph Adrian\n\n\n\nüìÇ Block 2\nTheoretical Background: Twitch & TV Election Debates\n\n\n\n4\n13.11.2024\nüìö Twitch-Nutzung im Fokus\nStudent groups\n\n\n5\n20.11.2024\nüìö (Wirkungs-)Effekte von Twitch & TV-Debatten\nStudent groups\n\n\n6\n27.11.2024\nüìö Politische Debatten & Social Media\nStudent groups\n\n\n\nüìÇ Block 3\nMethod: Natural Language Processing\n\n\n\n7\n04.12.2024\nüî® Text as data I: Introduction\nChristoph Adrian\n\n\n8\n11.12.2024\nüî® Text as data II: Advanced Methods\nChristoph Adrian\n\n\n9\n18.12.2024\nüî® Advanced Method I: Topic Modeling\nChristoph Adrian\n\n\n\nNo lecture\nüéÑChristmas Break\n\n\n\n10\n08.01.2025\nüî® Advanced Method II: Machine Learning\nChristoph Adrian\n\n\n\nüìÇ Block 4\nProject Work\n\n\n\n11\n15.01.2025\nüî® Project work\nStudent groups\n\n\n12\n22.01.2025\nüî® Project work\nStudent groups\n\n\n13\n29.01.2025\nüìä Project Presentation I\nStudent groups (TBD)\n\n\n14\n05.02.2025\nüìä Project Presentation & üèÅ Evaluation\nStudentds (TBD) & Christoph Adrian"
  },
  {
    "objectID": "slides/slides-09.html#eure-meinung-ist-gefragt",
    "href": "slides/slides-09.html#eure-meinung-ist-gefragt",
    "title": "üî® Topic Modeling",
    "section": "Eure Meinung ist gefragt!",
    "text": "Eure Meinung ist gefragt!\nBitte nehmt an der kurzen Evaluation teil\n\n\n\n\nBitte nehmen Sie √ºber den QR Code oder folgenden Link an der Evaluation teil:\n\nhttps://eva.fau.de/\nLosung: QNALW\n\n\n\n\n\n\n\n    \n\n\n\n‚àí+\n05:00"
  },
  {
    "objectID": "slides/slides-09.html#quick-reminder-preview",
    "href": "slides/slides-09.html#quick-reminder-preview",
    "title": "üî® Topic Modeling",
    "section": "Quick reminder & preview",
    "text": "Quick reminder & preview\nRekapitulation der letzten Sitzung\n\nTopic Modeling ist ein Verfahren des un√ºberwachten maschinellen Lernens, das sich zur Exploration und Deskription gro√üer Textmengen eignet um\nunbekannte, latente Themen auf Basis von h√§ufig gemeinsam auftretenden (Clustern an) W√∂rtern in Dokumenten zu identifizieren\n\nHeutiger Fokus: Umsetzung zentraler Schritte\n\nPreprocessing\nModell-Einstellung\nAnalyse & Interpretation\nValdierung"
  },
  {
    "objectID": "slides/slides-09.html#welche-preprocessing-schritte-sind-notwendig",
    "href": "slides/slides-09.html#welche-preprocessing-schritte-sind-notwendig",
    "title": "üî® Topic Modeling",
    "section": "Welche Preprocessing-Schritte sind notwendig?",
    "text": "Welche Preprocessing-Schritte sind notwendig?\nUmsetzung zentraler Schritte: 1.Preprocessing\n\nVerschiedene Verfahren m√∂glich bzw. empfohlen (z.B. Denny & Spirling, 2018; Maier et al., 2020)\nVerwendung der empfohlenen Schritte nach Maier et al. (2018):\n\n‚úÖ Deduplication;\n‚úÖ Tokenization;\n‚úÖ Transform all characters to lowercase;\nüèóÔ∏è Remove punctuation & special characters;\n‚ö†Ô∏è Create/remove custom Ngrams/stopwords;\n‚úÖ Term unification (lemmatization)\nüèóÔ∏è Relative Pruning"
  },
  {
    "objectID": "slides/slides-09.html#von-spacyr-zu-tokens",
    "href": "slides/slides-09.html#von-spacyr-zu-tokens",
    "title": "üî® Topic Modeling",
    "section": "Von spacyr zu Tokens",
    "text": "Von spacyr zu Tokens\nUmsetzung zentraler Schritte: 1.Preprocessing\n\n\n# spacyr-Korpus zu Tokens\nchat_spacyr_toks &lt;- chats_spacyr %&gt;% \n  as.tokens(\n    use_lemma = TRUE\n  ) %&gt;% \n  tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = FALSE,\n    remove_url = FALSE, \n    split_hyphens = FALSE,\n    split_tags = FALSE,\n  ) %&gt;% \n  tokens_remove(\n    pattern = stopwords(\"en\")\n  ) %&gt;% \n  tokens_ngrams(n = 1:3) \n\n# Output\nchat_spacyr_toks\n\n\nTokens consisting of 913,245 documents.\ndc03b89a-722d-4eaa-a895-736533a68aca :\n [1] \"60fps\"        \"LETSGO\"       \"60fps\"        \"letsgo\"       \"60fps\"       \n [6] \"letsgo\"       \"60fps\"        \"letsgo\"       \"60fps_LETSGO\" \"LETSGO_60fps\"\n[11] \"60fps_letsgo\" \"letsgo_60fps\"\n[ ... and 9 more ]\n\n6be50e12-2fd5-436f-b253-b2358b618380 :\n[1] \"captain\"           \"o\"                 \"captain\"          \n[4] \"captain_o\"         \"o_captain\"         \"captain_o_captain\"\n\nf5e41904-7f01-4f03-ad6c-2c0f07d70ed0 :\n[1] \"wokege\"            \"right\"             \"time\"             \n[4] \"wokege_right\"      \"right_time\"        \"wokege_right_time\"\n\n92dc6519-eb54-4c18-abef-27201314b22f :\n[1] \"GECKW\"              \"BITCH\"              \"GECKIN\"            \n[4] \"GECKW\"              \"GECKW_BITCH\"        \"BITCH_GECKIN\"      \n[7] \"GECKIN_GECKW\"       \"GECKW_BITCH_GECKIN\" \"BITCH_GECKIN_GECKW\"\n\n92055088-7067-48c0-aa11-9c6103bdf4c4 :\n [1] \"YOUCANT\"            \"bring\"              \"back\"              \n [4] \"30FPS\"              \"Cinema\"             \"YOUCANT_bring\"     \n [7] \"bring_back\"         \"back_30FPS\"         \"30FPS_Cinema\"      \n[10] \"YOUCANT_bring_back\" \"bring_back_30FPS\"   \"back_30FPS_Cinema\" \n\n03ad4706-aa67-4ddc-a1e4-6f8ca981778e :\n[1] \"time\"        \"Wokege\"      \"time_Wokege\"\n\n[ reached max_ndoc ... 913,239 more documents ]"
  },
  {
    "objectID": "slides/slides-09.html#wenn-die-bereinigung-zu-gut-funktioniert",
    "href": "slides/slides-09.html#wenn-die-bereinigung-zu-gut-funktioniert",
    "title": "üî® Topic Modeling",
    "section": "Wenn die Bereinigung zu gut funktioniert ‚Ä¶",
    "text": "Wenn die Bereinigung zu gut funktioniert ‚Ä¶\n1.Preprocessing: Herausforderungen durch leere Nachrichten\n\nAnalysen des stm Topic Models nutzen Bez√ºge auf die Stammdaten ‚ûú F√§lle von Modell und Stammdaten m√ºssen √ºbereinstimmen\nProbleme:\n\nDurch Tokenisierung & Pruning k√∂nnen ‚Äúleere‚Äù Chatnachrichten entstehen\nDiese leeren Nachrichten werden bei Sch√§tzung nicht ber√ºcksichtigt\n\nL√∂sung:\n\n(Mehrfache) Identifikation & Ausschluss von leeren Nachrichten"
  },
  {
    "objectID": "slides/slides-09.html#pr√ºfen-erweitern-filtern",
    "href": "slides/slides-09.html#pr√ºfen-erweitern-filtern",
    "title": "üî® Topic Modeling",
    "section": "Pr√ºfen ‚ûú Erweitern ‚ûú Filtern",
    "text": "Pr√ºfen ‚ûú Erweitern ‚ûú Filtern\n1.Preprocessing: Herausforderungen bei der Tokenisierung\n\n\n# Get document names\noriginal_docnames &lt;- chats$message_id\ntoken_docnames &lt;- docnames(chat_spacyr_toks)\n\n# Identify & exclude missing documents\nmissing_docs &lt;- setdiff(\n    original_docnames,\n    token_docnames)\nchats_filtered &lt;- chats %&gt;% \n  filter(!message_id %in% missing_docs)\n\n# Add docvars\ndocvars(chat_spacyr_toks) &lt;- chats_filtered\n\n# Subset tokens based on docvars\nmajority_report_chat_toks &lt;- tokens_subset(\n  chat_spacyr_toks,\n  streamer == \"the_majority_report\")\n\n# Output\nmajority_report_chat_toks\n\n\nTokens consisting of 24,708 documents and 33 docvars.\nChwKGkNJR2poT3pVdVlnREZha1FyUVlkblNrWS1B :\n[1] \"Donnie\"           \"say\"              \"sperm\"            \"Donnie_say\"      \n[5] \"say_sperm\"        \"Donnie_say_sperm\"\n\nChwKGkNLbXd3LXpVdVlnREZWd0wxZ0FkYW9FSWdB :\n [1] \"wait\"             \"drag\"             \"queen\"            \"Susan\"           \n [5] \"chat\"             \"wait_drag\"        \"drag_queen\"       \"queen_Susan\"     \n [9] \"Susan_chat\"       \"wait_drag_queen\"  \"drag_queen_Susan\" \"queen_Susan_chat\"\n\nChwKGkNKR1RsdV9VdVlnREZkNFhyUVlkZ2d3Tk5n :\n [1] \"person\"                          \"turqouise\"                      \n [3] \"waving::planet\"                  \"orange\"                         \n [5] \"purple\"                          \"ring\"                           \n [7] \"person_turqouise\"                \"turqouise_waving::planet\"       \n [9] \"waving::planet_orange\"           \"orange_purple\"                  \n[11] \"purple_ring\"                     \"person_turqouise_waving::planet\"\n[ ... and 3 more ]\n\nChwKGkNOQ3kxUExVdVlnREZVS1k1UWNkQ0t3Mlhn :\n[1] \"re\"           \"need\"         \"link\"         \"re_need\"      \"need_link\"   \n[6] \"re_need_link\"\n\nChwKGkNPcW5fZkxVdVlnREZlSFJsQWtkbThZaUtR :\n[1] \"praise\"     \"god\"        \"praise_god\"\n\nChwKGkNNUHZzdlhVdVlnREZha1FyUVlkblNrWS1B :\n[1] \"STREAM\"          \"start\"           \"15\"              \"STREAM_start\"   \n[5] \"start_15\"        \"STREAM_start_15\"\n\n[ reached max_ndoc ... 24,702 more documents ]"
  },
  {
    "objectID": "slides/slides-09.html#transformation-in-eine-dfm",
    "href": "slides/slides-09.html#transformation-in-eine-dfm",
    "title": "üî® Topic Modeling",
    "section": "Transformation in eine DFM",
    "text": "Transformation in eine DFM\nUmsetzung zentraler Schritte: 1.Preprocessing\n\n# Convert to DFM\nmajority_report_chat_dfm &lt;- majority_report_chat_toks %&gt;% \n  dfm()\n\n# Output\nmajority_report_chat_dfm %&gt;%\n    print(max_nfeat = 4)\n\nDocument-feature matrix of: 24,708 documents, 84,931 features (&gt;99.99% sparse) and 33 docvars.\n                                          features\ndocs                                       donnie say sperm donnie_say\n  ChwKGkNJR2poT3pVdVlnREZha1FyUVlkblNrWS1B      1   1     1          1\n  ChwKGkNLbXd3LXpVdVlnREZWd0wxZ0FkYW9FSWdB      0   0     0          0\n  ChwKGkNKR1RsdV9VdVlnREZkNFhyUVlkZ2d3Tk5n      0   0     0          0\n  ChwKGkNOQ3kxUExVdVlnREZVS1k1UWNkQ0t3Mlhn      0   0     0          0\n  ChwKGkNPcW5fZkxVdVlnREZlSFJsQWtkbThZaUtR      0   0     0          0\n  ChwKGkNNUHZzdlhVdVlnREZha1FyUVlkblNrWS1B      0   0     0          0\n[ reached max_ndoc ... 24,702 more documents, reached max_nfeat ... 84,927 more features ]"
  },
  {
    "objectID": "slides/slides-09.html#pruning-der-dfm",
    "href": "slides/slides-09.html#pruning-der-dfm",
    "title": "üî® Topic Modeling",
    "section": "Pruning der DFM",
    "text": "Pruning der DFM\nUmsetzung zentraler Schritte: 1.Preprocessing\n\n# Pruning\nmajority_report_chat_trim &lt;- majority_report_chat_dfm %&gt;% \n    dfm_trim(\n        min_docfreq = 50/nrow(chats),\n        max_docfreq = 0.99, \n        docfreq_type = \"prop\"\n   )\n\n# Output\nmajority_report_chat_trim %&gt;% \n    print(max_nfeat = 4)\n\nDocument-feature matrix of: 24,708 documents, 11,035 features (99.97% sparse) and 33 docvars.\n                                          features\ndocs                                       donnie say sperm wait\n  ChwKGkNJR2poT3pVdVlnREZha1FyUVlkblNrWS1B      1   1     1    0\n  ChwKGkNLbXd3LXpVdVlnREZWd0wxZ0FkYW9FSWdB      0   0     0    1\n  ChwKGkNKR1RsdV9VdVlnREZkNFhyUVlkZ2d3Tk5n      0   0     0    0\n  ChwKGkNOQ3kxUExVdVlnREZVS1k1UWNkQ0t3Mlhn      0   0     0    0\n  ChwKGkNPcW5fZkxVdVlnREZlSFJsQWtkbThZaUtR      0   0     0    0\n  ChwKGkNNUHZzdlhVdVlnREZha1FyUVlkblNrWS1B      0   0     0    0\n[ reached max_ndoc ... 24,702 more documents, reached max_nfeat ... 11,031 more features ]"
  },
  {
    "objectID": "slides/slides-09.html#konvertierung-f√ºr-stm-topic-modeling",
    "href": "slides/slides-09.html#konvertierung-f√ºr-stm-topic-modeling",
    "title": "üî® Topic Modeling",
    "section": "Konvertierung f√ºr stm Topic Modeling",
    "text": "Konvertierung f√ºr stm Topic Modeling\nUmsetzung zentraler Schritte: 1.Preprocessing\n\n# Convert for stm topic modeling\nmajority_report_chat_stm &lt;- majority_report_chat_trim %&gt;% \n   convert(to = \"stm\")\n\nWarning in dfm2stm(x, docvars, omit_empty = TRUE): Dropped 24,708 empty\ndocument(s)\n\n# Output\nmajority_report_chat_stm %&gt;% summary()\n\n          Length Class      Mode     \ndocuments 23060  -none-     list     \nvocab     11035  -none-     character\nmeta         33  data.frame list"
  },
  {
    "objectID": "slides/slides-09.html#entscheidungen-√ºber-entscheidungen",
    "href": "slides/slides-09.html#entscheidungen-√ºber-entscheidungen",
    "title": "üî® Topic Modeling",
    "section": "Entscheidungen √ºber Entscheidungen",
    "text": "Entscheidungen √ºber Entscheidungen\nUmsetzung zentraler Schritte: 2.Modell-Einstellung\n\nWelches Verfahren bzw. welchen Algorithmus w√§hlen?\n\nMatrixfactorisierung (LSA, NMF)\nProbabilistische Modelle (LDA, CTM, STM)\nDeep Learning (BERT, GPT-2)\n\nWelche Parameter bzw. Hyperparameter sind wie zu ber√ºcksichtigen?\n\nAnzahl der Iterationen\nSeed f√ºr Reproduzierbarkeit\nInitialisierungsmethode\n\nWie viele Themen (K) sollen identifiziert werden?"
  },
  {
    "objectID": "slides/slides-09.html#die-suche-nach-der-optimalen-anzahl-von-themen",
    "href": "slides/slides-09.html#die-suche-nach-der-optimalen-anzahl-von-themen",
    "title": "üî® Topic Modeling",
    "section": "Die Suche nach der optimalen Anzahl von Themen",
    "text": "Die Suche nach der optimalen Anzahl von Themen\nUmsetzung zentraler Schritte: 2.Modell-Einstellung\n\nWahl von K (ob das Modell angewiesen wird, 5, 15 oder 100 Themen zu identifizieren) hat erheblichen Einfluss auf die Ergebnisse:\n\nje kleiner K, desto breiter und allgemeiner sind die Themen\nje gr√∂√üer K, desto feink√∂rniger und spezifischer, aber auch √ºberlappender und weniger exklusiv sind\n\nkeine allgemeing√ºltige L√∂sung f√ºr die Bestimmung, da abh√§ngig von vielen Faktoren, z.B.\n\nals was Themen im Kontext der Analyse theoretisch definiert sind\ndie Beschaffenheit des Korpus"
  },
  {
    "objectID": "slides/slides-09.html#how-to-find-k",
    "href": "slides/slides-09.html#how-to-find-k",
    "title": "üî® Topic Modeling",
    "section": "How to find K",
    "text": "How to find K\n2.Modell-Einstellung: Suche nach dem Modell mit dem optimalen K\n\nDas stm-Paket (v1.3.7, Roberts et al., 2019) bietet zwei integrierte L√∂sungen, um das optimale K zu finden:\n\nsearchK() Funktion\nVerwendung des Argumentes K = 0 bei der Sch√§tzung des Modells\nEmpfehlung: (Manuelles) Training und Bewertung!\n\nEntscheidung basiert u.a. auf:\n\nStastischem Fit (z.B. Coherence, Perplexity)\nInterpretierbarkeit (z.B. Top Features, Top Documents)\nRank-1-Metrik (z.B. H√§ufigkeit bestimmter Themen)"
  },
  {
    "objectID": "slides/slides-09.html#manuell-trainiert-exploriert",
    "href": "slides/slides-09.html#manuell-trainiert-exploriert",
    "title": "üî® Topic Modeling",
    "section": "Manuell trainiert & exploriert",
    "text": "Manuell trainiert & exploriert\nUmsetzung zentraler Schritte: 2.Modell-Einstellung\n\n\n\n\n# Set up parallel processing using furrr\nfuture::plan(future::multisession()) \n\n# Estimate models\nstm_search  &lt;- tibble(\n    k = seq(from = 4, to = 20, by = 2)\n    ) %&gt;%\n    mutate(\n        mdl = furrr::future_map(\n            k, \n            ~stm::stm(\n                documents = majority_report_chat_stm$documents,\n                vocab = majority_report_chat_stm$vocab, \n                prevalence =~ platform + debate + message_during_debate, \n                K = ., \n                seed = 42,\n                max.em.its = 1000,\n                data = majority_report_chat_stm$meta,\n                init.type = \"Spectral\",\n                verbose = TRUE),\n            .options = furrr::furrr_options(seed = 42)\n            )\n    )\n\n\n\nstm_search$mdl\n\n[[1]]\nA topic model with 4 topics, 23060 documents and a 11035 word dictionary.\n\n[[2]]\nA topic model with 6 topics, 23060 documents and a 11035 word dictionary.\n\n[[3]]\nA topic model with 8 topics, 23060 documents and a 11035 word dictionary.\n\n[[4]]\nA topic model with 10 topics, 23060 documents and a 11035 word dictionary.\n\n[[5]]\nA topic model with 12 topics, 23060 documents and a 11035 word dictionary.\n\n[[6]]\nA topic model with 14 topics, 23060 documents and a 11035 word dictionary.\n\n[[7]]\nA topic model with 16 topics, 23060 documents and a 11035 word dictionary.\n\n[[8]]\nA topic model with 18 topics, 23060 documents and a 11035 word dictionary.\n\n[[9]]\nA topic model with 20 topics, 23060 documents and a 11035 word dictionary."
  },
  {
    "objectID": "slides/slides-09.html#berechnung-der-modell-diagnostik",
    "href": "slides/slides-09.html#berechnung-der-modell-diagnostik",
    "title": "üî® Topic Modeling",
    "section": "Berechnung der Modell-Diagnostik",
    "text": "Berechnung der Modell-Diagnostik\n2.Modell-Einstellung: Suche nach dem Modell mit dem optimalen K\n\n# Create heldout\nheldout &lt;- make.heldout(\n  majority_report_chat_stm$documents,\n  majority_report_chat_stm$vocab,\n  seed = 42)\n\n# Create model diagnostics\nstm_results &lt;- stm_search %&gt;%\n  mutate(\n    exclusivity = map(mdl, exclusivity),\n    semantic_coherence = map(mdl, semanticCoherence, majority_report_chat_stm$documents),\n    eval_heldout = map(mdl, eval.heldout, heldout$missing),\n    residual = map(mdl, checkResiduals, majority_report_chat_stm$documents),\n    bound =  map_dbl(mdl, function(x) max(x$convergence$bound)),\n    lfact = map_dbl(mdl, function(x) lfactorial(x$settings$dim$K)),\n    lbound = bound + lfact,\n    iterations = map_dbl(mdl, function(x) length(x$convergence$bound))\n    )"
  },
  {
    "objectID": "slides/slides-09.html#√ºberblick-√ºber-modell-diagnostik",
    "href": "slides/slides-09.html#√ºberblick-√ºber-modell-diagnostik",
    "title": "üî® Topic Modeling",
    "section": "√úberblick √ºber Modell-Diagnostik",
    "text": "√úberblick √ºber Modell-Diagnostik\n2.Modell-Einstellung: Suche nach dem Modell mit dem optimalen K\n\nstm_results\n\n# A tibble: 9 √ó 10\n      k mdl    exclusivity semantic_coherence eval_heldout residual        bound\n  &lt;dbl&gt; &lt;list&gt; &lt;list&gt;      &lt;list&gt;             &lt;list&gt;       &lt;list&gt;          &lt;dbl&gt;\n1     4 &lt;STM&gt;  &lt;dbl [4]&gt;   &lt;dbl [4]&gt;          &lt;named list&gt; &lt;named list&gt; -745060.\n2     6 &lt;STM&gt;  &lt;dbl [6]&gt;   &lt;dbl [6]&gt;          &lt;named list&gt; &lt;named list&gt; -739258.\n3     8 &lt;STM&gt;  &lt;dbl [8]&gt;   &lt;dbl [8]&gt;          &lt;named list&gt; &lt;named list&gt; -734111.\n4    10 &lt;STM&gt;  &lt;dbl [10]&gt;  &lt;dbl [10]&gt;         &lt;named list&gt; &lt;named list&gt; -729647.\n5    12 &lt;STM&gt;  &lt;dbl [12]&gt;  &lt;dbl [12]&gt;         &lt;named list&gt; &lt;named list&gt; -725092.\n6    14 &lt;STM&gt;  &lt;dbl [14]&gt;  &lt;dbl [14]&gt;         &lt;named list&gt; &lt;named list&gt; -723050.\n7    16 &lt;STM&gt;  &lt;dbl [16]&gt;  &lt;dbl [16]&gt;         &lt;named list&gt; &lt;named list&gt; -722053.\n8    18 &lt;STM&gt;  &lt;dbl [18]&gt;  &lt;dbl [18]&gt;         &lt;named list&gt; &lt;named list&gt; -718276.\n9    20 &lt;STM&gt;  &lt;dbl [20]&gt;  &lt;dbl [20]&gt;         &lt;named list&gt; &lt;named list&gt; -719322.\n# ‚Ñπ 3 more variables: lfact &lt;dbl&gt;, lbound &lt;dbl&gt;, iterations &lt;dbl&gt;"
  },
  {
    "objectID": "slides/slides-09.html#kurzer-crashkurs",
    "href": "slides/slides-09.html#kurzer-crashkurs",
    "title": "üî® Topic Modeling",
    "section": "Kurzer Crashkurs",
    "text": "Kurzer Crashkurs\n√úberblick √ºber die verschiedenen Evaluationskritierien\n\nHeld-Out Likelihood misst, wie gut ein Modell ungesehene Daten vorhersagt (ABER: kein allgemeing√ºltiger Schwellenwert, nur Vergleich identischer Daten). H√∂here Werte weisen auf eine bessere Vorhersageleistung hin.\nLower bound ist eine Ann√§herung an die Log-Likelihood des Modells. Ein h√∂herer Wert deutet auf eine bessere Anpassung an die Daten hin.\nResiduen geben die Differenz zwischen den beobachteten und den vorhergesagten Werten an. Kleinere Residuen deuten auf eine bessere Modellanpassung hin. Im Idealfall sollten die Residuen so klein wie m√∂glich sein.\nSemantische Koh√§renz misst, wie semantisch verwandt die wichtigsten W√∂rter eines Themas sind, wobei h√∂here Werte auf koh√§rentere Themen hinweisen."
  },
  {
    "objectID": "slides/slides-09.html#vergleich-des-statistischen-fits",
    "href": "slides/slides-09.html#vergleich-des-statistischen-fits",
    "title": "üî® Topic Modeling",
    "section": "Vergleich des statistischen Fits",
    "text": "Vergleich des statistischen Fits\n2.Modell-Einstellung: Suche nach dem Modell mit dem optimalen K\n\n\nExpand for full code\n# Visualize\nstm_results %&gt;%\n  transmute(\n    k,\n    `Lower bound` = lbound,\n    Residuals = map_dbl(residual, \"dispersion\"),\n    `Semantic coherence` = map_dbl(semantic_coherence, mean),\n    `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\")) %&gt;%\n  gather(Metric, Value, -k) %&gt;%\n  ggplot(aes(k, Value, color = Metric)) +\n    geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +\n    geom_point(size = 3) +\n    scale_x_continuous(breaks = seq(from = 4, to = 20, by = 2)) +\n    facet_wrap(~Metric, scales = \"free_y\") +\n    labs(x = \"K (Anzahl der Themen)\",\n         y = NULL,\n         title = \"Statistischer Fit der STM-Modelle\",\n         subtitle = \"Koh√§renz sollte hoch, Residuen niedrig sein\"\n    ) +\n    theme_pubr()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead."
  },
  {
    "objectID": "slides/slides-09.html#hohe-koh√§renz-bei-hoher-exklusivit√§t",
    "href": "slides/slides-09.html#hohe-koh√§renz-bei-hoher-exklusivit√§t",
    "title": "üî® Topic Modeling",
    "section": "Hohe Koh√§renz bei hoher Exklusivit√§t",
    "text": "Hohe Koh√§renz bei hoher Exklusivit√§t\n2.Modell-Einstellung: Suche nach dem Modell mit dem optimalen K\n\n\nExpand for full code\n# Models for comparison\nmodels_for_comparison = c(12, 14, 18)\n\n# Create figures\nfig_excl &lt;- stm_results %&gt;% \n  # Edit data\n  select(k, exclusivity, semantic_coherence) %&gt;%\n  filter(k %in% models_for_comparison) %&gt;%\n  unnest(cols = c(exclusivity, semantic_coherence))  %&gt;%\n  mutate(k = as.factor(k)) %&gt;%\n  # Build graph\n  ggplot(aes(semantic_coherence, exclusivity, color = k)) +\n    geom_point(size = 2, alpha = 0.7) +\n    labs(\n      x = \"Semantic coherence\",\n      y = \"Exclusivity\"\n      # title = \"Comparing exclusivity and semantic coherence\",\n      # subtitle = \"Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity\"\n      ) +\n      theme_pubr()  \n\n# Create plotly\nfig_excl %&gt;% plotly::ggplotly()"
  },
  {
    "objectID": "slides/slides-09.html#extraktion-der-beta--gamma-matrix",
    "href": "slides/slides-09.html#extraktion-der-beta--gamma-matrix",
    "title": "üî® Topic Modeling",
    "section": "Extraktion der Beta- & Gamma-Matrix",
    "text": "Extraktion der Beta- & Gamma-Matrix\n2.Modell-Einstellung: Interpretierbarkeit der Top Features\n\n# Define model\ntpm_k14 &lt;- stm_results %&gt;% \n   filter(k == 14) |&gt; \n   pull(mdl) %&gt;% .[[1]]\n\n\n\n\n\ntpm_k14 %&gt;% \n  tidy(., matrix = \"frex\") \n\n# A tibble: 154,490 √ó 2\n   topic term                \n   &lt;int&gt; &lt;chr&gt;               \n 1     1 look_like           \n 2     1 look_like_go        \n 3     1 like_go             \n 4     1 hahahahahahaha      \n 5     1 look_like_s         \n 6     1 think_go            \n 7     1 check_moderator     \n 8     1 fact_check_moderator\n 9     1 moderator_fact      \n10     1 moderator_fact_check\n# ‚Ñπ 154,480 more rows\n\n\n\n\ntpm_k14 %&gt;% \n  tidy(.,matrix = \"gamma\", \n    document_names = names(majority_report_chat_stm$documents)\n    ) \n\n# A tibble: 322,840 √ó 3\n   document                                 topic   gamma\n   &lt;chr&gt;                                    &lt;int&gt;   &lt;dbl&gt;\n 1 ChwKGkNJR2poT3pVdVlnREZha1FyUVlkblNrWS1B     1 0.0261 \n 2 ChwKGkNLbXd3LXpVdVlnREZWd0wxZ0FkYW9FSWdB     1 0.0265 \n 3 ChwKGkNKR1RsdV9VdVlnREZkNFhyUVlkZ2d3Tk5n     1 0.0123 \n 4 ChwKGkNOQ3kxUExVdVlnREZVS1k1UWNkQ0t3Mlhn     1 0.0200 \n 5 ChwKGkNPcW5fZkxVdVlnREZlSFJsQWtkbThZaUtR     1 0.0232 \n 6 ChwKGkNNUHZzdlhVdVlnREZha1FyUVlkblNrWS1B     1 0.0236 \n 7 ChwKGkNLT1JuX2pVdVlnREZZX0FsQWtkcEw4Wmd3     1 0.434  \n 8 ChwKGkNLRElvZmpVdVlnREZaWExGZ2tkTy1ZSXVR     1 0.0118 \n 9 ChwKGkNNblNqZm5VdVlnREZhX0l3Z1FkZUg0bHZn     1 0.0356 \n10 ChwKGkNMeUkyUHZVdVlnREZXQUhyUVlkTUJvZ193     1 0.00307\n# ‚Ñπ 322,830 more rows"
  },
  {
    "objectID": "slides/slides-09.html#extraktion-der-top-features-nach-thema",
    "href": "slides/slides-09.html#extraktion-der-top-features-nach-thema",
    "title": "üî® Topic Modeling",
    "section": "Extraktion der Top Features nach Thema",
    "text": "Extraktion der Top Features nach Thema\n2.Modell-Einstellung: Interpretierbarkeit der Top Features\n\n\n\n# Create gamma data\ntop_gamma_k14 &lt;- tpm_k14 %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::summarise(\n    gamma = mean(gamma),\n    .groups = \"drop\") %&gt;%\n  dplyr::arrange(desc(gamma))\n\n# Create beta data\ntop_beta_k14 &lt;- tpm_k14 %&gt;%\n  tidytext::tidy(.) %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::arrange(-beta) %&gt;%\n  dplyr::top_n(7, wt = beta) %&gt;% \n  dplyr::select(topic, term) %&gt;%\n  dplyr::summarise(\n    terms_beta = toString(term),\n    .groups = \"drop\")\n\n\n\n# Merge gamma & beta data\ntop_topics_terms_k14 &lt;- top_beta_k14 %&gt;% \n  dplyr::left_join(\n    top_gamma_k14, \n    by = \"topic\") %&gt;%\n  dplyr::mutate(\n          topic = paste0(\"Topic \", topic),\n          topic = reorder(topic, gamma)\n      )"
  },
  {
    "objectID": "slides/slides-09.html#beschreiben-top-features-ein-topic-sinnvoll",
    "href": "slides/slides-09.html#beschreiben-top-features-ein-topic-sinnvoll",
    "title": "üî® Topic Modeling",
    "section": "Beschreiben Top Features ein Topic sinnvoll?",
    "text": "Beschreiben Top Features ein Topic sinnvoll?\n2.Modell-Einstellung: Interpretierbarkeit der Top Features\n\n\ntop_topics_terms_k14 %&gt;%\n  mutate(across(gamma, ~round(.,3))) %&gt;% \n  dplyr::arrange(-gamma) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(\n    table.width = gt::pct(90), \n    table.font.size = \"12px\"\n    )\n\n\n\n\n\n\n\n\ntopic\nterms_beta\ngamma\n\n\n\n\nTopic 8\nmake, 's, lul, emma, fuchsia, liar, kekl\n0.115\n\n\nTopic 7\ngood, right, now, yes, plan, lie, bad\n0.113\n\n\nTopic 12\nkamala, want, biden, eat, take, vote, god\n0.109\n\n\nTopic 5\nget, s, wow, mad, omg, thank, nice\n0.101\n\n\nTopic 4\nlmao, omegalul, red, green, orange, baby, kekw\n0.079\n\n\nTopic 3\ntime, sam, love, man, need, old, big\n0.075\n\n\nTopic 11\nsay, oh, ..., know, look, shit, yeah\n0.075\n\n\nTopic 1\ngo, like, fact, debate, look, check, keep\n0.058\n\n\nTopic 13\ntrump, just, donald, lose, racist, win, can\n0.056\n\n\nTopic 9\nlol, one, give, ...., wtf, china, okay\n0.051\n\n\nTopic 6\npeople, think, go, back, work, try, change\n0.050\n\n\nTopic 10\nlet, talk, ‚Äôs, can, like, sound, see\n0.045\n\n\nTopic 2\nstop, start, please, israel, use, laugh, agree\n0.039\n\n\nTopic 14\nface, guy, don, bring, real, country, rolling_on_the_floor_laughe\n0.034"
  },
  {
    "objectID": "slides/slides-09.html#extraktion-zusammenf√ºhrung-der-daten",
    "href": "slides/slides-09.html#extraktion-zusammenf√ºhrung-der-daten",
    "title": "üî® Topic Modeling",
    "section": "Extraktion & Zusammenf√ºhrung der Daten",
    "text": "Extraktion & Zusammenf√ºhrung der Daten\n2.Modell-Einstellung: Interpretierbarkeit der Top Documents\n\n\n\n# Prepare for merging\ntopic_gammas_k14 &lt;- tpm_k14 %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(document) %&gt;% \n  tidyr::pivot_wider(\n    id_cols = document, \n    names_from = \"topic\", \n    names_prefix = \"gamma_topic_\",\n    values_from = \"gamma\")\n      \ngammas_k14 &lt;- tpm_k14 %&gt;%\n  tidytext::tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(document) %&gt;% \n  dplyr::slice_max(gamma) %&gt;% \n  dplyr::mutate(\n    main_topic = ifelse(\n      gamma &gt; 0.5, topic, NA)) %&gt;% \n  rename(\n    top_topic = topic,\n    top_gamma = gamma) %&gt;% \n  ungroup() %&gt;% \n  left_join(.,\n    topic_gammas_k14,\n    by = join_by(document))\n\n\n\n# Identify empty documents\nempty_docs &lt;- Matrix::rowSums(\n  as(majority_report_chat_trim, \"Matrix\")) == 0 \nempty_docs_ids &lt;- majority_report_chat_trim@docvars$docname[empty_docs]\n\n# Merge with original data\nchats_topics &lt;- chats_filtered %&gt;%\n  filter(!(message_id %in% empty_docs_ids)) %&gt;% \n  filter(streamer == \"the_majority_report\") %&gt;%   \n  bind_cols(gammas_k14) %&gt;% \n  select(-document)"
  },
  {
    "objectID": "slides/slides-09.html#angereicherter-datensatz",
    "href": "slides/slides-09.html#angereicherter-datensatz",
    "title": "üî® Topic Modeling",
    "section": "Angereicherter Datensatz",
    "text": "Angereicherter Datensatz\n2.Modell-Einstellung: Interpretierbarkeit der Top Documents\n\nchats_topics %&gt;% glimpse\n\nRows: 23,060\nColumns: 50\n$ streamer              &lt;chr&gt; \"the_majority_report\", \"the_majority_report\", \"t‚Ä¶\n$ url                   &lt;chr&gt; \"https://www.youtube.com/watch?v=lzobJil9Sgc\", \"‚Ä¶\n$ platform              &lt;chr&gt; \"youtube\", \"youtube\", \"youtube\", \"youtube\", \"you‚Ä¶\n$ debate                &lt;chr&gt; \"presidential\", \"presidential\", \"presidential\", ‚Ä¶\n$ user_name             &lt;chr&gt; \"Scott Plant\", \"Rebecca W\", \"Galactic News Netwo‚Ä¶\n$ user_id               &lt;chr&gt; \"UC4mxlnk193JrXVAp6K-vEpQ\", \"UCeenHJ1v62biyOyKwL‚Ä¶\n$ user_display_name     &lt;chr&gt; \"Scott Plant\", \"Rebecca W\", \"Galactic News Netwo‚Ä¶\n$ user_badges           &lt;list&gt; [], [], [], [], [], [], [], [], [], [], [], [],‚Ä¶\n$ message_timestamp     &lt;dbl&gt; -152, -151, -145, -138, -137, -132, -126, -126, ‚Ä¶\n$ message_id            &lt;chr&gt; \"ChwKGkNJR2poT3pVdVlnREZha1FyUVlkblNrWS1B\", \"Chw‚Ä¶\n$ message_type          &lt;chr&gt; \"text_message\", \"text_message\", \"text_message\", ‚Ä¶\n$ message_content       &lt;chr&gt; \"Donnie will say, \\\"That is my own sperm.\\\"\", \"w‚Ä¶\n$ message_emotes        &lt;list&gt; [], [], [[\"UCkszU2WH9gy1mb0dV-11UJg/ssIfY7OFG5O‚Ä¶\n$ message_length        &lt;int&gt; 40, 45, 52, 38, 10, 32, 8, 14, 2, 90, 20, 36, 20‚Ä¶\n$ message_timecode      &lt;Period&gt; -2M -32S, -2M -31S, -2M -25S, -2M -18S, -2M -‚Ä¶\n$ message_time          &lt;chr&gt; \"23:57:28\", \"23:57:29\", \"23:57:35\", \"23:57:42\", ‚Ä¶\n$ message_during_debate &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_has_badge        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_premium       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_subscriber    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_turbo         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_moderator     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_partner       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_subgifter     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_broadcaster   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_vip           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_twitchdj      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_founder       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_staff         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_game_dev      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_ambassador    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_no_audio         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_no_video         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ top_topic             &lt;int&gt; 11, 7, 4, 3, 4, 3, 1, 4, 9, 4, 8, 3, 1, 1, 3, 13‚Ä¶\n$ top_gamma             &lt;dbl&gt; 0.4435422, 0.3412468, 0.7627751, 0.5663056, 0.46‚Ä¶\n$ main_topic            &lt;int&gt; NA, NA, 4, 3, NA, NA, NA, 4, NA, 4, NA, NA, NA, ‚Ä¶\n$ gamma_topic_1         &lt;dbl&gt; 0.026098022, 0.026450828, 0.012260893, 0.0200424‚Ä¶\n$ gamma_topic_2         &lt;dbl&gt; 0.014058480, 0.016014175, 0.006519458, 0.1322111‚Ä¶\n$ gamma_topic_3         &lt;dbl&gt; 0.043655546, 0.066001729, 0.018185091, 0.5663056‚Ä¶\n$ gamma_topic_4         &lt;dbl&gt; 0.03876696, 0.14976529, 0.76277514, 0.03011074, ‚Ä¶\n$ gamma_topic_5         &lt;dbl&gt; 0.186801763, 0.043801244, 0.020095565, 0.0373451‚Ä¶\n$ gamma_topic_6         &lt;dbl&gt; 0.021470740, 0.024622665, 0.009041711, 0.0174135‚Ä¶\n$ gamma_topic_7         &lt;dbl&gt; 0.036282513, 0.341246826, 0.017882159, 0.0289225‚Ä¶\n$ gamma_topic_8         &lt;dbl&gt; 0.04538521, 0.14311198, 0.06168206, 0.03558740, ‚Ä¶\n$ gamma_topic_9         &lt;dbl&gt; 0.021910232, 0.023552979, 0.012143933, 0.0165138‚Ä¶\n$ gamma_topic_10        &lt;dbl&gt; 0.020656194, 0.020843309, 0.013562820, 0.0161670‚Ä¶\n$ gamma_topic_11        &lt;dbl&gt; 0.443542243, 0.027314995, 0.019249172, 0.0206369‚Ä¶\n$ gamma_topic_12        &lt;dbl&gt; 0.044397591, 0.044680183, 0.019709728, 0.0343713‚Ä¶\n$ gamma_topic_13        &lt;dbl&gt; 0.027631227, 0.037326873, 0.011473224, 0.0214522‚Ä¶\n$ gamma_topic_14        &lt;dbl&gt; 0.029343282, 0.035266919, 0.015419046, 0.0229199‚Ä¶"
  },
  {
    "objectID": "slides/slides-09.html#top-topic-im-fokus",
    "href": "slides/slides-09.html#top-topic-im-fokus",
    "title": "üî® Topic Modeling",
    "section": "Top Topic im Fokus",
    "text": "Top Topic im Fokus\n2.Modell-Einstellung: Passen Top Document zum Thema?\n\n\nExpand for full code\nchats_topics %&gt;% \n  filter(top_topic == 8) %&gt;% \n  arrange(-top_gamma) %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(message_id, user_name, message_time, message_content, top_gamma, top_topic) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(table.font.size = \"10px\")\n\n\n\n\n\n\n\n\nmessage_id\nuser_name\nmessage_time\nmessage_content\ntop_gamma\ntop_topic\n\n\n\n\nChwKGkNKdlRqY1BwdVlnREZRREV3Z1FkV2I4U1hn\nDavid Davis\n01:29:52\n:watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon:\n0.9628609\n8\n\n\nChwKGkNLMlp3cUxzdVlnREZVN0NsQWtkT0JBRTN3\nJules Winnfeild üè≥Ô∏è‚Äç‚ößÔ∏è\n01:42:09\n:face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape:\n0.9583042\n8\n\n\nChwKGkNNclk0dFBZdVlnREZZYWg1UWNkUlhvNVB3\nCanalEduge\n00:14:24\n:face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape:\n0.9437816\n8\n\n\nChwKGkNJaVdpNVBmdVlnREZTV1Q1UWNkUWg0dEJn\nJules Winnfeild üè≥Ô∏è‚Äç‚ößÔ∏è\n00:43:27\n:face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape:\n0.9315330\n8\n\n\nChwKGkNORG1uTWpvdVlnREZkd3VyUVlkSVFrVU5R\n#BobbleHead\n01:25:34\nWORLDSTAR own's the Trademark on the Algorithm that identified all the Pedophiles = Blame T.M.Z. #ReleaseTheBlackBaby\n0.9313871\n8\n\n\nChwKGkNPV09fYmJwdVlnREZXc3ByUVlkbk9Vc3d3\nDavid Davis\n01:29:26\n:watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon:\n0.9303014\n8\n\n\nChwKGkNMV244NkRvdVlnREZUMFRyUVlkYmZzUmpB\n#BobbleHead\n01:24:12\nWORLDSTAR own's the Trademark on the Algorithm that identified all the Pedophiles = Blame T.M.Z.\n0.9287878\n8\n\n\nChwKGkNLN0N4ZTdldVlnREZWNDZyUVlkRmZJRklR\nCorporations8MyBaby\n00:42:10\n:face-fuchsia-tongue-out::face-fuchsia-tongue-out::face-fuchsia-tongue-out::face-fuchsia-tongue-out:\n0.9269656\n8\n\n\n9c014ab4-89a7-4f9d-97c8-be3da2868f58\nnightbot\n00:10:47\nJoin the official Majority Report discord community! https://discord.gg/majority\n0.9215614\n8\n\n\nfcb53a8b-4b75-4557-b3eb-d273b7069d88\nnightbot\n00:26:14\nJoin the official Majority Report discord community! https://discord.gg/majority\n0.9215614\n8"
  },
  {
    "objectID": "slides/slides-09.html#thema-12-im-fokus",
    "href": "slides/slides-09.html#thema-12-im-fokus",
    "title": "üî® Topic Modeling",
    "section": "Thema 12 im Fokus",
    "text": "Thema 12 im Fokus\n2.Modell-Einstellung: Passen Top Document zum Thema?\n\n\nExpand for full code\nchats_topics %&gt;% \n  filter(top_topic == 12) %&gt;% \n  arrange(-top_gamma) %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(message_id, user_name, message_time, message_content, top_gamma, top_topic) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(table.font.size = \"10px\")\n\n\n\n\n\n\n\n\nmessage_id\nuser_name\nmessage_time\nmessage_content\ntop_gamma\ntop_topic\n\n\n\n\nChwKGkNNZXg5LUxxdVlnREZkcVc1UWNkeGpNTDJB\nSamSedersLeftTeste\n01:35:27\nThe vice president is BLACK BLACK BLACK BLACK BLACK BLACK\n0.9227423\n12\n\n\nChwKGkNNdVU0NTNndVlnREZkNEwxZ0FkbWxFSFN3\nRilly Kewl\n00:48:18\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNJU1BtS0RndVlnREZRREV3Z1FkV2I4U1hn\nRilly Kewl\n00:48:23\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNJS1NvcVRndVlnREZRMHUxZ0FkU1FFSzZB\nRilly Kewl\n00:48:31\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNLSG9uYUxodVlnREZhY0cxZ0FkSVJjSGdB\nRilly Kewl\n00:52:56\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNPbU90YV9odVlnREZWZ3FyUVlkaUpnNUpn\nRilly Kewl\n00:53:23\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNLYmxuZjdzdVlnREZiMHUxZ0FkT0owN0h3\nRilly Kewl\n01:45:21\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNLX3F4cl90dVlnREZWbzAxZ0FkdzVFTTR3\nRilly Kewl\n01:47:38\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNOYm1yTVB0dVlnREZWb0gxZ0FkQnF3QWRR\nRilly Kewl\n01:47:46\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nbad4de96-6c3f-4495-9bd5-da395d9af90b\ngrandshadowfox\n01:07:37\nGrandshadowfox subscribed with Prime. They've subscribed for 16 months! 15 months\n0.9064436\n12"
  },
  {
    "objectID": "slides/slides-09.html#thema-4-im-fokus",
    "href": "slides/slides-09.html#thema-4-im-fokus",
    "title": "üî® Topic Modeling",
    "section": "Thema 4 im Fokus",
    "text": "Thema 4 im Fokus\n2.Modell-Einstellung: Passen Top Document zum Thema?\n\n\nExpand for full code\nchats_topics %&gt;% \n  filter(top_topic == 4) %&gt;% \n  arrange(-top_gamma) %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(message_id, user_name, message_time, message_content, top_gamma, top_topic) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(table.font.size = \"10px\")\n\n\n\n\n\n\n\n\nmessage_id\nuser_name\nmessage_time\nmessage_content\ntop_gamma\ntop_topic\n\n\n\n\nChwKGkNLX3Z3SzdkdVlnREZhb0NyUVlkVER3aVRn\nrhys\n00:35:28\n:text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over:\n0.9687024\n4\n\n\nChwKGkNJN1hsSXJudVlnREZWTGNGZ2tkYnFnYmJB\nJules Winnfeild üè≥Ô∏è‚Äç‚ößÔ∏è\n01:18:56\n:fish-orange-wide-eyes::fish-orange-wide-eyes::fish-orange-wide-eyes::fish-orange-wide-eyes::fish-orange-wide-eyes:\n0.9649579\n4\n\n\nChwKGkNJQ3ZzYTdXdVlnREZSek1GZ2tkMndnZ1Bn\nfish Monger\n00:04:09\nideas:finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one:\n0.9647715\n4\n\n\nChwKGkNNcmVxb0RmdVlnREZhb0NyUVlkVER3aVRn\nrhys\n00:42:48\n:text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over:\n0.9630820\n4\n\n\n32d36382-5eaf-4da6-a2dc-c9683b98162b\nnightbot\n00:01:27\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\n4ffbae78-db39-40e9-bcf8-b5c0965fe2a4\nnightbot\n00:09:42\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\na08570c3-f835-4568-9332-b97bf22ee61b\nnightbot\n02:01:22\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\n46b82320-e59d-486e-a58f-acf35b03fe4a\nnightbot\n02:09:43\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\n191d1514-cc7e-4a65-8c9e-0ce5d28f1a5d\nnightbot\n02:22:30\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\ned759097-6071-4394-b810-5adafd52f652\nnightbot\n02:35:23\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4"
  },
  {
    "objectID": "slides/slides-09.html#flie√üender-√ºbergang-in-die-analyse",
    "href": "slides/slides-09.html#flie√üender-√ºbergang-in-die-analyse",
    "title": "üî® Topic Modeling",
    "section": "Flie√üender √úbergang in die Analyse",
    "text": "Flie√üender √úbergang in die Analyse\nUmsetzung zentraler Schritte: 3.Analyse & Interpretation\nstm erm√∂glicht den Einfluss unabh√§ngiger Variablen zu modellieren, genauer auf:\n\ndie Pr√§valenz von Themen (prevalence-Argument)\nden Inhalt von Themen (content-Argument)\n\nInterpreation:\n\nIdentifikation & Ausschluss von ‚ÄûBackground‚Äú-Topics\nIdentifikation & Labelling von relevanten Topics\nGgf. Gruppierung in √ºbergreifende Kontexte (z.B. ‚Äûpolitische Themen‚Äú)\nNutzung f√ºr deskriptive oder inferenzstatistische Verfahren"
  },
  {
    "objectID": "slides/slides-09.html#user-mit-den-meisten-beitr√§gen-zu-thema-4",
    "href": "slides/slides-09.html#user-mit-den-meisten-beitr√§gen-zu-thema-4",
    "title": "üî® Topic Modeling",
    "section": "User mit den meisten Beitr√§gen zu Thema 4",
    "text": "User mit den meisten Beitr√§gen zu Thema 4\n3.Analyse & Interpretation - Beispiel f√ºr deskriptive Verfahren\n\n\nchats_topics %&gt;% \n  filter(top_topic == 8) %&gt;% \n  count(user_name, sort = TRUE) %&gt;% \n  mutate(\n    prop = round(n/sum(n)*100, 2)) %&gt;% \n  slice_head(n = 10) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() \n\n\n\n\n\n\n\n\nuser_name\nn\nprop\n\n\n\n\nbuuuuuuuuuuuuuuuuuuuuuut\n59\n1.83\n\n\nsauvignoncitizen\n50\n1.55\n\n\nSay What\n49\n1.52\n\n\nJules Winnfeild üè≥Ô∏è‚Äç‚ößÔ∏è\n47\n1.45\n\n\nasiak\n46\n1.42\n\n\nhardradajm\n40\n1.24\n\n\nBob Carmody\n34\n1.05\n\n\nT.R.\n33\n1.02\n\n\nmaj_k1bbles\n31\n0.96\n\n\nogdimwit\n31\n0.96"
  },
  {
    "objectID": "slides/slides-09.html#pr√§valenz-vs.-h√§ufigkeit",
    "href": "slides/slides-09.html#pr√§valenz-vs.-h√§ufigkeit",
    "title": "üî® Topic Modeling",
    "section": "Pr√§valenz vs.¬†H√§ufigkeit",
    "text": "Pr√§valenz vs.¬†H√§ufigkeit\n3.Analyse & Interpretation - Beispiel f√ºr deskriptive Verfahren\n\n\n\n\nExpand for full code\ntop_gamma_k14 %&gt;% \n  ggplot(aes(as.factor(topic), gamma)) +\n  geom_col(fill = \"#F57350\") +\n  labs(\n    x = \"Topic\",\n    y = \"Mean gamma\"\n  ) +\n  coord_flip() +\n  scale_y_reverse() +\n  scale_x_discrete(position = \"top\") +\n  theme_pubr()\n\n\n\n\n\n\n\n\n\n\n\n\nExpand for full code\nchats_topics %&gt;% \n  mutate(across(top_topic, as.factor)) %&gt;% \n  ggplot(aes(top_topic, y = after_stat(prop), group = 1)) +\n  geom_bar(fill = \"#1DA1F2\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    x = \"\", \n    y = \"Relative frequency\"\n  ) +\n  coord_flip() +\n  theme_pubr()"
  },
  {
    "objectID": "slides/slides-09.html#einfluss-von-meta-variablen",
    "href": "slides/slides-09.html#einfluss-von-meta-variablen",
    "title": "üî® Topic Modeling",
    "section": "Einfluss von Meta-Variablen",
    "text": "Einfluss von Meta-Variablen\n3.Analyse & Interpretation - Beispiel f√ºr inferenzstatistische Verfahren\n\neffects &lt;- estimateEffect(\n  formula =~ platform + debate + message_during_debate,\n  stmobj = tpm_k14, \n  metadata = chats_topics)\n\nWarning in estimateEffect(formula = ~platform + debate + message_during_debate, : Covariate matrix is singular.  See the details of ?estimateEffect() for some common causes.\n             Adding a small prior 1e-5 for numerical stability.\n\n\n\n\n\n\nsummary(effects, topics = 12)\n\n\nCall:\nestimateEffect(formula = ~platform + debate + message_during_debate, \n    stmobj = tpm_k14, metadata = chats_topics)\n\n\nTopic 12:\n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)              0.130299  29.076022   0.004  0.99642   \nplatformyoutube         -0.044308  29.075963  -0.002  0.99878   \ndebatevice presidential -0.054949  29.075974  -0.002  0.99849   \nmessage_during_debate    0.012174   0.004523   2.691  0.00712 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nsummary(effects, topics = 8)\n\n\nCall:\nestimateEffect(formula = ~platform + debate + message_during_debate, \n    stmobj = tpm_k14, metadata = chats_topics)\n\n\nTopic 8:\n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)              0.343159  29.348608   0.012    0.991\nplatformyoutube         -0.253151  29.348565  -0.009    0.993\ndebatevice presidential -0.233550  29.348558  -0.008    0.994\nmessage_during_debate    0.001028   0.004541   0.226    0.821"
  },
  {
    "objectID": "slides/slides-09.html#shiny-app-als-hilfe-f√ºr-die-analyse",
    "href": "slides/slides-09.html#shiny-app-als-hilfe-f√ºr-die-analyse",
    "title": "üî® Topic Modeling",
    "section": "Shiny-App als Hilfe f√ºr die Analyse",
    "text": "Shiny-App als Hilfe f√ºr die Analyse\nVisualisierung mit stminsights (v4.1.0, Schwemmer, 2021)"
  },
  {
    "objectID": "slides/slides-09.html#die-4-rs",
    "href": "slides/slides-09.html#die-4-rs",
    "title": "üî® Topic Modeling",
    "section": "Die 4 R`s",
    "text": "Die 4 R`s\nUmsetzung zentraler Schritte: 4.Validierung\n\nReliabilit√§t/Robustheit: Kommen wir mit anderen Instrumenten zu √§hnlichen Ergebnissen? (Roberts et al., 2016; Wilkerson & Casas, 2017)\nReproduzierbarkeit: K√∂nnen wir mit den gleichen Daten & Instrumenten die Ergebnisse reproduzieren? (Chung-hong Chan et al., 2024)\nReplizierbarkeit: Lassen sich unsere Ergebnisse f√ºr andere Daten reproduzieren? (Breuer & Haim, 2024; Long, 2021)\n\n\nReliabilit√§t/Robustheit: Add Graphik aus Hase Reproduzierbarkeit: Open Source Software nutzen, mit z.B. ‚ÄûQuarto‚Äú arbeiten (sequenzielle Reihenfolge der Codeausf√ºhrung garantieren!), Kompendium (Code & Daten in einheitlicher Struktur; Docker), Abh√§ngigkeiten, z.B. von Paket-Versionen, reduzieren Replizierbarkeit: Pr√§registrierung, auf statistische Power achten (Poweranalyse, z. B. mit Simulationen?), selbst exakte/konzeptuelle Replikationen durchf√ºhren"
  },
  {
    "objectID": "slides/slides-09.html#messen-wir-was-wir-messen-wollen",
    "href": "slides/slides-09.html#messen-wir-was-wir-messen-wollen",
    "title": "üî® Topic Modeling",
    "section": "Messen wir, was wir messen wollen",
    "text": "Messen wir, was wir messen wollen\nVerschiedenen M√∂glichkeit der Qualit√§tssicherung\n\nValidierung hilft zu verstehen, wo wir falsch liegen und wie falsch wir liegen.\nQualit√§tssicherung z.B. via (Jana Bernhard et al., 2023; Quinn et al., 2009) ‚Ä¶\n\nTheoretischer (!) Ableitung von Messungen (Chen et al., 2023)\nVergleich mit manueller Codierung (z.B. Chan & S√§ltzer, 2020)\nVergleich mit externen Ereignissen"
  },
  {
    "objectID": "slides/slides-09.html#validieren-validieren-validieren",
    "href": "slides/slides-09.html#validieren-validieren-validieren",
    "title": "üî® Topic Modeling",
    "section": "Validieren, Validieren, Validieren",
    "text": "Validieren, Validieren, Validieren\nKritisiche Anmerkungen zum Topic Modeling\n\nAutomated text analysis methods can substantially reduce the costs and time of analyzing massive collections of political texts. When applied to any one problem, however, the output of the models may be misleading or simply wrong. [‚Ä¶] What should be avoided, then, is the blind use of any method without a validation step. (Grimmer & Stewart, 2013, p. S.5)\n\n\nKlassifikationsmodell klassifiziert alle Dokumente, ein Diktion√§r spuckt f√ºr jedes Dokument ein Ergebnis aus, ein Topic Model findet immer die vorgegebene Anzahl an Themen.\nOb es sich dabei auch um inhaltlich sinnvolle Ergebnisse handelt, kann und muss durch manuelle Validierungen festgestellt werden.\nModerne Verfahren (z.B. BERT) potentiell besser geeignet f√ºr bestimmte Texte."
  },
  {
    "objectID": "slides/slides-09.html#and-now-you",
    "href": "slides/slides-09.html#and-now-you",
    "title": "üî® Topic Modeling",
    "section": "üß™ And now ‚Ä¶ you!",
    "text": "üß™ And now ‚Ä¶ you!\nNext steps\n\nLaden das .zip-Archiv stm_session_09.RData.zip von StudOn herunter und entpacke die Dateien an einen Ort deiner Wahl.\n√ñffnet RStudio.\nF√ºhrt folgenden Code-Chunk aus:\n\n\ninstall.packages(\"stminsights\")\nlibrary(stminsights)\nrun_stminsights()\n\n\nLadet den Datensatz in die App.\nMacht euch mit den verschiedenen Funtionen der App vertraut und versucht, die Ergebnisse aus der Sitzung zu reproduzieren."
  },
  {
    "objectID": "slides/slides-09.html#references",
    "href": "slides/slides-09.html#references",
    "title": "üî® Topic Modeling",
    "section": "References",
    "text": "References\n\n\nBreuer, J., & Haim, M. (2024). Are we replicating yet? Reproduction and replication in communication research. Media and Communication, 12. https://doi.org/10.17645/mac.8382\n\n\nChan, C., & S√§ltzer, M. (2020). Oolong: An r package for validating automated content analysis tools. Journal of Open Source Software, 5(55), 2461. https://doi.org/10.21105/joss.02461\n\n\nChen, Y., Peng, Z., Kim, S.-H., & Choi, C. W. (2023). What We Can Do and Cannot Do with Topic Modeling: A Systematic Review. Communication Methods and Measures, 17(2), 111‚Äì130. https://doi.org/10.1080/19312458.2023.2167965\n\n\nChung-hong Chan, Tim Schatto-Eckrodt, & Johannes Gruber. (2024). What makes computational communication science (ir)reproducible? Computational Communication Research, 6(1), 1. https://doi.org/10.5117/ccr2024.1.5.chan\n\n\nDenny, M. J., & Spirling, A. (2018). Text Preprocessing For Unsupervised Learning: Why It Matters, When It Misleads, And What To Do About It. Political Analysis, 26(2), 168‚Äì189. https://doi.org/10.1017/pan.2017.44\n\n\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267‚Äì297. https://doi.org/10/f458q9\n\n\nJana Bernhard, Martin Teuffenbach, & Hajo G. Boomgaarden. (2023). Topic Model Validation Methods and their Impact on Model Selection and Evaluation. Computational Communication Research, 5(1), 1. https://doi.org/10.5117/ccr2023.1.13.bern\n\n\nLong, J. A. (2021). Improving the replicability and generalizability of inferences in quantitative communication research. Annals of the International Communication Association, 45(3), 207‚Äì220. https://doi.org/10.1080/23808985.2021.1979421\n\n\nMaier, D., Niekler, A., Wiedemann, G., & Stoltenberg, D. (2020). How Document Sampling and Vocabulary Pruning Affect the Results of Topic Models. Computational Communication Research, 2(2), 139‚Äì152. https://doi.org/10.5117/ccr2020.2.001.maie\n\n\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., H√§ussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93‚Äì118. https://doi.org/10.1080/19312458.2018.1430754\n\n\nQuinn, K. M., Monroe, B. L., Colaresi, M., Crespin, M. H., & Radev, D. R. (2009). How to Analyze Political Attention with Minimal Assumptions and Costs. American Journal of Political Science, 54(1), 209‚Äì228. https://doi.org/10.1111/j.1540-5907.2009.00427.x\n\n\nRoberts, M. E., Stewart, B. M., & Tingley, D. (2016). Navigating the local modes of big data: The case of topic models (pp. 51‚Äì97). Cambridge University Press. https://doi.org/10.1017/cbo9781316257340.004\n\n\nRoberts, M. E., Stewart, B. M., & Tingley, D. (2019). stm: An R Package for Structural Topic Models. Journal of Statistical Software, 91(1), 1‚Äì40. https://doi.org/10.18637/jss.v091.i02\n\n\nSchwemmer, C. (2021). Stminsights: A shiny application for inspecting structural topic models. https://github.com/cschwem2er/stminsights\n\n\nWilkerson, J., & Casas, A. (2017). Large-Scale Computerized Text Analysis in Political Science: Opportunities and Challenges. Annual Review of Political Science, 20(1), 529‚Äì544. https://doi.org/10.1146/annurev-polisci-052615-025542"
  },
  {
    "objectID": "sessions/session-02.html",
    "href": "sessions/session-02.html",
    "title": "Session 2",
    "section": "",
    "text": "‚úçÔ∏è Start working on R-Video-Tutorials.\n‚úçÔ∏è Book your time slot for the mandatory feedback session on StudOn.\n‚úçÔ∏è Remeber the registration deadline (28./29.10) for the seminar examination on campo."
  },
  {
    "objectID": "sessions/session-02.html#prepare",
    "href": "sessions/session-02.html#prepare",
    "title": "Session 2",
    "section": "",
    "text": "‚úçÔ∏è Start working on R-Video-Tutorials.\n‚úçÔ∏è Book your time slot for the mandatory feedback session on StudOn.\n‚úçÔ∏è Remeber the registration deadline (28./29.10) for the seminar examination on campo."
  },
  {
    "objectID": "sessions/session-02.html#participate",
    "href": "sessions/session-02.html#participate",
    "title": "Session 2",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Session 02"
  },
  {
    "objectID": "sessions/session-02.html#suggested-readings",
    "href": "sessions/session-02.html#suggested-readings",
    "title": "Session 2",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nDavidson, B. I., Wischerath, D., Racek, D., Parry, D. A., Godwin, E., Hinds, J., Van Der Linden, D., Roscoe, J. F., Ayravainen, L. E. M., & Cork, A. (2023). Platform-controlled social media APIs threaten open science. https://osf.io/ps32z\nDriel, I. I. van, Giachanou, A., Pouwels, J. L., Boeschoten, L., Beyens, I., & Valkenburg, P. M. (2022). Promises and Pitfalls of Social Media Data Donations. Communication Methods and Measures, 1‚Äì17. https://doi.org/10.1080/19312458.2022.2109608\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Digital trace data (1st ed., pp. 100‚Äì118). Routledge. https://www.taylorfrancis.com/books/9781003024583/chapters/10.4324/9781003024583-8\nOlteanu, A., Castillo, C., Diaz, F., & Kƒ±cƒ±man, E. (2019). Social data: Biases, methodological pitfalls, and ethical boundaries. Frontiers in Big Data, 2, 13. https://doi.org/10.3389/fdata.2019.00013\nReeves, B., Ram, N., Robinson, T. N., Cummings, J. J., Giles, C. L., Pan, J., Chiatti, A., Cho, M., Roehrick, K., Yang, X., Gagneja, A., Brinberg, M., Muise, D., Lu, Y., Luo, M., Fitzgerald, A., & Yeykelis, L. (2021). Screenomics : A Framework to Capture and Analyze Personal Life Experiences and the Ways that Technology Shapes Them. HumanComputer Interaction, 36(2), 150‚Äì201. https://doi.org/10.1080/07370024.2019.1578652"
  },
  {
    "objectID": "sessions/session-02.html#useful-tools-resources",
    "href": "sessions/session-02.html#useful-tools-resources",
    "title": "Session 2",
    "section": "Useful tools & resources",
    "text": "Useful tools & resources\n\nPeeters, S., & Hagen, S. (2022). The 4CAT Capture and Analysis Toolkit: A Modular Tool for Transparent and Traceable Social Media Research. Computational Communication Research, 4(2), 571‚Äì589. https://doi.org/10.5117/ccr2022.2.007.hage\nPeeters, S. (2022). Zeeschuimer. Zenodo. https://zenodo.org/record/6826877"
  },
  {
    "objectID": "sessions/session-02.html#section",
    "href": "sessions/session-02.html#section",
    "title": "Session 2",
    "section": "",
    "text": "Back to course schedule ‚èé"
  },
  {
    "objectID": "sessions/session-05.html",
    "href": "sessions/session-05.html",
    "title": "Session 5",
    "section": "",
    "text": "## Literature\n\n\n\n\n\n\nThe following sections list the mandatory articles for the each presenting group. Additional optional articles are provided under Further reading.\n\n\n\n\nüìö Thema 3: (Wirkungs-)Effekte der -Nutzung/Interaktion\n\nXi, D., Xu, W., Tang, L., & Han, B. (2024). The impact of streamer emotions on viewer gifting behavior: Evidence from entertainment live streaming. Internet Research, 34(3), 748‚Äì783. https://doi.org/10.1108/INTR-05-2022-0350\nBr√ºndl, S., Matt, C., Hess, T., & Engert, S. (2023). How synchronous participation affects the willingness to subscribe to social live streaming services: The role of co-interactive behavior on twitch. European Journal of Information Systems, 32(5), 800‚Äì817. https://doi.org/10.1080/0960085X.2022.2062468\nWolff, G. H., & Shen, C. (2022). Audience size, moderator activity, gender, and content diversity: Exploring user participation and financial commitment on Twitch.tv. New Media & Society, 146144482110699. https://doi.org/10.1177/14614448211069996\n\n\nFurther Readings\n\nHan, C., Seering, J., Kumar, D., Hancock, J. T., & Durumeric, Z. (2023). Hate Raids on Twitch: Echoes of the Past, New Modalities, and Implications for Platform Governance. Proceedings of the ACM on Human-Computer Interaction, 7(CSCW1), 1‚Äì28. https://doi.org/10.1145/3579609\nMeisner, C. (2023). Networked Responses to Networked Harassment? Creators‚Äô Coordinated Management of ‚ÄúHate Raids‚Äù on Twitch. Social Media + Society, 9(2), 20563051231179696. https://doi.org/10.1177/20563051231179696\nTomlinson, C. (2024). Community Grievances, personal responsibility, and DIY protection: Frustrations and solution-seeking among marginalized Twitch streamers. Convergence: The International Journal of Research into New Media Technologies, 30(1), 358‚Äì374. https://doi.org/10.1177/13548565231184060\nHou, F., Guan, Z., Li, B., & Chong, A. Y. L. (2020). Factors influencing people‚Äôs continuous watching intention and consumption intention in live streaming. Internet Research, 30(1), 141‚Äì163. https://doi.org/10.1108/INTR-04-2018-0177\nChinchilla, P., & Kim, J. (2024). ‚ÄúLet‚Äôs Chill and Chat‚Äù: Exploring the Effects of Streamers‚Äô Self-Disclosure on Parasocial Interaction via Social Presence. International Journal of HumanComputer Interaction, 1‚Äì11. https://doi.org/10.1080/10447318.2024.2390263\n\n\n\n\nüìö Thema 4: (Wirkungs-)Effekte von TV-Wahldebatten\n\nLe Pennec, C., & Pons, V. (2023). How do campaigns shape vote choice? Multicountry evidence from 62 elections and 56 TV debates*. The Quarterly Journal of Economics, 138(2), 703‚Äì767. https://doi.org/10.1093/qje/qjad002\nWaldvogel, T., K√∂nig, P., Wagschal, U., Becker, B., & Weishaupt, S. (2022). It‚Äôs the emotion, stupid! Emotional responses to televised debates and their impact on voting intention. Open Political Science, 5(1), 13‚Äì28. https://doi.org/10.1515/openps-2022-0146\nJennings, F. J., Bramlett, J. C., McKinney, M. S., & Hardy, M. M. (2020). Tweeting Along Partisan Lines: Identity-Motivated Elaboration and Presidential Debates. Social Media + Society, 6(4), 2056305120965518. https://doi.org/10.1177/2056305120965518\nWarner, B. R., Park, J., Kim, G.-E., McKinney, M. S., & Paul, Wm. B. (2024). Do Presidential Primary Debates Increase Political Polarization? American Behavioral Scientist, 68(1), 80‚Äì96. https://doi.org/10.1177/00027642211026613\n\n\nFurther Readings\n\nBenoit, W. L., Hansen, G. J., & Verser, R. M. (2003). A meta-analysis of the effects of viewing u.s. Presidential debates. Communication Monographs, 70(4), 335‚Äì350. https://doi.org/10.1080/0363775032000179133\nMcKinney, M. S., & Warner, B. R. (2013). Do Presidential Debates Matter? Examining a Decade of Campaign Debate Effects. Argumentation and Advocacy, 49(4), 238‚Äì258. https://doi.org/10.1080/00028533.2013.11821800\n\n\n\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "sessions/session-10.html",
    "href": "sessions/session-10.html",
    "title": "Session 10",
    "section": "",
    "text": "üñ•Ô∏è Session 10"
  },
  {
    "objectID": "sessions/session-10.html#participate",
    "href": "sessions/session-10.html#participate",
    "title": "Session 10",
    "section": "",
    "text": "üñ•Ô∏è Session 10"
  },
  {
    "objectID": "sessions/session-10.html#useful-packages",
    "href": "sessions/session-10.html#useful-packages",
    "title": "Session 10",
    "section": "Useful packages",
    "text": "Useful packages\n\nquanteda üåê | \nBenoit, K., Watanabe, K., Wang, H., Nulty, P., Obeng, A., M√ºller, S., & Matsuo, A. (2018). Quanteda: An r package for the quantitative analysis of textual data. Journal of Open Source Software, 3(30), 774. https://doi.org/10.21105/joss.00774\nquanteda.sentiment \ntidymodels üåê | \nKuhn, M., & Wickham, H. (2020). Tidymodels: A collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org\nellmer üåê | \nWickham, H., & Cheng, J. (2024). Ellmer: Chat with large language models. https://ellmer.tidyverse.org\nrollama üåê | \nGruber, J. B., & Weber, M. (2024). Rollama: Communicate with ‚Äôollama‚Äô. https://jbgruber.github.io/rollama/\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "sessions/session-07.html",
    "href": "sessions/session-07.html",
    "title": "Session 7",
    "section": "",
    "text": "üñ•Ô∏è Session 07"
  },
  {
    "objectID": "sessions/session-07.html#participate",
    "href": "sessions/session-07.html#participate",
    "title": "Session 7",
    "section": "",
    "text": "üñ•Ô∏è Session 07"
  },
  {
    "objectID": "sessions/session-07.html#suggested-readings",
    "href": "sessions/session-07.html#suggested-readings",
    "title": "Session 7",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nAtteveldt, W. van, Trilling, D., & Arc√≠la, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O‚ÄôReilly."
  },
  {
    "objectID": "sessions/session-07.html#useful-packages",
    "href": "sessions/session-07.html#useful-packages",
    "title": "Session 7",
    "section": "Useful packages",
    "text": "Useful packages\n\nchat_downloader üåê | \n\ntwitch-dl üåê | )"
  },
  {
    "objectID": "sessions/session-07.html#useful-resources",
    "href": "sessions/session-07.html#useful-resources",
    "title": "Session 7",
    "section": "Useful resources",
    "text": "Useful resources\n\nüåê twitchtracker.com\nüåê twitchemotes.com\nüåê twitchmetrics.net/emotes\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "sessions/session-08.html",
    "href": "sessions/session-08.html",
    "title": "Session 8",
    "section": "",
    "text": "üñ•Ô∏è Session 08"
  },
  {
    "objectID": "sessions/session-08.html#participate",
    "href": "sessions/session-08.html#participate",
    "title": "Session 8",
    "section": "",
    "text": "üñ•Ô∏è Session 08"
  },
  {
    "objectID": "sessions/session-08.html#suggested-readings",
    "href": "sessions/session-08.html#suggested-readings",
    "title": "Session 8",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nAtteveldt, W. van, Trilling, D., & Arc√≠la, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nGrimmer, J., Roberts, M. E., & Stewart, B. M. (2022). Text as data: A new framework for machine learning and the social sciences. Princeton University Press.\nJurafsky, D., & Martin, J. H. (2024). Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition with language models (3rd ed.). https://web.stanford.edu/~jurafsky/slp3/\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O‚ÄôReilly.\n\n\n‚Ä¶ with focus on n-gram methods\n\nNicholls, T. (2019). Detecting Textual Reuse in News Stories, At Scale. International Journal of Communication, 13(0), 25. https://ijoc.org/index.php/ijoc/article/view/9904\nArendt, F., & Karadas, N. (2017). Content analysis of mediated associations: An automated text-analytic approach. Communication Methods and Measures, 11(2), 105‚Äì120. https://doi.org/10.1080/19312458.2016.1276894\n\n\n\n‚Ä¶ with focus on topic modeling\n\nChen, Y., Peng, Z., Kim, S.-H., & Choi, C. W. (2023). What We Can Do and Cannot Do with Topic Modeling: A Systematic Review. Communication Methods and Measures, 17(2), 111‚Äì130. https://doi.org/10.1080/19312458.2023.2167965\nHase, V. (2023). Automated Content Analysis (F. Oehmer-Pedrazzi, S. H. Kessler, E. Humprecht, K. Sommer, & L. Castro, Eds.; pp. 23‚Äì36). Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-36179-2_3\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., H√§ussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93‚Äì118. https://doi.org/10.1080/19312458.2018.1430754"
  },
  {
    "objectID": "sessions/session-08.html#useful-packages",
    "href": "sessions/session-08.html#useful-packages",
    "title": "Session 8",
    "section": "Useful packages",
    "text": "Useful packages\n\nquanteda üåê | \nBenoit, K., Watanabe, K., Wang, H., Nulty, P., Obeng, A., M√ºller, S., & Matsuo, A. (2018). Quanteda: An r package for the quantitative analysis of textual data. Journal of Open Source Software, 3(30), 774. https://doi.org/10.21105/joss.00774\ntextreuse üåê | \nLi, Y., & Mullen, L. (2024). Textreuse: Detect text reuse and document similarity. https://docs.ropensci.org/textreuse (website) https://github.com/ropensci/textreuse\ntidytext üåê | \nSilge, J., & Robinson, D. (2016). Tidytext: Text mining and analysis using tidy data principles in r. The Journal of Open Source Software, 1(3), 37. https://doi.org/10.21105/joss.00037\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "exercises/exercise-09.html",
    "href": "exercises/exercise-09.html",
    "title": "Topic Modeling with stminsights",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "exercises/exercise-09.html#background",
    "href": "exercises/exercise-09.html#background",
    "title": "Topic Modeling with stminsights",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays‚Äôs data basis: Topic Modeling\n\n\n\nTranscripts & Chats of the Live-Streams from  hasanabi and  zackrawrr and | TheMajorityReport for the Presidential (Harris vs.¬†Trump) and Vice-Presidential (Vance vs.¬†Walz) Debates 2024\n\n\n\nThe best way to learn R is by trying. This document tries to display a version of the ‚Äúnormal‚Äù data processing procedure.\nUse tidytuesday data as an example to showcase the potential"
  },
  {
    "objectID": "exercises/exercise-09.html#preparation",
    "href": "exercises/exercise-09.html#preparation",
    "title": "Topic Modeling with stminsights",
    "section": "Preparation",
    "text": "Preparation\n\nPackages\nThe pacman::p_load() function from the pacman package is used to load the packages, which has several advantages over the conventional method with library():\n\nConcise syntax\nAutomatic installation (if the package is not already installed)\nLoading multiple packages at once\nAutomatic search for dependencies\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, \n    magrittr, janitor,\n    ggpubr, \n    gt, gtExtras,\n    countdown, \n    quanteda, # quanteda text processing\n    quanteda.textplots, quanteda.textstats, quanteda.textmodels,\n    tidytext, \n    udpipe, spacyr, # POS tagging\n    stm, stminsights,\n    easystats, tidyverse\n)\n\n\n\nImport und Vorverarbeitung der Daten\n\n\n\n\n\n\nInformation\n\n\n\nFor information about how the data, especially the topic modeling results, were prepared, processed and estimated, please see the tutorial.\n\n\n\n# Import base data\nchats &lt;- qs::qread(here(\"local_data/chat-debates_full.qs\"))$correct\n\n# Import corpora\nchats_spacyr &lt;- qs::qread(here(\"local_data/chat-corpus_spacyr.qs\"))\nstm_search &lt;- qs::qread(here(\"local_data/stm-majority_report-search.qs\"))\nstm_results &lt;- qs::qread(here(\"local_data/stm-majority_report-results.qs\"))\n\n\nVorverarbeitung der Daten\n\nchats_valid &lt;- chats %&gt;% \n  mutate(\n    across(c(debate, platform), ~as.factor(.x))\n  ) \n\n\n\nVorverarbeitung des Korpus\n\n# spacyr-Korpus zu Tokens\nchat_spacyr_toks &lt;- chats_spacyr %&gt;% \n  as.tokens(\n    use_lemma = TRUE\n  ) %&gt;% \n  tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = FALSE,\n    remove_url = FALSE, \n    split_hyphens = FALSE,\n    split_tags = FALSE,\n  ) %&gt;% \n  tokens_remove(\n    pattern = stopwords(\"en\")\n  ) %&gt;% \n  tokens_ngrams(n = 1:3) \n\n\n\nAdd docvars\n\n# Get document names from the original data\noriginal_docnames &lt;- chats$message_id\n\n# Get document names from the tokens object\ntoken_docnames &lt;- docnames(chat_spacyr_toks)\n\n# Identify missing documents\nmissing_docs &lt;- setdiff(original_docnames, token_docnames)\n\n# Exclude \"empty\" messages\nchats_filtered &lt;- chats_valid %&gt;% \n  filter(!message_id %in% missing_docs)\n\n# Add docvars\ndocvars(chat_spacyr_toks) &lt;- chats_filtered\n\n\n\nFokus on The Majority Report\n\n# Subset tokens based on docvars\nmajority_report_chat_toks &lt;- tokens_subset(\n  chat_spacyr_toks, streamer == \"the_majority_report\")\n\n\n# Convert to DFM\nmajority_report_chat_dfm &lt;- majority_report_chat_toks %&gt;% \n  dfm()\n\n# Pruning\nmajority_report_chat_trim &lt;- majority_report_chat_dfm %&gt;% \n    dfm_trim(\n        min_docfreq = 50/nrow(chats),\n        max_docfreq = 0.99, \n        docfreq_type = \"prop\"\n   )\n\n# Convert for stm topic modeling\nmajority_report_chat_stm &lt;- majority_report_chat_trim %&gt;% \n   convert(to = \"stm\")\n\n\nempty_docs &lt;- Matrix::rowSums(\n  as(majority_report_chat_trim, \"Matrix\")) == 0 \nempty_docs_ids &lt;- majority_report_chat_trim@docvars$docname[empty_docs]\n\nchats_model &lt;- chats_filtered %&gt;% \n  filter(!(message_id %in% empty_docs_ids)) %&gt;% \n  filter(streamer == \"the_majority_report\")\n\n\n\nExport topic models\n\nK = 12\n\n# Get model\ntpm_k12 &lt;- stm_results %&gt;% \n   filter(k == 12) |&gt; \n   pull(mdl) %&gt;% .[[1]]\n\n# Estimate effects\neffects_k12 &lt;- estimateEffect(\n  formula =~ platform + debate + message_during_debate,\n  stmobj = tpm_k12, \n  metadata = chats_model)\n\n\n\nK = 14\n\n# Get model\ntpm_k14 &lt;- stm_results %&gt;% \n   filter(k == 14) |&gt; \n   pull(mdl) %&gt;% .[[1]]\n\n# Estimate effects\neffects_k14 &lt;- estimateEffect(\n  formula =~ platform + debate + message_during_debate,\n  stmobj = tpm_k14, \n  metadata = chats_model)\n\n\n\nK = 18\n\n# Get model\ntpm_k18 &lt;- stm_results %&gt;% \n   filter(k == 18) |&gt; \n   pull(mdl) %&gt;% .[[1]]\n\n# Estimate effects\neffects_k18 &lt;- estimateEffect(\n  formula =~ platform + debate + message_during_debate,\n  stmobj = tpm_k18, \n  metadata = chats_model)"
  },
  {
    "objectID": "exercises/exercise-09.html#praktische-√ºbung",
    "href": "exercises/exercise-09.html#praktische-√ºbung",
    "title": "Topic Modeling with stminsights",
    "section": "üõ†Ô∏è Praktische √úbung",
    "text": "üõ†Ô∏è Praktische √úbung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden üìã Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation ausgef√ºhrt haben. Das k√∂nnen Sie tun, indem Sie den ‚ÄúRun all chunks above‚Äù-Knopf  des n√§chsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in das Tutorial (.qmd oder .html). Beim Tutorial handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Tutorial also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\nPrepare and save workspace\n\n# Set names required by stminsights\ndata &lt;- majority_report_chat_trim\nout &lt;- majority_report_chat_stm\n\n# Clean workspace\nrm(list = setdiff(\n  ls(),\n  c(\"data\", \"out\",\n    \"tpm_k12\", \"tpm_k14\", \"tpm_k18\", \n    \"effects_k12\", \"effects_k14\", \"effects_k18\")))\n\n# Save workspace\nsave.image(here(\"stm_session_09.RData\"))\n\n\n\nStart stminsights\n\nlibrary(stminsights)\nrun_stminsights()"
  },
  {
    "objectID": "exercises/exercise-03_collab.html",
    "href": "exercises/exercise-03_collab.html",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "",
    "text": "Link to slides\n Download source file"
  },
  {
    "objectID": "exercises/exercise-03_collab.html#background",
    "href": "exercises/exercise-03_collab.html#background",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays‚Äôs data basis: Hollywood Age Gaps\n\n\n\n\nAn informational site showing the age gap between movie love interests.\n\nThe data follows certain rules:\n\nThe two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)\nThe youngest of the two actors is at least 17 years old\nNot animated characters\n\n\n\n\nThe best way to learn R is by trying. This document tries to display a version of the ‚Äúnormal‚Äù data processing procedure.\nUse tidytuesday data as an example to showcase the potential"
  },
  {
    "objectID": "exercises/exercise-03_collab.html#preparation",
    "href": "exercises/exercise-03_collab.html#preparation",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "Preparation",
    "text": "Preparation\n\nPackages\nThe pacman::p_load() package is used to load the packages, which has several advantages over the conventional method with library():\n\nConcise syntax\nAutomatic installation (if the package is not already installed)\nLoading multiple packages at once\nAutomatic search for dependencies\n\n\npacman::p_load(\n  here, \n  magrittr, \n  tidyverse,\n  janitor,\n  easystats,\n  sjmisc,\n  ggpubr)\n\n\n\nImport und Vorverarbeitung der Daten\n\nVariablennamen und -beschreibungen\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nmovie_name\nName of the film\n\n\nrelease_year\nRelease year\n\n\ndirector\nDirector of the film\n\n\nage_difference\nAge difference between the characters in whole years\n\n\ncouple_number\nAn identifier for the couple in case multiple couples are listed for this film\n\n\nactor_1_name\nThe name of the older actor in this couple\n\n\nactor_2_name\nThe name of the younger actor in this couple\n\n\nactor_1_birthdate\nThe birthdate of the older member of the couple\n\n\nactor_2_birthdate\nThe birthdate of the younger member of the couple\n\n\nactor_1_age\nThe age of the older actor when the film was released\n\n\nactor_2_age\nThe age of the younger actor when the film was released\n\n\n\n\n# Import data from URL\nage_gaps &lt;- read_csv(\"http://hollywoodagegap.com/movies.csv\") %&gt;% \n  janitor::clean_names()\n\n\n# Correct data\nage_gaps_correct &lt;- age_gaps %&gt;% \n  mutate(\n    across(ends_with(\"_birthdate\"), ~as.Date(.)) # set dates to dates\n  )"
  },
  {
    "objectID": "exercises/exercise-03_collab.html#praktische-√ºbung",
    "href": "exercises/exercise-03_collab.html#praktische-√ºbung",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "üõ†Ô∏è Praktische √úbung",
    "text": "üõ†Ô∏è Praktische √úbung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden üìã Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation ausgef√ºhrt haben. Das k√∂nnen Sie tun, indem Sie den ‚ÄúRun all chunks above‚Äù-Knopf  des n√§chsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\nüîé Welche Rolle spielt das Geschlecht?\n\n\n\n\n\n\nSpielt das Geschlecht eine Rolle?\n\n\n\n\nDer folgende Abschitt befasst sich nun erg√§nzend mit der Frage, welche Rolle das Geschlecht mit Blick auf die ‚ÄúG√ºltigkeit‚Äù der vorherigen Ergebnisse spielt\nDazu sind jedoch weitere Explorations- und √úberarbeitungsschritte notwendig\n\n\n\n\n\nüìã Exercise 1: √úbepr√ºfung der _gender-Variablen\n\n\n\n\n\n\nArbeitsauftrag 1.1\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_correct die Variablen actor_1_gender und actor_2_gender an.\n\n\n\n# INSERT CODE HERE\n\n\n\n\n\n\n\nArbeitsauftrag 1.2\n\n\n\nNutzen Sie die Funktion sjmisc::flat_talbe() und den Datensatz age_gaps_correct um eine Kreuztabelle der Variablen actor_1_gender und actor_2_gender zu erstellen.\n\n\n\n# INSERT CODE HERE\n\n\n\nüîé Sind die Daten ‚Äúkonsistent‚Äù?\n\n√úberpr√ºfung der Sortierung\n\nage_gaps_correct %&gt;% \n  summarise(\n      p1_older = mean(actor_1_age &gt; actor_2_age), # older person first?\n      p1_male  = mean(actor_1_gender == \"man\"),  # male person first? \n      p_1_first_alpha = mean(actor_1_name &lt; actor_2_name) # alphabetical order?\n  )\n\n# A tibble: 1 √ó 3\n  p1_older p1_male p_1_first_alpha\n     &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;\n1    0.813   0.987           0.496\n\n\n\n\n\n√úberpr√ºfung der Anzahl pro Paare pro Film\n\n# Create data\ncouples &lt;- age_gaps_correct %&gt;% \n  group_by(movie_name) %&gt;% \n  summarise(n = n()) \n\n# Distribution\ncouples %&gt;% frq(n)\n\nn &lt;integer&gt; \n# total N=863 valid N=863 mean=1.39 sd=0.75\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    1 | 628 | 72.77 |   72.77 |  72.77\n    2 | 162 | 18.77 |   18.77 |  91.54\n    3 |  54 |  6.26 |    6.26 |  97.80\n    4 |  14 |  1.62 |    1.62 |  99.42\n    5 |   3 |  0.35 |    0.35 |  99.77\n    6 |   1 |  0.12 |    0.12 |  99.88\n    7 |   1 |  0.12 |    0.12 | 100.00\n &lt;NA&gt; |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n# Movies with a loot of couples \ncouples %&gt;% \n  filter(n &gt; 3) %&gt;% \n  arrange(desc(n))\n\n# A tibble: 19 √ó 2\n   movie_name                      n\n   &lt;chr&gt;                       &lt;int&gt;\n 1 Love Actually                   7\n 2 The Family Stone                6\n 3 A View to a Kill                5\n 4 He's Just Not That Into You     5\n 5 Mona Lisa Smile                 5\n 6 A Star Is Born                  4\n 7 American Pie                    4\n 8 Boogie Nights                   4\n 9 Book Club                       4\n10 Closer                          4\n11 Pushing Tin                     4\n12 Sex and the City                4\n13 Soul Food                       4\n14 Tag                             4\n15 The Favourite                   4\n16 The Girl on the Train           4\n17 The Other Woman                 4\n18 Tomorrow Never Dies             4\n19 Twilight                        4\n\n\n\n\nKorrekturen\n\nage_gaps_consistent &lt;- age_gaps_correct %&gt;% \n  # If multiple couples, assign couple number by movie\n  mutate(\n      couple_number = row_number(),\n      .by = \"movie_name\"\n  ) %&gt;% \n  # Change data structure (one line per actor in a coulpe of a movie)\n  pivot_longer(\n    cols = starts_with(c(\"actor_1_\", \"actor_2_\")),\n    names_to = c(NA, NA, \".value\"),\n    names_sep = \"_\"\n  ) %&gt;% \n  # Put older actor first\n  arrange(desc(age_difference), movie_name, birthdate) %&gt;% \n    mutate(\n    position = row_number(),\n    .by = c(\"movie_name\", \"couple_number\")\n  ) %&gt;% \n  pivot_wider(\n    names_from = \"position\",\n    names_glue = \"actor_{position}_{.value}\",\n    values_from = c(\"name\", \"gender\", \"birthdate\", \"age\")\n  ) %&gt;% \n  mutate(\n    couple_structure = case_when(\n      actor_1_gender == \"woman\" & actor_2_gender == \"woman\" ~ 1,\n      actor_1_gender == \"man\" & actor_2_gender == \"man\" ~ 2,\n      actor_1_gender != \"man\" ~ 3, \n      actor_1_gender == \"man\" ~ 4,\n    ),\n    older_male_hetero  = sjmisc::rec(\n      couple_structure, \n      rec=\"3=0; 4=1; ELSE=NA\", \n      to.factor = TRUE\n    )\n  )\n\n\n\nüîé Die zweite Datenexploration\n\n\nüìã Exercise 2: Alterskombinationen im √úberblick\n\n\n\n\n\n\nArbeitauftrag 2\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_consistent die Variablen couple_structure und older_male_hetero an.\n\n\n\n# INSERT CODE HERE\n\n\n\nüìã Exercise 3: Wie sind die Altersunterschiede unterteilt, unter Ber√ºcksichtiung des Geschlechts?\n\n\n\n\n\n\nArbeitsauftrag 3.1 (graphische √úberpr√ºfung)\n\n\n\n\nErstellen Sie, auf Basis des Datensatzes age_gaps_consistent, einen ggplot.\nNutzen Sie im Argument aes() die Variable age_difference als x-Variable und older_male_hetero f√ºr das Argument fill.\nNutzen Sie geom_bar zur Erzeugung des Plots.\nOptional: Verwenden Sie theme_pubr\n\n\n\n\n# INSERT CODE HERE\n\n\n\n\n\n\n\nArbeitsauftrag 3.2 (√úberpr√ºfung durch Modellierung)\n\n\n\n\nErstellen Sie ein lineares Modell (lm), das die Variable age_difference als abh√§ngige Variable und die Variablen release_year und older_male_hetero als unabh√§ngige Variablen verwendet. Nutzen Sie dazu den Datensatz age_gaps_consistent.\nGeben Sie die Parameter des Modells mit der Funktion parameters::parameters() aus.\nBewerten Sie die Modellleistung mit der Funktion performance::model_performance().\nErstellen Sie einen Bericht √ºber das Modell mit der Funktion report::report().\n\n\n\n\n# INSERT CODE HERE"
  },
  {
    "objectID": "exercises/exercise-07_solution.html",
    "href": "exercises/exercise-07_solution.html",
    "title": "Twitch Chat Analysis",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "exercises/exercise-07_solution.html#background",
    "href": "exercises/exercise-07_solution.html#background",
    "title": "Twitch Chat Analysis",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays‚Äôs data basis: Twitch Chat & Transcripts\n\n\n\nTranscripts & Chats of the Live-Streams from  hasanabi and  zackrawrr and | TheMajorityReport for the Presidential (Harris vs.¬†Trump) and Vice-Presidential (Vance vs.¬†Walz) Debates 2024\n\n\n\nThe best way to learn R is by trying. This document tries to display a version of the ‚Äúnormal‚Äù data processing procedure.\nUse tidytuesday data as an example to showcase the potential"
  },
  {
    "objectID": "exercises/exercise-07_solution.html#preparation",
    "href": "exercises/exercise-07_solution.html#preparation",
    "title": "Twitch Chat Analysis",
    "section": "Preparation",
    "text": "Preparation\n\nPackages\nThe pacman::p_load() function from the pacman package is used to load the packages, which has several advantages over the conventional method with library():\n\nConcise syntax\nAutomatic installation (if the package is not already installed)\nLoading multiple packages at once\nAutomatic search for dependencies\n\n\npacman::p_load(\n    here, taylor,\n    magrittr, janitor,\n    ggpubr, \n    gt, gtExtras,\n    countdown, \n    quanteda, # quanteda text processing\n    quanteda.textplots, \n    easystats, tidyverse\n)\n\n\n\nImport und Vorverarbeitung der Daten\n\nchats &lt;- qs::qread(here(\"local_data/chat-debates_full.qs\"))$correct\ntranscripts &lt;- qs::qread(here(\"local_data/transcripts-debates_full.qs\"))$correct"
  },
  {
    "objectID": "exercises/exercise-07_solution.html#praktische-√ºbung",
    "href": "exercises/exercise-07_solution.html#praktische-√ºbung",
    "title": "Twitch Chat Analysis",
    "section": "üõ†Ô∏è Praktische √úbung",
    "text": "üõ†Ô∏è Praktische √úbung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden üìã Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation ausgef√ºhrt haben. Das k√∂nnen Sie tun, indem Sie den ‚ÄúRun all chunks above‚Äù-Knopf  des n√§chsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in das Tutorial (.qmd oder .html). Beim Tutorial handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Tutorial also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden."
  },
  {
    "objectID": "exercises/exercise-07_solution.html#kennenlernen-des-chat-datensatzes",
    "href": "exercises/exercise-07_solution.html#kennenlernen-des-chat-datensatzes",
    "title": "Twitch Chat Analysis",
    "section": "üîé Kennenlernen des Chat-Datensatzes",
    "text": "üîé Kennenlernen des Chat-Datensatzes\n\nüìã Exercise 1: Create corpus\n\nCreate new dataset corp_chats\n\nBased on the dataset chats, create a corpus object with the quanteda package.\nUse the corpus() function with the docid_field argument set to ‚Äúmessage_id‚Äù and the text_field argument set to ‚Äúmessage_content‚Äù.\nCheck if the transformation was successful by using the summary() function.\n\n\n\n# Create new dataset clean_tidy_tweets\ncorp_chats &lt;- chats %&gt;% \n  quanteda::corpus(\n    docid_field = \"message_id\", \n    text_field = \"message_content\"\n  )\n\n# Check\nsummary(corp_chats)\n\n\n\nüìã Exercise 2: Tokenization & DFM conversion\n\nCreate new datasets toks_chats & dfm_chats\n\n\nBased on the dataset corp_chats, create tokens using the tokens() function from the quanteda package.\nConvert the tokens to a document-feature matrix (DFM) using the dfm() function from the quanteda package.\nCheck if the transformations were successful (e.g.¬†by using the print() function).\n\n\n# Create tokens\ntoks_chats &lt;- corp_chats %&gt;%\n    quanteda::tokens() \n \n# Create DFM\ndfm_chats &lt;- toks_chats %&gt;%\n    quanteda::dfm()\n\n# Check\ntoks_chats %&gt;% print()\ndfm_chats %&gt;% print()\n\n\n\nüìã Exercise 3: Analyse DFM\n\nBased on dfm_chats\n\nUse the textstat_frequency() function from the quanteda package to get the top 50 tokens.\nDisplay the results.\n\nBased on the results, what preprocessing steps could be useful?\n\n\n# Top 50 Tokens\ndfm_chats %&gt;% \n  quanteda.textstats::textstat_frequency(n = 50) \n\n\n\nüìã Exercise 4: Preprocessing\n\nCreate a new dataset dfm_chats_preprocessed\n\nBased on corp_chats, preprocess the data according to the steps you think are necessary (e.g.¬†removing punctuation, symbols, numbers, URLs, and stopwords).\nDepending on the steps you choose, you might need to use the tokens_remove() function from the quanteda package.\nCreate a new DFM object dfm_chats_preprocessed.\nUse the textstat_frequency() function from the quanteda package on the newly created dataset to get the top 50 tokens and compare the result with the results of Exercise 3.\n\n\n\n# Preprocessing\ndfm_chats_preprocessed &lt;- corp_chats %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  ) %&gt;% \n  quanteda::tokens_remove(\n    pattern = quanteda::stopwords(\"en\")\n  ) %&gt;% \n  quanteda::dfm(\n    tolower = TRUE\n  )\n\n# Check\ndfm_chats_preprocessed %&gt;% \n  quanteda.textstats::textstat_frequency(n = 50)"
  },
  {
    "objectID": "exercises/exercise-03.html",
    "href": "exercises/exercise-03.html",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "exercises/exercise-03.html#background",
    "href": "exercises/exercise-03.html#background",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays‚Äôs data basis: Hollywood Age Gaps\n\n\n\n\nAn informational site showing the age gap between movie love interests.\n\nThe data follows certain rules:\n\nThe two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)\nThe youngest of the two actors is at least 17 years old\nNot animated characters\n\n\n\n\nThe best way to learn R is by trying. This document tries to display a version of the ‚Äúnormal‚Äù data processing procedure.\nUse tidytuesday data as an example to showcase the potential"
  },
  {
    "objectID": "exercises/exercise-03.html#preparation",
    "href": "exercises/exercise-03.html#preparation",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "Preparation",
    "text": "Preparation\n\nPackages\nThe pacman::p_load() package is used to load the packages, which has several advantages over the conventional method with library():\n\nConcise syntax\nAutomatic installation (if the package is not already installed)\nLoading multiple packages at once\nAutomatic search for dependencies\n\n\npacman::p_load(\n  here, \n  magrittr, \n  tidyverse,\n  janitor,\n  easystats,\n  sjmisc,\n  ggpubr)\n\n\n\nImport und Vorverarbeitung der Daten\n\nVariablennamen und -beschreibungen\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nmovie_name\nName of the film\n\n\nrelease_year\nRelease year\n\n\ndirector\nDirector of the film\n\n\nage_difference\nAge difference between the characters in whole years\n\n\ncouple_number\nAn identifier for the couple in case multiple couples are listed for this film\n\n\nactor_1_name\nThe name of the older actor in this couple\n\n\nactor_2_name\nThe name of the younger actor in this couple\n\n\nactor_1_birthdate\nThe birthdate of the older member of the couple\n\n\nactor_2_birthdate\nThe birthdate of the younger member of the couple\n\n\nactor_1_age\nThe age of the older actor when the film was released\n\n\nactor_2_age\nThe age of the younger actor when the film was released\n\n\n\n\n# Import data from URL\nage_gaps &lt;- read_csv(\"http://hollywoodagegap.com/movies.csv\") %&gt;% \n  janitor::clean_names()\n\n\n# Correct data\nage_gaps_correct &lt;- age_gaps %&gt;% \n  mutate(\n    across(ends_with(\"_birthdate\"), ~as.Date(.)) # set dates to dates\n  )"
  },
  {
    "objectID": "exercises/exercise-03.html#praktische-√ºbung",
    "href": "exercises/exercise-03.html#praktische-√ºbung",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "üõ†Ô∏è Praktische √úbung",
    "text": "üõ†Ô∏è Praktische √úbung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden üìã Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation ausgef√ºhrt haben. Das k√∂nnen Sie tun, indem Sie den ‚ÄúRun all chunks above‚Äù-Knopf  des n√§chsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in das Tutorial (.qmd oder .html). Beim Tutorial handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Tutorial also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\nüîé Welche Rolle spielt das Geschlecht?\n\n\n\n\n\n\nSpielt das Geschlecht eine Rolle?\n\n\n\n\nDer folgende Abschitt befasst sich nun erg√§nzend mit der Frage, welche Rolle das Geschlecht mit Blick auf die ‚ÄúG√ºltigkeit‚Äù der vorherigen Ergebnisse spielt\nDazu sind jedoch weitere Explorations- und √úberarbeitungsschritte notwendig\n\n\n\n\n\nüìã Exercise 1: √úbepr√ºfung der _gender-Variablen\n\n\n\n\n\n\nArbeitsauftrag 1.1\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_correct die Variablen actor_1_gender und actor_2_gender an.\n\n\n\n# INSERT CODE HERE\n\n\n\n\n\n\n\nArbeitsauftrag 1.2\n\n\n\nNutzen Sie die Funktion sjmisc::flat_talbe() und den Datensatz age_gaps_correct um eine Kreuztabelle der Variablen actor_1_gender und actor_2_gender zu erstellen.\n\n\n\n# INSERT CODE HERE\n\n\n\nüîé Sind die Daten ‚Äúkonsistent‚Äù?\n\n√úberpr√ºfung der Sortierung\n\nage_gaps_correct %&gt;% \n  summarise(\n      p1_older = mean(actor_1_age &gt; actor_2_age), # older person first?\n      p1_male  = mean(actor_1_gender == \"man\"),  # male person first? \n      p_1_first_alpha = mean(actor_1_name &lt; actor_2_name) # alphabetical order?\n  )\n\n# A tibble: 1 √ó 3\n  p1_older p1_male p_1_first_alpha\n     &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;\n1    0.813   0.987           0.495\n\n\n\n\n\n√úberpr√ºfung der Anzahl pro Paare pro Film\n\n# Create data\ncouples &lt;- age_gaps_correct %&gt;% \n  group_by(movie_name) %&gt;% \n  summarise(n = n()) \n\n# Distribution\ncouples %&gt;% frq(n)\n\nn &lt;integer&gt; \n# total N=864 valid N=864 mean=1.39 sd=0.75\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    1 | 629 | 72.80 |   72.80 |  72.80\n    2 | 162 | 18.75 |   18.75 |  91.55\n    3 |  54 |  6.25 |    6.25 |  97.80\n    4 |  14 |  1.62 |    1.62 |  99.42\n    5 |   3 |  0.35 |    0.35 |  99.77\n    6 |   1 |  0.12 |    0.12 |  99.88\n    7 |   1 |  0.12 |    0.12 | 100.00\n &lt;NA&gt; |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n# Movies with a loot of couples \ncouples %&gt;% \n  filter(n &gt; 3) %&gt;% \n  arrange(desc(n))\n\n# A tibble: 19 √ó 2\n   movie_name                      n\n   &lt;chr&gt;                       &lt;int&gt;\n 1 Love Actually                   7\n 2 The Family Stone                6\n 3 A View to a Kill                5\n 4 He's Just Not That Into You     5\n 5 Mona Lisa Smile                 5\n 6 A Star Is Born                  4\n 7 American Pie                    4\n 8 Boogie Nights                   4\n 9 Book Club                       4\n10 Closer                          4\n11 Pushing Tin                     4\n12 Sex and the City                4\n13 Soul Food                       4\n14 Tag                             4\n15 The Favourite                   4\n16 The Girl on the Train           4\n17 The Other Woman                 4\n18 Tomorrow Never Dies             4\n19 Twilight                        4\n\n\n\n\nKorrekturen\n\nage_gaps_consistent &lt;- age_gaps_correct %&gt;% \n  # If multiple couples, assign couple number by movie\n  mutate(\n      couple_number = row_number(),\n      .by = \"movie_name\"\n  ) %&gt;% \n  # Change data structure (one line per actor in a coulpe of a movie)\n  pivot_longer(\n    cols = starts_with(c(\"actor_1_\", \"actor_2_\")),\n    names_to = c(NA, NA, \".value\"),\n    names_sep = \"_\"\n  ) %&gt;% \n  # Put older actor first\n  arrange(desc(age_difference), movie_name, birthdate) %&gt;% \n    mutate(\n    position = row_number(),\n    .by = c(\"movie_name\", \"couple_number\")\n  ) %&gt;% \n  pivot_wider(\n    names_from = \"position\",\n    names_glue = \"actor_{position}_{.value}\",\n    values_from = c(\"name\", \"gender\", \"birthdate\", \"age\")\n  ) %&gt;% \n  mutate(\n    couple_structure = case_when(\n      actor_1_gender == \"woman\" & actor_2_gender == \"woman\" ~ 1,\n      actor_1_gender == \"man\" & actor_2_gender == \"man\" ~ 2,\n      actor_1_gender != \"man\" ~ 3, \n      actor_1_gender == \"man\" ~ 4,\n    ),\n    older_male_hetero  = sjmisc::rec(\n      couple_structure, \n      rec=\"3=0; 4=1; ELSE=NA\", \n      to.factor = TRUE\n    )\n  )\n\n\n\nüîé Die zweite Datenexploration\n\n\nüìã Exercise 2: Alterskombinationen im √úberblick\n\n\n\n\n\n\nArbeitauftrag 2\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_consistent die Variablen couple_structure und older_male_hetero an.\n\n\n\n# INSERT CODE HERE\n\n\n\nüìã Exercise 3: Wie sind die Altersunterschiede unterteilt, unter Ber√ºcksichtiung des Geschlechts?\n\n\n\n\n\n\nArbeitsauftrag 3.1 (graphische √úberpr√ºfung)\n\n\n\n\nErstellen Sie, auf Basis des Datensatzes age_gaps_consistent, einen ggplot.\nNutzen Sie im Argument aes() die Variable age_difference als x-Variable und older_male_hetero f√ºr das Argument fill.\nNutzen Sie geom_bar zur Erzeugung des Plots.\nOptional: Verwenden Sie theme_pubr\n\n\n\n\n# INSERT CODE HERE\n\n\n\n\n\n\n\nArbeitsauftrag 3.2 (√úberpr√ºfung durch Modellierung)\n\n\n\n\nErstellen Sie ein lineares Modell (lm), das die Variable age_difference als abh√§ngige Variable und die Variablen release_year und older_male_hetero als unabh√§ngige Variablen verwendet. Nutzen Sie dazu den Datensatz age_gaps_consistent.\nGeben Sie die Parameter des Modells mit der Funktion parameters::parameters() aus.\nBewerten Sie die Modellleistung mit der Funktion performance::model_performance().\nErstellen Sie einen Bericht √ºber das Modell mit der Funktion report::report().\n\n\n\n\n# INSERT CODE HERE"
  },
  {
    "objectID": "tutorials/tutorial-10.html",
    "href": "tutorials/tutorial-10.html",
    "title": "üî® Sentiment Analysis with R",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "tutorials/tutorial-10.html#background",
    "href": "tutorials/tutorial-10.html#background",
    "title": "üî® Sentiment Analysis with R",
    "section": "Background",
    "text": "Background"
  },
  {
    "objectID": "tutorials/tutorial-10.html#preparation",
    "href": "tutorials/tutorial-10.html#preparation",
    "title": "üî® Sentiment Analysis with R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, \n    magrittr, janitor,\n    ggpubr, ggdist, ggsci,\n    gt, gtExtras,\n    countdown, \n    quanteda, # quanteda text processing\n    quanteda.textplots, quanteda.textstats, \n    quanteda.textmodels, quanteda.sentiment, \n    ellmer, rollama,\n    tidymodels,\n    easystats, tidyverse\n)\n\n\n# Import base data\nchats &lt;- qs::qread(here(\"local_data/chats.qs\"))\ncorp_chats &lt;- chats %&gt;% \n    quanteda::corpus(docid_field = \"message_id\", text_field = \"message_content\")"
  },
  {
    "objectID": "tutorials/tutorial-10.html#codechunks-aus-der-sitzung",
    "href": "tutorials/tutorial-10.html#codechunks-aus-der-sitzung",
    "title": "üî® Sentiment Analysis with R",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nPraktische Anwedung von quanteda.sentiment\n\nchats_polarity &lt;- corp_chats %&gt;% \n  textstat_polarity(\n    dictionary = data_dictionary_LSD2015) %&gt;% \n  rename(polarity = sentiment)\n\n\nchats_polarity %&gt;% \n    head(n = 10) \n\n                                 doc_id  polarity\n1  dc03b89a-722d-4eaa-a895-736533a68aca  0.000000\n2  6be50e12-2fd5-436f-b253-b2358b618380  0.000000\n3  f5e41904-7f01-4f03-ad6c-2c0f07d70ed0  1.098612\n4  92dc6519-eb54-4c18-abef-27201314b22f -1.098612\n5  92055088-7067-48c0-aa11-9c6103bdf4c4  0.000000\n6  03ad4706-aa67-4ddc-a1e4-6f8ca981778e  0.000000\n7  00c5dd9c-41b8-4430-8b2e-be67c5e363ac  0.000000\n8  923c7eac-d92e-4cac-876a-07d4fa45cb57  0.000000\n9  6bdfb03d-fdbd-48b6-9b81-2fc56785fd67 -1.098612\n10 7f25fc9f-b000-41fa-91b8-60672cd3e608  0.000000\n\n\n\nchats_valence &lt;- corp_chats %&gt;% \n  textstat_valence(\n    dictionary = data_dictionary_AFINN) %&gt;% \n  rename(valence = sentiment)\n\n\nchats_valence %&gt;% \n    head(n = 10)\n\n                                 doc_id valence\n1  dc03b89a-722d-4eaa-a895-736533a68aca       0\n2  6be50e12-2fd5-436f-b253-b2358b618380       0\n3  f5e41904-7f01-4f03-ad6c-2c0f07d70ed0       0\n4  92dc6519-eb54-4c18-abef-27201314b22f      -5\n5  92055088-7067-48c0-aa11-9c6103bdf4c4       0\n6  03ad4706-aa67-4ddc-a1e4-6f8ca981778e       0\n7  00c5dd9c-41b8-4430-8b2e-be67c5e363ac       0\n8  923c7eac-d92e-4cac-876a-07d4fa45cb57       0\n9  6bdfb03d-fdbd-48b6-9b81-2fc56785fd67       0\n10 7f25fc9f-b000-41fa-91b8-60672cd3e608       0\n\n\n\n\nPraktische Anwedung von vader\n\nchats_vader &lt;- chats %&gt;% \n  mutate(\n    vader_output = map(message_content, ~vader::get_vader(.x)), \n    # Extract word-level scores\n    word_scores = map(vader_output, ~ .x[\n        names(.x) != \"compound\" &\n        names(.x) != \"pos\" & \n        names(.x) != \"neu\" & \n        names(.x) != \"neg\" & \n        names(.x) != \"but_count\"]),  \n    compound = map_dbl(vader_output, ~ as.numeric(.x[\"compound\"])),\n    pos = map_dbl(vader_output, ~ as.numeric(.x[\"pos\"])),\n    neu = map_dbl(vader_output, ~ as.numeric(.x[\"neu\"])),\n    neg = map_dbl(vader_output, ~ as.numeric(.x[\"neg\"])),\n    but_count = map_dbl(vader_output, ~ as.numeric(.x[\"but_count\"]))\n  )\n\n\nqs::qsave(chats_vader, file = here(\"local_data/chats-vader.qs\"))\n\n\nchats_vader %&gt;% \n    select(message_id, compound:but_count) %&gt;% \n    head(n = 20)\n\n# A tibble: 20 √ó 6\n   message_id                           compound   pos   neu   neg but_count\n   &lt;chr&gt;                                   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 dc03b89a-722d-4eaa-a895-736533a68aca    0         0 1     0             0\n 2 6be50e12-2fd5-436f-b253-b2358b618380    0         0 1     0             0\n 3 f5e41904-7f01-4f03-ad6c-2c0f07d70ed0    0         0 1     0             0\n 4 92dc6519-eb54-4c18-abef-27201314b22f   -0.586     0 0.513 0.487         0\n 5 92055088-7067-48c0-aa11-9c6103bdf4c4    0         0 1     0             0\n 6 03ad4706-aa67-4ddc-a1e4-6f8ca981778e    0         0 1     0             0\n 7 00c5dd9c-41b8-4430-8b2e-be67c5e363ac    0         0 1     0             0\n 8 923c7eac-d92e-4cac-876a-07d4fa45cb57    0         0 1     0             0\n 9 6bdfb03d-fdbd-48b6-9b81-2fc56785fd67    0         0 1     0             0\n10 7f25fc9f-b000-41fa-91b8-60672cd3e608    0         0 1     0             0\n11 a00e2ca8-2e76-4941-b360-b6b311701cba    0         0 1     0             0\n12 637e5e96-9f26-4a87-955e-74f2fb29685a    0         0 1     0             0\n13 4b0a6fbe-54d6-4d06-8d08-875112abcd92    0         0 1     0             0\n14 cf57874e-a239-4bce-a766-4bb7636847b7    0         0 1     0             0\n15 51b66d60-0f6b-43a6-a40c-cb6d51cde1a9    0         0 1     0             0\n16 08d7ae3c-1180-4e26-940e-de763fbe6f18    0         0 1     0             0\n17 72494412-fe24-44ad-9a02-de22e8e54724    0         0 1     0             0\n18 93a9da3e-63ab-4eea-bb51-73bff8dadf13    0         0 1     0             0\n19 3aa667c1-a8b1-4f18-94a6-920b8a9ee37b    0         0 1     0             0\n20 daebee85-4885-48f2-8086-9b9172285792   -0.586     0 0.513 0.487         0\n\n\n\n\nZusammenf√ºhrung Dictionary-Sentiments\n\nchats_sentiment &lt;- chats %&gt;% \n    left_join(chats_polarity, by = join_by(\"message_id\" == \"doc_id\")) %&gt;%\n    left_join(chats_valence, by = join_by(\"message_id\" == \"doc_id\")) %&gt;% \n    left_join(chats_vader %&gt;% \n        select(message_id, vader_output, word_scores, compound, pos, neu, neg, but_count), \n        by = \"message_id\")\n\n\nchats_sentiment %&gt;% \n    select(message_id, polarity, valence, compound) %&gt;%\n    datawizard::describe_distribution()\n\nVariable |  Mean |   SD | IQR |         Range | Skewness | Kurtosis |      n | n_Missing\n----------------------------------------------------------------------------------------\npolarity | -0.06 | 0.65 |   0 | [-4.44, 3.61] |    -0.18 |     1.30 | 913375 |         0\nvalence  | -0.04 | 1.38 |   0 | [-5.00, 5.00] |    -0.21 |     2.54 | 913375 |         0\ncompound |  0.01 | 0.30 |   0 | [-1.00, 1.00] |    -0.12 |     1.11 | 913170 |       205\n\n\n\n\nVerschiedene VADER-Visualisierungen\n\nchats_vader_sample &lt;- chats_vader %&gt;%\n    filter(message_length &lt; 100) %&gt;%\n    slice_sample(n = 10) \n\n\nchats_vader_sample %&gt;%\n    ggplot(aes(x = message_content, y = compound, fill = compound &gt; 0)) +\n        geom_bar(stat = \"identity\", width = 0.7) +\n        scale_fill_manual(values = c(\"TRUE\" = \"blue\", \"FALSE\" = \"red\"), labels = c(\"Positive\", \"Negative\")) +\n        labs(\n            title = \"Overall Compound Sentiment for Each Sentence\",\n            x = \"Sentences\",\n            y = \"Compound Sentiment\",\n            fill = \"Sentiment\") +\n        coord_flip() +  # Flip for easier readability\n        theme_minimal() +\n        theme(\n            axis.text.x = element_text(angle = 45, hjust = 1))  # Label wrapping and adjusting angle\n\n\n\n\n\n\n\n\n\nchats_vader_sample %&gt;% \n    mutate(\n        pos_pct = pos * 100,\n        neu_pct = neu * 100,\n        neg_pct = neg * 100) %&gt;% \n  select(message_content, pos_pct, neu_pct, neg_pct) %&gt;% \n  pivot_longer(\n    cols = c(pos_pct, neu_pct, neg_pct),\n    names_to = \"sentiment\",\n    values_to = \"percentage\") %&gt;% \n  mutate(\n    sentiment = factor(\n        sentiment,\n        levels = c(\"pos_pct\", \"neu_pct\", \"neg_pct\"),\n        labels = c(\"Positive\", \"Neutral\", \"Negative\"))) %&gt;% \n  ggplot(aes(x = message_content, y = percentage, fill = sentiment)) +\n    geom_bar(stat = \"identity\", width = 0.7) +\n    scale_fill_manual(values = c(\"Positive\" = \"blue\", \"Neutral\" = \"gray\", \"Negative\" = \"red\")) +\n    labs(\n        title = \"Proportion of Positive, Neutral, and Negative Sentiment\",\n        x = \"Sentences\",\n        y = \"Percentage\",\n        fill = \"Sentiment\") +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nChat mit LLMs in R\n\nellmer\n\nellmer_chat_llama &lt;- ellmer::chat_ollama(\n    model = \"llama3.2\"\n)\n\nellmer_chat_llama$chat(\"Why is the sky blue?\")\n\nThe reason the sky appears blue is due to a phenomenon called Rayleigh \nscattering. This process occurs when sunlight interacts with tiny molecules of \ngases in the Earth's atmosphere, such as nitrogen and oxygen.\n\nHere's what happens:\n\n1. Sunlight enters the Earth's atmosphere and is made up of different \nwavelengths of light, which are like colors.\n2. The shorter wavelengths (blue and violet) of light are scattered more than \nthe longer wavelengths (red, orange, and yellow).\n3. This scattering occurs because the smaller molecules in the atmosphere are \nmore effective at reflecting the shorter wavelengths.\n4. As a result, when sunlight hits an observer's eye, it has to travel through \nmore of these tiny molecules, which scatter the blue light.\n5. Our eyes perceive this scattered blue light as the color of the sky.\n\nHowever, that's not the whole story. There are also other factors at play:\n\n* The Earth's atmosphere is not uniform in terms of gas composition and \ndensity, so different regions will have different scattering effects.\n* Certain particles like dust and pollution can scatter light in a way that \nchanges its apparent hue, making clouds look gray or hazy instead of blue.\n* Atmospheric conditions like fog or haze can also distort the usual blue \ncolor, making it appear more grayish or murky.\n\nSo, while Rayleigh scattering is the primary reason the sky appears blue, it's \nnot the only factor at play!\n\n(By the way, did you know that during sunrise and sunset, the sky sometimes \nappears red? That's because the sunlight has to travel through a longer \ndistance in the atmosphere to reach our eyes, which means more of its shorter \nwavelengths ‚Äì like blue and violet ‚Äì are scattered away.\n\n\n\nellmer_chat_mistral &lt;- ellmer::chat_ollama(\n    model = \"mistral\"\n)\n\nellmer_chat_mistral$chat(\"Why is the sky blue?\")\n\n The reason the sky appears blue during a sunny day is due to a process called \nRayleigh scattering. As sunlight passes through Earth's atmosphere, it collides\nwith molecules and tiny particles in the air. These small particles scatter the\nshorter wavelengths of light, such as blue, more effectively than the longer \nwavelengths of red or yellow.\n\nThe sky appears brightest in the color that gets scattered the most, and since \nblue light is scattered slightly more than any other color of visible light, we\nperceive the sky as blue. This does not mean that the sky's actual color \nchanges to blue but rather our eyes are sensitive to blue light and the \nintensity of blue light reaching our eyes compared to other colors makes the \nsky appear blue. Additionally, sunlight reaching us contains a greater amount \nof blue relative to red or yellow when it hits Earth, adding to this \nphenomenon.\n\nThis is one reason sunsets often take on the appearance of orange or red \ncolors- during sunset and sunrise, sunlight has to travel through more of the \nEarth's atmosphere than it does at noon, allowing more of the longer wavelength\nlight (red) to be scattered, which makes them appear reddish.\n\nHope this helps explain why the sky appears blue!\n\n\n\n\nrollama\n\ndemo_2_llama3_2 &lt;- rollama::query(\n     \"What is the longest five letter word in english?\",\n    model = \"llama3.2\",\n    screen = FALSE,\n    output = \"text\"\n)\n\nglue::glue(demo_2_llama3_2)\n\nThe longest five-letter word in English is \"stamps\" or \"strengths\" but another five letter word that is commonly used is \"steaks\".\n\n\n\ndemo_2_mistral &lt;- rollama::query(\n    \"What is the longest five letter word in english?\",\n    model = \"mistral\",\n    screen = FALSE,\n    output = \"text\"\n)\n\nglue::glue(demo_2_mistral)\n\n The longest common five-letter English word without repeating letters is \"stewardesses.\" However, if we consider uncommon words and allow repetitions of a single letter (a palindrome), then the longest five-letter word becomes \"deified\" or \"undeified\". But for everyday use and most dictionaries, \"stewardesses\" takes the crown.\n\n\n\ndemo_3_llama3_2 &lt;- rollama::query(\n    \"Is 9677 a prime number?\",\n    model = \"llama3.2\",\n    screen = FALSE,\n    output = \"text\"\n)\n\nglue::glue(demo_3_llama3_2)\n\nTo determine if 9677 is a prime number, we can check for factors other than 1 and itself.\n\nAfter checking, I found that 9677 is not a prime number. It can be factored into:\n\n9677 = 61 √ó 159\n\nTherefore, 9677 is a composite number, not a prime number.\n\n\n\ndemo_3_mistral &lt;- rollama::query(\n    \"Is 9677 a prime number?\",\n    model = \"mistral\",\n    screen = FALSE,\n    output = \"text\"\n)\n\nglue::glue(demo_3_mistral)\n\n9677 is not a prime number. A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. For 9677, it can be divided evenly by 1, 7, 1381, and 9677, so it does not meet the criteria for being a prime number.\n\n\n\n\n\nSentimentscores mit LLM\n\n# Erstellung einer kleinen Stichprobe\nsubsample &lt;- chats_sentiment %&gt;% \n    filter(message_length &gt; 20 & message_length &lt; 50) %&gt;%\n    slice_sample(n = 10) \n\n# Process each review using make_query\nqueries &lt;- rollama::make_query(\n    text = subsample$message_content,\n    prompt = \"Classify the sentiment of the provided text. Provide a sentiment score ranging from -1 (very negative) to 1 (very positive).\",\n    template = \"{prefix}{text}\\n{prompt}\",\n    system = \"Classify the sentiment of this text. Respond with only a numerical sentiment score.\",\n    prefix = \"Text: \"\n)\n\n# Create sentiment score for different models\nmodels &lt;- c(\"llama3.2\", \"gemma2\", \"mistral\")\nnames &lt;- c(\"llama\", \"gemma\", \"mistral\")\nfor (i in seq_along(models)) {\n  subsample[[names[i]]] &lt;- rollama::query(queries, model = models[i], screen = FALSE, output = \"text\")\n}\n\n\nsubsample %&gt;% \n  select(message_content, polarity, valence, compound, llama, gemma, mistral) %&gt;% \n  gt() \n\n\n\n\n\n\n\nmessage_content\npolarity\nvalence\ncompound\nllama\ngemma\nmistral\n\n\n\n\nit loos like a snapchat filter\n1.098612\n2.0\n0.361\n0.5\n0\n0.35 (Neutral to slightly positive, indicating the text is describing something aesthetically pleasing, but it lacks strong emotional language that would lead to a higher positive sentiment score.)\n\n\npeepoSmile no fed posting\n0.000000\n-1.0\n-0.296\n0\n0.5\n0.25 (Slightly Negative)\n\n\n3 gallons of anything is a lot\n0.000000\n0.0\n0.000\n0\n0\n0.25 (Slightly Positive)\n\n\nbotted the likes award\n1.609438\n2.5\n0.743\n0\n-1\n0.25\n\n\nif valve is bought out by microsoft im done.\n0.000000\n0.0\n0.000\n0.75\n-0.8\n0.35 (Negative)\n\n\nI don‚Äôt think my opponent is a bad person\n-1.609438\n-3.0\n-0.542\n0\n0.33\n0.5 (Neutral to Positive)\n\n\nDid You See The Cult Leader arrested?\n-1.098612\n-3.0\n-0.477\n0\n0\n0 (Neutral)\n\n\npeople hatin on introverts again\n0.000000\n0.0\n0.000\n0\n-0.8\n0.25 (slightly negative)\n\n\nHE KEEPS GOING BACK TO IMMIGRATION\n0.000000\n0.0\n0.000\n0\n-0.8\n0.5 (Neutral or Mixed, as it does not provide clear indications of positive or negative sentiment)\n\n\nbackground actors LMAO. LUL\n0.000000\n4.0\n0.635\n0\n0.6\n0.5 (This text contains humor or amusement, but it doesn't explicitly express strong positive or negative emotions)\n\n\n\n\n\n\n\n\n\nWeitef√ºhrende Analysen\n\nchats_sentiment %&gt;% \n    ggpubr::ggdensity(\n        x = \"compound\",\n        color = \"streamer\"\n    )\n\n\n\n\n\n\n\n\n\n\nExpand for full code\nchats_sentiment %&gt;% \n    mutate(message_length_fct = case_when( \n        message_length &lt;= 7 ~ \"&lt;= 7 words\",\n        message_length &gt; 7 & message_length &lt;= 34 ~ \"8 to 34 words\",\n        message_length &gt;= 34 ~ \"&gt; 34 words\")\n     ) %&gt;%\n    group_by(message_length_fct) %&gt;%\n    mutate(n = n()) %&gt;%\n    ggviolin(\n        x = \"message_length_fct\",\n        y = \"compound\", \n        fill = \"message_length_fct\"\n    ) +\n    stat_summary(\n        fun.data = function(x) data.frame(y = max(x) + 0.15, label = paste0(\"n=\", length(x))),\n        geom = \"text\",\n        size = 3,\n        color = \"black\"\n    ) +\n    labs(\n        x = \"L√§nge der Nachricht\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nBeispiele f√ºr Validierung\n\nchats_sentiment %&gt;% \n    filter(compound &gt;= 0.95) %&gt;% \n    arrange(desc(compound)) %&gt;% \n    select(message_content, compound) %&gt;% \n    head(n = 3) %&gt;% \n    gt() %&gt;% gtExtras::gt_theme_538()\n\n\n\n\n\n\n\nmessage_content\ncompound\n\n\n\n\nmizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3\n0.997\n\n\nhasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY\n0.996\n\n\nhasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY\n0.996\n\n\n\n\n\n\n\n\nchats_sentiment %&gt;% \n    filter(compound &lt;= -0.95) %&gt;% \n    arrange(compound) %&gt;% \n    select(message_content, compound) %&gt;% \n    head(n = 3) %&gt;% \n    gt() %&gt;% gtExtras::gt_theme_538()\n\n\n\n\n\n\n\nmessage_content\ncompound\n\n\n\n\npepeMeltdown OH SHIT MY OIL FUCK FUCK FUCK pepeMeltdown OH SHIT MY OIL FUCK FUCK FUCK pepeMeltdown OH SHIT MY OIL FUCK FUCK FUCK pepeMeltdown OH SHIT MY OIL FUCK FUCK FUCK\n-0.997\n\n\nbut why kill more birdsbut why kill more birdsbut why kill more birdsbut why kill more birdsbut why kill more birdsbut why kill more birdsbut why kill more birds\n-0.996\n\n\ncry bully ass losers KEKL cry bully ass losers KEKL cry bully ass losers KEKL cry bully ass losers KEKL cry bully ass losers KEKL\n-0.996\n\n\n\n\n\n\n\n\nchats_sentiment %&gt;% \n    filter(compound &gt;= 0.95) %&gt;% \n    sjmisc::frq(\n        user_name, \n        min.frq = 5,\n        sort.frq = \"desc\")\n\nuser_name &lt;character&gt; \n# total N=289 valid N=289 mean=85.70 sd=50.72\n\nValue                |   N | Raw % | Valid % | Cum. %\n-----------------------------------------------------\nnotilandefinitelynot |  17 |  5.88 |    5.88 |   5.88\ndirty_barn_owl       |  16 |  5.54 |    5.54 |  11.42\naliisontw1tch        |   7 |  2.42 |    2.42 |  13.84\nomnivalor            |   7 |  2.42 |    2.42 |  16.26\nx7yz42               |   6 |  2.08 |    2.08 |  18.34\nchakek1993414        |   5 |  1.73 |    1.73 |  20.07\ndoortoratworld       |   5 |  1.73 |    1.73 |  21.80\nmuon_2ms             |   5 |  1.73 |    1.73 |  23.53\nn &lt; 5                | 221 | 76.47 |   76.47 | 100.00\n&lt;NA&gt;                 |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;"
  },
  {
    "objectID": "tutorials/tutorial-10.html#exkurs-machine-learning",
    "href": "tutorials/tutorial-10.html#exkurs-machine-learning",
    "title": "üî® Sentiment Analysis with R",
    "section": "Exkurs: Machine Learning",
    "text": "Exkurs: Machine Learning\n\n\n\n\n\n\nImportant information\n\n\n\n\nThe following code chunks were not part of the session or the slides.\nBased on the blog post (with screencast) by Julia Silge, the following sections exemplify the implementation of sentiment analysis using the tidymodels package.\n\n\n\n\nExtract data\n\nchats_tidymodels &lt;- chats_sentiment %&gt;% \n    mutate(\n        rating = case_when(\n            compound &gt; 0.5 ~ \"positive\",\n            compound &lt; -0.5 ~ \"negative\",\n            TRUE ~ \"neutral\"), \n        word_count = str_count(message_content, \"\\\\S+\")\n    ) %&gt;% \n    filter(rating != \"neutral\")\n\n\n# Distribution of compound sentiment scores\nchats_tidymodels %&gt;% \n    ggplot(aes(x = compound)) +\n        geom_histogram(binwidth = 0.1, fill = \"lightblue\", color = \"darkblue\") +\n        labs(\n            title = \"Distribution of Compound Sentiment Scores\",\n            x = \"Compound Sentiment\",\n            y = \"Frequency\") +\n    theme_minimal()   \n\n\n\n\n\n\n\n# Distribution of word count\nchats_tidymodels %&gt;% \n    ggplot(aes(word_count)) +\n    geom_histogram(fill = \"midnightblue\", alpha = 0.8) +\n    theme_minimal()\n\n\n\n\n\n\n\nchats_tidymodels %&gt;% \n    datawizard::describe_distribution(word_count)\n\nVariable   | Mean |   SD | IQR |         Range | Skewness | Kurtosis |      n | n_Missing\n-----------------------------------------------------------------------------------------\nword_count | 7.87 | 7.75 |   7 | [1.00, 85.00] |     2.60 |    10.33 | 137662 |         0\n\n\n\n\nBuild model\n\nlibrary(tidymodels)\n\nset.seed(42)\nchats_rating_splits &lt;- initial_split(chats_tidymodels, strata = rating)\nchats_ratings_train &lt;- training(chats_rating_splits)\nchats_ratings_test &lt;- testing(chats_rating_splits)\n\n\nlibrary(textrecipes)\n\nchats_ratings_rec &lt;- recipe(rating ~ message_content, data = chats_ratings_train) %&gt;% \n    step_tokenize(message_content) %&gt;% \n    step_stopwords(message_content) %&gt;%\n    step_tokenfilter(message_content, max_tokens = 1000) %&gt;%\n    step_tfidf(message_content) %&gt;% \n    step_normalize(all_predictors()) \n\nchats_ratings_prep &lt;- prep(chats_ratings_rec)\n\n\nlasso_spec &lt;- logistic_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\nlasso_wf &lt;- workflow() %&gt;%\n  add_recipe(chats_ratings_rec) %&gt;%\n  add_model(lasso_spec)\n\nlasso_wf\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: logistic_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n5 Recipe Steps\n\n‚Ä¢ step_tokenize()\n‚Ä¢ step_stopwords()\n‚Ä¢ step_tokenfilter()\n‚Ä¢ step_tfidf()\n‚Ä¢ step_normalize()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\n\nTune model parameters\n\nlambda_grid &lt;- grid_regular(penalty(), levels = 40)\n\nset.seed(42)\nchats_ratings_folds &lt;- bootstraps(chats_ratings_train, strata = rating)\nchats_ratings_folds\n\n# Bootstrap sampling using stratification \n# A tibble: 25 √ó 2\n   splits                 id         \n   &lt;list&gt;                 &lt;chr&gt;      \n 1 &lt;split [103246/37925]&gt; Bootstrap01\n 2 &lt;split [103246/38017]&gt; Bootstrap02\n 3 &lt;split [103246/38102]&gt; Bootstrap03\n 4 &lt;split [103246/37913]&gt; Bootstrap04\n 5 &lt;split [103246/37915]&gt; Bootstrap05\n 6 &lt;split [103246/38017]&gt; Bootstrap06\n 7 &lt;split [103246/38050]&gt; Bootstrap07\n 8 &lt;split [103246/37819]&gt; Bootstrap08\n 9 &lt;split [103246/37900]&gt; Bootstrap09\n10 &lt;split [103246/38085]&gt; Bootstrap10\n# ‚Ñπ 15 more rows\n\n\n\ndoParallel::registerDoParallel()\n\nset.seed(2020)\nlasso_grid &lt;- tune_grid( \n  lasso_wf,\n  resamples = chats_ratings_folds,\n  grid = lambda_grid,\n  metrics = metric_set(roc_auc, ppv, npv)\n)\n\n\nlasso_grid %&gt;%\n  collect_metrics()\n\n# A tibble: 120 √ó 7\n    penalty .metric .estimator  mean     n  std_err .config              \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                \n 1 1   e-10 npv     binary     0.952    25 0.000320 Preprocessor1_Model01\n 2 1   e-10 ppv     binary     0.931    25 0.000324 Preprocessor1_Model01\n 3 1   e-10 roc_auc binary     0.982    25 0.000116 Preprocessor1_Model01\n 4 1.80e-10 npv     binary     0.952    25 0.000320 Preprocessor1_Model02\n 5 1.80e-10 ppv     binary     0.931    25 0.000324 Preprocessor1_Model02\n 6 1.80e-10 roc_auc binary     0.982    25 0.000116 Preprocessor1_Model02\n 7 3.26e-10 npv     binary     0.952    25 0.000320 Preprocessor1_Model03\n 8 3.26e-10 ppv     binary     0.931    25 0.000324 Preprocessor1_Model03\n 9 3.26e-10 roc_auc binary     0.982    25 0.000116 Preprocessor1_Model03\n10 5.88e-10 npv     binary     0.952    25 0.000320 Preprocessor1_Model04\n# ‚Ñπ 110 more rows\n\n\n\nlasso_grid %&gt;%\n  collect_metrics() %&gt;%\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_line(linewidth = 1.5, show.legend = FALSE) +\n  facet_wrap(~.metric) +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nChoose the final model\n\nbest_auc &lt;- lasso_grid %&gt;% select_best(metric = \"roc_auc\")\nbest_auc\n\n# A tibble: 1 √ó 2\n   penalty .config              \n     &lt;dbl&gt; &lt;chr&gt;                \n1 0.000464 Preprocessor1_Model27\n\n\n\nfinal_lasso &lt;- finalize_workflow(lasso_wf, best_auc)\nfinal_lasso\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: logistic_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n5 Recipe Steps\n\n‚Ä¢ step_tokenize()\n‚Ä¢ step_stopwords()\n‚Ä¢ step_tokenfilter()\n‚Ä¢ step_tfidf()\n‚Ä¢ step_normalize()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.000464158883361277\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\nlibrary(vip)\n\nfinal_lasso %&gt;%\n  fit(chats_ratings_train) %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vi(lambda = best_auc$penalty) %&gt;%\n  group_by(Sign) %&gt;%\n  top_n(20, wt = abs(Importance)) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    Importance = abs(Importance),\n    Variable = str_remove(Variable, \"tfidf_message_content_\"),\n    Variable = fct_reorder(Variable, Importance)\n  ) %&gt;%\n  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~Sign, scales = \"free_y\") +\n  labs(y = NULL)\n\n\n\n\n\n\n\n\n\nchats_ratings_final &lt;- last_fit(final_lasso, chats_rating_splits)\nchats_ratings_final %&gt;%\n  collect_metrics()\n\n# A tibble: 3 √ó 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary        0.947  Preprocessor1_Model1\n2 roc_auc     binary        0.984  Preprocessor1_Model1\n3 brier_class binary        0.0433 Preprocessor1_Model1\n\n\n\nchats_ratings_final %&gt;% \n  collect_predictions() %&gt;%\n  conf_mat(rating, .pred_class)\n\n          Truth\nPrediction negative positive\n  negative    17103     1269\n  positive      560    15484"
  },
  {
    "objectID": "tutorials/tutorial-09.html",
    "href": "tutorials/tutorial-09.html",
    "title": "üî® Topic Modeling in R",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "tutorials/tutorial-09.html#background",
    "href": "tutorials/tutorial-09.html#background",
    "title": "üî® Topic Modeling in R",
    "section": "Background",
    "text": "Background"
  },
  {
    "objectID": "tutorials/tutorial-09.html#preparation",
    "href": "tutorials/tutorial-09.html#preparation",
    "title": "üî® Topic Modeling in R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, \n    magrittr, janitor,\n    ggpubr, \n    gt, gtExtras,\n    countdown, \n    quanteda, # quanteda text processing\n    quanteda.textplots, quanteda.textstats, quanteda.textmodels,\n    tidytext, \n    udpipe, spacyr, # POS tagging\n    stm, stminsights,\n    easystats, tidyverse\n)\n\n\n# Import base data\nchats &lt;- qs::qread(here(\"local_data/chat-debates_full.qs\"))$correct\n\n# Import corpora\nchats_spacyr &lt;- qs::qread(here(\"local_data/chat-corpus_spacyr.qs\"))\nstm_search &lt;- qs::qread(here(\"local_data/stm-majority_report-search.qs\"))\nstm_results &lt;- qs::qread(here(\"local_data/stm-majority_report-results.qs\"))"
  },
  {
    "objectID": "tutorials/tutorial-09.html#codechunks-aus-der-sitzung",
    "href": "tutorials/tutorial-09.html#codechunks-aus-der-sitzung",
    "title": "üî® Topic Modeling in R",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nVorverarbeitung der Daten\n\nchats_valid &lt;- chats %&gt;% \n  mutate(\n    across(c(debate, platform), ~as.factor(.x))\n  ) \n\n\n\nVorverarbeitung des Korpus\n\n# spacyr-Korpus zu Tokens\nchat_spacyr_toks &lt;- chats_spacyr %&gt;% \n  as.tokens(\n    use_lemma = TRUE\n  ) %&gt;% \n  tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = FALSE,\n    remove_url = FALSE, \n    split_hyphens = FALSE,\n    split_tags = FALSE,\n  ) %&gt;% \n  tokens_remove(\n    pattern = stopwords(\"en\")\n  ) %&gt;% \n  tokens_ngrams(n = 1:3) \n\n\n\nAdd docvars\n\n# Get document names from the original data\noriginal_docnames &lt;- chats$message_id\n\n# Get document names from the tokens object\ntoken_docnames &lt;- docnames(chat_spacyr_toks)\n\n# Identify missing documents\nmissing_docs &lt;- setdiff(original_docnames, token_docnames)\n\n# Exclude \"empty\" messages\nchats_filtered &lt;- chats_valid %&gt;% \n  filter(!message_id %in% missing_docs)\n\n# Add docvars\ndocvars(chat_spacyr_toks) &lt;- chats_filtered\n\n\n\nFokus on The Majority Report\n\n# Subset tokens based on docvars\nmajority_report_chat_toks &lt;- tokens_subset(\n  chat_spacyr_toks, streamer == \"the_majority_report\")\n\n\n# Convert to DFM\nmajority_report_chat_dfm &lt;- majority_report_chat_toks %&gt;% \n  dfm()\n\n# Pruning\nmajority_report_chat_trim &lt;- majority_report_chat_dfm %&gt;% \n    dfm_trim(\n        min_docfreq = 50/nrow(chats),\n        max_docfreq = 0.99, \n        docfreq_type = \"prop\"\n   )\n\n# Convert for stm topic modeling\nmajority_report_chat_stm &lt;- majority_report_chat_trim %&gt;% \n   convert(to = \"stm\")\n\n\n\nEstimate models\n\n# Define parameters\nfuture::plan(future::multisession()) # use multiple sessions\ntopic_range &lt;- seq(from = 4, to = 20, by = 2) \n\n# Initiate notifications & time tracking\ntictoc::tic(\"STM chats - Majority Report\")\n\n# Estimate models\nstm_search  &lt;- tibble(k = topic_range) %&gt;%\n    mutate(\n        mdl = furrr::future_map(\n            k, \n            ~stm::stm(\n                documents = majority_report_chat_stm$documents,\n                vocab = majority_report_chat_stm$vocab, \n                prevalence =~ platform + debate + message_during_debate, \n                K = ., \n                seed = 42,\n                max.em.its = 1000,\n                data = majority_report_chat_stm$meta,\n                init.type = \"Spectral\",\n                verbose = TRUE),\n            .options = furrr::furrr_options(seed = 42)\n            )\n    )\n\n# Sent status update and finish time tracking\ntictoc::toc(log = TRUE)\n\n\n# Create heldout\nheldout &lt;- make.heldout(\n  majority_report_chat_stm$documents,\n  majority_report_chat_stm$vocab,\n  seed = 42)\n\n# Create model diagnostics\nstm_results &lt;- stm_search %&gt;%\n  mutate(\n    exclusivity = map(mdl, exclusivity),\n    semantic_coherence = map(mdl, semanticCoherence, majority_report_chat_stm$documents),\n    eval_heldout = map(mdl, eval.heldout, heldout$missing),\n    residual = map(mdl, checkResiduals, majority_report_chat_stm$documents),\n    bound =  map_dbl(mdl, function(x) max(x$convergence$bound)),\n    lfact = map_dbl(mdl, function(x) lfactorial(x$settings$dim$K)),\n    lbound = bound + lfact,\n    iterations = map_dbl(mdl, function(x) length(x$convergence$bound))\n    )"
  },
  {
    "objectID": "tutorials/tutorial-09.html#vergleich-des-statistischen-fits",
    "href": "tutorials/tutorial-09.html#vergleich-des-statistischen-fits",
    "title": "üî® Topic Modeling in R",
    "section": "Vergleich des statistischen Fits",
    "text": "Vergleich des statistischen Fits\n\nstm_results %&gt;%\n  transmute(\n    k,\n    `Lower bound` = lbound,\n    Residuals = map_dbl(residual, \"dispersion\"),\n    `Semantic coherence` = map_dbl(semantic_coherence, mean),\n    `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\")) %&gt;%\n  gather(Metric, Value, -k) %&gt;%\n  ggplot(aes(k, Value, color = Metric)) +\n    geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +\n    geom_point(size = 3) +\n    scale_x_continuous(breaks = seq(from = 4, to = 20, by = 2)) +\n    facet_wrap(~Metric, scales = \"free_y\") +\n    labs(x = \"K (Anzahl der Themen)\",\n         y = NULL,\n         title = \"Statistischer Fit der STM-Modelle\",\n         subtitle = \"Koh√§renz sollte hoch, Residuen niedrig sein\"\n    ) +\n    theme_pubr()"
  },
  {
    "objectID": "tutorials/tutorial-09.html#hohe-koh√§renz-bei-hoher-exklusivit√§t",
    "href": "tutorials/tutorial-09.html#hohe-koh√§renz-bei-hoher-exklusivit√§t",
    "title": "üî® Topic Modeling in R",
    "section": "Hohe Koh√§renz bei hoher Exklusivit√§t",
    "text": "Hohe Koh√§renz bei hoher Exklusivit√§t\n\n# Models for comparison\nmodels_for_comparison = c(12, 14, 18)\n\n# Create figures\nfig_excl &lt;- stm_results %&gt;% \n  # Edit data\n  select(k, exclusivity, semantic_coherence) %&gt;%\n  filter(k %in% models_for_comparison) %&gt;%\n  unnest(cols = c(exclusivity, semantic_coherence))  %&gt;%\n  mutate(k = as.factor(k)) %&gt;%\n  # Build graph\n  ggplot(aes(semantic_coherence, exclusivity, color = k)) +\n    geom_point(size = 2, alpha = 0.7) +\n    labs(\n      x = \"Semantic coherence\",\n      y = \"Exclusivity\"\n      # title = \"Comparing exclusivity and semantic coherence\",\n      # subtitle = \"Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity\"\n      ) +\n      theme_pubr()  \n\n# Create plotly\nfig_excl %&gt;% plotly::ggplotly()"
  },
  {
    "objectID": "tutorials/tutorial-09.html#extraktion-der-beta--gamma-matrix",
    "href": "tutorials/tutorial-09.html#extraktion-der-beta--gamma-matrix",
    "title": "üî® Topic Modeling in R",
    "section": "Extraktion der Beta- & Gamma-Matrix",
    "text": "Extraktion der Beta- & Gamma-Matrix\n\n# Define model\ntpm_k14 &lt;- stm_results %&gt;% \n   filter(k == 14) |&gt; \n   pull(mdl) %&gt;% .[[1]]\n\n# Beta matrix\ntpm_k14 %&gt;% \n  tidy(., matrix = \"frex\") \n\n# A tibble: 154,490 √ó 2\n   topic term                \n   &lt;int&gt; &lt;chr&gt;               \n 1     1 look_like           \n 2     1 look_like_go        \n 3     1 like_go             \n 4     1 hahahahahahaha      \n 5     1 look_like_s         \n 6     1 think_go            \n 7     1 check_moderator     \n 8     1 fact_check_moderator\n 9     1 moderator_fact      \n10     1 moderator_fact_check\n# ‚Ñπ 154,480 more rows\n\n# Gamma matrix\ntpm_k14 %&gt;% \n  tidy(.,matrix = \"gamma\", \n    document_names = names(majority_report_chat_stm$documents)\n    ) \n\n# A tibble: 322,840 √ó 3\n   document                                 topic   gamma\n   &lt;chr&gt;                                    &lt;int&gt;   &lt;dbl&gt;\n 1 ChwKGkNJR2poT3pVdVlnREZha1FyUVlkblNrWS1B     1 0.0261 \n 2 ChwKGkNLbXd3LXpVdVlnREZWd0wxZ0FkYW9FSWdB     1 0.0265 \n 3 ChwKGkNKR1RsdV9VdVlnREZkNFhyUVlkZ2d3Tk5n     1 0.0123 \n 4 ChwKGkNOQ3kxUExVdVlnREZVS1k1UWNkQ0t3Mlhn     1 0.0200 \n 5 ChwKGkNPcW5fZkxVdVlnREZlSFJsQWtkbThZaUtR     1 0.0232 \n 6 ChwKGkNNUHZzdlhVdVlnREZha1FyUVlkblNrWS1B     1 0.0236 \n 7 ChwKGkNLT1JuX2pVdVlnREZZX0FsQWtkcEw4Wmd3     1 0.434  \n 8 ChwKGkNLRElvZmpVdVlnREZaWExGZ2tkTy1ZSXVR     1 0.0118 \n 9 ChwKGkNNblNqZm5VdVlnREZhX0l3Z1FkZUg0bHZn     1 0.0356 \n10 ChwKGkNMeUkyUHZVdVlnREZXQUhyUVlkTUJvZ193     1 0.00307\n# ‚Ñπ 322,830 more rows"
  },
  {
    "objectID": "tutorials/tutorial-09.html#extraktion-der-top-features-nach-thema",
    "href": "tutorials/tutorial-09.html#extraktion-der-top-features-nach-thema",
    "title": "üî® Topic Modeling in R",
    "section": "Extraktion der Top Features nach Thema",
    "text": "Extraktion der Top Features nach Thema\n\n# Create gamma data\ntop_gamma_k14 &lt;- tpm_k14 %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::summarise(\n    gamma = mean(gamma),\n    .groups = \"drop\") %&gt;%\n  dplyr::arrange(desc(gamma))\n\n# Create beta data\ntop_beta_k14 &lt;- tpm_k14 %&gt;%\n  tidytext::tidy(.) %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::arrange(-beta) %&gt;%\n  dplyr::top_n(7, wt = beta) %&gt;% \n  dplyr::select(topic, term) %&gt;%\n  dplyr::summarise(\n    terms_beta = toString(term),\n    .groups = \"drop\")\n\n# Merge gamma & beta data\ntop_topics_terms_k14 &lt;- top_beta_k14 %&gt;% \n  dplyr::left_join(\n    top_gamma_k14, \n    by = \"topic\") %&gt;%\n  dplyr::mutate(\n          topic = paste0(\"Topic \", topic),\n          topic = reorder(topic, gamma)\n      )\n\n# Create output\ntop_topics_terms_k14 %&gt;%\n  mutate(across(gamma, ~round(.,3))) %&gt;% \n  dplyr::arrange(-gamma) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(\n    table.width = gt::pct(100), \n    table.font.size = \"12px\"\n    )\n\n\n\n\n\n\n\ntopic\nterms_beta\ngamma\n\n\n\n\nTopic 8\nmake, 's, lul, emma, fuchsia, liar, kekl\n0.115\n\n\nTopic 7\ngood, right, now, yes, plan, lie, bad\n0.113\n\n\nTopic 12\nkamala, want, biden, eat, take, vote, god\n0.109\n\n\nTopic 5\nget, s, wow, mad, omg, thank, nice\n0.101\n\n\nTopic 4\nlmao, omegalul, red, green, orange, baby, kekw\n0.079\n\n\nTopic 3\ntime, sam, love, man, need, old, big\n0.075\n\n\nTopic 11\nsay, oh, ..., know, look, shit, yeah\n0.075\n\n\nTopic 1\ngo, like, fact, debate, look, check, keep\n0.058\n\n\nTopic 13\ntrump, just, donald, lose, racist, win, can\n0.056\n\n\nTopic 9\nlol, one, give, ...., wtf, china, okay\n0.051\n\n\nTopic 6\npeople, think, go, back, work, try, change\n0.050\n\n\nTopic 10\nlet, talk, ‚Äôs, can, like, sound, see\n0.045\n\n\nTopic 2\nstop, start, please, israel, use, laugh, agree\n0.039\n\n\nTopic 14\nface, guy, don, bring, real, country, rolling_on_the_floor_laughe\n0.034"
  },
  {
    "objectID": "tutorials/tutorial-09.html#extraktion-zusammenf√ºhrung-der-daten",
    "href": "tutorials/tutorial-09.html#extraktion-zusammenf√ºhrung-der-daten",
    "title": "üî® Topic Modeling in R",
    "section": "Extraktion & Zusammenf√ºhrung der Daten",
    "text": "Extraktion & Zusammenf√ºhrung der Daten\n\n# Prepare for merging\ntopic_gammas_k14 &lt;- tpm_k14 %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(document) %&gt;% \n  tidyr::pivot_wider(\n    id_cols = document, \n    names_from = \"topic\", \n    names_prefix = \"gamma_topic_\",\n    values_from = \"gamma\")\n      \ngammas_k14 &lt;- tpm_k14 %&gt;%\n  tidytext::tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(document) %&gt;% \n  dplyr::slice_max(gamma) %&gt;% \n  dplyr::mutate(\n    main_topic = ifelse(\n      gamma &gt; 0.5, topic, NA)) %&gt;% \n  rename(\n    top_topic = topic,\n    top_gamma = gamma) %&gt;% \n  ungroup() %&gt;% \n  left_join(.,\n    topic_gammas_k14,\n    by = join_by(document))\n\n# Identify empty documents\nempty_docs &lt;- Matrix::rowSums(\n  as(majority_report_chat_trim, \"Matrix\")) == 0 \nempty_docs_ids &lt;- majority_report_chat_trim@docvars$docname[empty_docs]\n\n# Merge with original data\nchats_topics &lt;- chats_filtered %&gt;%\n  filter(!(message_id %in% empty_docs_ids)) %&gt;% \n  filter(streamer == \"the_majority_report\") %&gt;%   \n  bind_cols(gammas_k14) %&gt;% \n  select(-document)   \n\n# Preview\nchats_topics %&gt;% glimpse\n\nRows: 23,060\nColumns: 50\n$ streamer              &lt;chr&gt; \"the_majority_report\", \"the_majority_report\", \"t‚Ä¶\n$ url                   &lt;chr&gt; \"https://www.youtube.com/watch?v=lzobJil9Sgc\", \"‚Ä¶\n$ platform              &lt;fct&gt; youtube, youtube, youtube, youtube, youtube, you‚Ä¶\n$ debate                &lt;fct&gt; presidential, presidential, presidential, presid‚Ä¶\n$ user_name             &lt;chr&gt; \"Scott Plant\", \"Rebecca W\", \"Galactic News Netwo‚Ä¶\n$ user_id               &lt;chr&gt; \"UC4mxlnk193JrXVAp6K-vEpQ\", \"UCeenHJ1v62biyOyKwL‚Ä¶\n$ user_display_name     &lt;chr&gt; \"Scott Plant\", \"Rebecca W\", \"Galactic News Netwo‚Ä¶\n$ user_badges           &lt;list&gt; [], [], [], [], [], [], [], [], [], [], [], [],‚Ä¶\n$ message_timestamp     &lt;dbl&gt; -152, -151, -145, -138, -137, -132, -126, -126, ‚Ä¶\n$ message_id            &lt;chr&gt; \"ChwKGkNJR2poT3pVdVlnREZha1FyUVlkblNrWS1B\", \"Chw‚Ä¶\n$ message_type          &lt;chr&gt; \"text_message\", \"text_message\", \"text_message\", ‚Ä¶\n$ message_content       &lt;chr&gt; \"Donnie will say, \\\"That is my own sperm.\\\"\", \"w‚Ä¶\n$ message_emotes        &lt;list&gt; [], [], [[\"UCkszU2WH9gy1mb0dV-11UJg/ssIfY7OFG5O‚Ä¶\n$ message_length        &lt;int&gt; 40, 45, 52, 38, 10, 32, 8, 14, 2, 90, 20, 36, 20‚Ä¶\n$ message_timecode      &lt;Period&gt; -2M -32S, -2M -31S, -2M -25S, -2M -18S, -2M -‚Ä¶\n$ message_time          &lt;chr&gt; \"23:57:28\", \"23:57:29\", \"23:57:35\", \"23:57:42\", ‚Ä¶\n$ message_during_debate &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_has_badge        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_premium       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_subscriber    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_turbo         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_moderator     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_partner       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_subgifter     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_broadcaster   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_vip           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_twitchdj      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_founder       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_staff         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_game_dev      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_is_ambassador    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_no_audio         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ user_no_video         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ top_topic             &lt;int&gt; 11, 7, 4, 3, 4, 3, 1, 4, 9, 4, 8, 3, 1, 1, 3, 13‚Ä¶\n$ top_gamma             &lt;dbl&gt; 0.4435422, 0.3412468, 0.7627751, 0.5663056, 0.46‚Ä¶\n$ main_topic            &lt;int&gt; NA, NA, 4, 3, NA, NA, NA, 4, NA, 4, NA, NA, NA, ‚Ä¶\n$ gamma_topic_1         &lt;dbl&gt; 0.026098022, 0.026450828, 0.012260893, 0.0200424‚Ä¶\n$ gamma_topic_2         &lt;dbl&gt; 0.014058480, 0.016014175, 0.006519458, 0.1322111‚Ä¶\n$ gamma_topic_3         &lt;dbl&gt; 0.043655546, 0.066001729, 0.018185091, 0.5663056‚Ä¶\n$ gamma_topic_4         &lt;dbl&gt; 0.03876696, 0.14976529, 0.76277514, 0.03011074, ‚Ä¶\n$ gamma_topic_5         &lt;dbl&gt; 0.186801763, 0.043801244, 0.020095565, 0.0373451‚Ä¶\n$ gamma_topic_6         &lt;dbl&gt; 0.021470740, 0.024622665, 0.009041711, 0.0174135‚Ä¶\n$ gamma_topic_7         &lt;dbl&gt; 0.036282513, 0.341246826, 0.017882159, 0.0289225‚Ä¶\n$ gamma_topic_8         &lt;dbl&gt; 0.04538521, 0.14311198, 0.06168206, 0.03558740, ‚Ä¶\n$ gamma_topic_9         &lt;dbl&gt; 0.021910232, 0.023552979, 0.012143933, 0.0165138‚Ä¶\n$ gamma_topic_10        &lt;dbl&gt; 0.020656194, 0.020843309, 0.013562820, 0.0161670‚Ä¶\n$ gamma_topic_11        &lt;dbl&gt; 0.443542243, 0.027314995, 0.019249172, 0.0206369‚Ä¶\n$ gamma_topic_12        &lt;dbl&gt; 0.044397591, 0.044680183, 0.019709728, 0.0343713‚Ä¶\n$ gamma_topic_13        &lt;dbl&gt; 0.027631227, 0.037326873, 0.011473224, 0.0214522‚Ä¶\n$ gamma_topic_14        &lt;dbl&gt; 0.029343282, 0.035266919, 0.015419046, 0.0229199‚Ä¶"
  },
  {
    "objectID": "tutorials/tutorial-09.html#themen-im-fokus",
    "href": "tutorials/tutorial-09.html#themen-im-fokus",
    "title": "üî® Topic Modeling in R",
    "section": "Themen im Fokus",
    "text": "Themen im Fokus\n\nTop Topic\n\nchats_topics %&gt;% \n  filter(top_topic == 8) %&gt;% \n  arrange(-top_gamma) %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(message_id, user_name, message_time, message_content, top_gamma, top_topic) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(table.font.size = \"10px\")\n\n\n\n\n\n\n\nmessage_id\nuser_name\nmessage_time\nmessage_content\ntop_gamma\ntop_topic\n\n\n\n\nChwKGkNKdlRqY1BwdVlnREZRREV3Z1FkV2I4U1hn\nDavid Davis\n01:29:52\n:watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon:\n0.9628609\n8\n\n\nChwKGkNLMlp3cUxzdVlnREZVN0NsQWtkT0JBRTN3\nJules Winnfeild üè≥Ô∏è‚Äç‚ößÔ∏è\n01:42:09\n:face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape:\n0.9583042\n8\n\n\nChwKGkNNclk0dFBZdVlnREZZYWg1UWNkUlhvNVB3\nCanalEduge\n00:14:24\n:face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape:\n0.9437816\n8\n\n\nChwKGkNJaVdpNVBmdVlnREZTV1Q1UWNkUWg0dEJn\nJules Winnfeild üè≥Ô∏è‚Äç‚ößÔ∏è\n00:43:27\n:face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape:\n0.9315330\n8\n\n\nChwKGkNORG1uTWpvdVlnREZkd3VyUVlkSVFrVU5R\n#BobbleHead\n01:25:34\nWORLDSTAR own's the Trademark on the Algorithm that identified all the Pedophiles = Blame T.M.Z. #ReleaseTheBlackBaby\n0.9313871\n8\n\n\nChwKGkNPV09fYmJwdVlnREZXc3ByUVlkbk9Vc3d3\nDavid Davis\n01:29:26\n:watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon:\n0.9303014\n8\n\n\nChwKGkNMV244NkRvdVlnREZUMFRyUVlkYmZzUmpB\n#BobbleHead\n01:24:12\nWORLDSTAR own's the Trademark on the Algorithm that identified all the Pedophiles = Blame T.M.Z.\n0.9287878\n8\n\n\nChwKGkNLN0N4ZTdldVlnREZWNDZyUVlkRmZJRklR\nCorporations8MyBaby\n00:42:10\n:face-fuchsia-tongue-out::face-fuchsia-tongue-out::face-fuchsia-tongue-out::face-fuchsia-tongue-out:\n0.9269656\n8\n\n\n9c014ab4-89a7-4f9d-97c8-be3da2868f58\nnightbot\n00:10:47\nJoin the official Majority Report discord community! https://discord.gg/majority\n0.9215614\n8\n\n\nfcb53a8b-4b75-4557-b3eb-d273b7069d88\nnightbot\n00:26:14\nJoin the official Majority Report discord community! https://discord.gg/majority\n0.9215614\n8\n\n\n\n\n\n\n\n\n\nThema 12\n\nchats_topics %&gt;% \n  filter(top_topic == 12) %&gt;% \n  arrange(-top_gamma) %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(message_id, user_name, message_time, message_content, top_gamma, top_topic) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(table.font.size = \"10px\")\n\n\n\n\n\n\n\nmessage_id\nuser_name\nmessage_time\nmessage_content\ntop_gamma\ntop_topic\n\n\n\n\nChwKGkNNZXg5LUxxdVlnREZkcVc1UWNkeGpNTDJB\nSamSedersLeftTeste\n01:35:27\nThe vice president is BLACK BLACK BLACK BLACK BLACK BLACK\n0.9227423\n12\n\n\nChwKGkNNdVU0NTNndVlnREZkNEwxZ0FkbWxFSFN3\nRilly Kewl\n00:48:18\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNJU1BtS0RndVlnREZRREV3Z1FkV2I4U1hn\nRilly Kewl\n00:48:23\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNJS1NvcVRndVlnREZRMHUxZ0FkU1FFSzZB\nRilly Kewl\n00:48:31\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNLSG9uYUxodVlnREZhY0cxZ0FkSVJjSGdB\nRilly Kewl\n00:52:56\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNPbU90YV9odVlnREZWZ3FyUVlkaUpnNUpn\nRilly Kewl\n00:53:23\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNLYmxuZjdzdVlnREZiMHUxZ0FkT0owN0h3\nRilly Kewl\n01:45:21\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNLX3F4cl90dVlnREZWbzAxZ0FkdzVFTTR3\nRilly Kewl\n01:47:38\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNOYm1yTVB0dVlnREZWb0gxZ0FkQnF3QWRR\nRilly Kewl\n01:47:46\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nbad4de96-6c3f-4495-9bd5-da395d9af90b\ngrandshadowfox\n01:07:37\nGrandshadowfox subscribed with Prime. They've subscribed for 16 months! 15 months\n0.9064436\n12\n\n\n\n\n\n\n\n\n\nThema 4\n\nchats_topics %&gt;% \n  filter(top_topic == 4) %&gt;% \n  arrange(-top_gamma) %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(message_id, user_name, message_time, message_content, top_gamma, top_topic) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(table.font.size = \"10px\")\n\n\n\n\n\n\n\nmessage_id\nuser_name\nmessage_time\nmessage_content\ntop_gamma\ntop_topic\n\n\n\n\nChwKGkNLX3Z3SzdkdVlnREZhb0NyUVlkVER3aVRn\nrhys\n00:35:28\n:text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over:\n0.9687024\n4\n\n\nChwKGkNJN1hsSXJudVlnREZWTGNGZ2tkYnFnYmJB\nJules Winnfeild üè≥Ô∏è‚Äç‚ößÔ∏è\n01:18:56\n:fish-orange-wide-eyes::fish-orange-wide-eyes::fish-orange-wide-eyes::fish-orange-wide-eyes::fish-orange-wide-eyes:\n0.9649579\n4\n\n\nChwKGkNJQ3ZzYTdXdVlnREZSek1GZ2tkMndnZ1Bn\nfish Monger\n00:04:09\nideas:finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one:\n0.9647715\n4\n\n\nChwKGkNNcmVxb0RmdVlnREZhb0NyUVlkVER3aVRn\nrhys\n00:42:48\n:text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over:\n0.9630820\n4\n\n\n32d36382-5eaf-4da6-a2dc-c9683b98162b\nnightbot\n00:01:27\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\n4ffbae78-db39-40e9-bcf8-b5c0965fe2a4\nnightbot\n00:09:42\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\na08570c3-f835-4568-9332-b97bf22ee61b\nnightbot\n02:01:22\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\n46b82320-e59d-486e-a58f-acf35b03fe4a\nnightbot\n02:09:43\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\n191d1514-cc7e-4a65-8c9e-0ce5d28f1a5d\nnightbot\n02:22:30\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\ned759097-6071-4394-b810-5adafd52f652\nnightbot\n02:35:23\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4"
  },
  {
    "objectID": "tutorials/tutorial-09.html#user-mit-den-meisten-beitr√§gen-zu-thema-4",
    "href": "tutorials/tutorial-09.html#user-mit-den-meisten-beitr√§gen-zu-thema-4",
    "title": "üî® Topic Modeling in R",
    "section": "User mit den meisten Beitr√§gen zu Thema 4",
    "text": "User mit den meisten Beitr√§gen zu Thema 4\n\nchats_topics %&gt;% \n  filter(top_topic == 8) %&gt;% \n  count(user_name, sort = TRUE) %&gt;% \n  mutate(\n    prop = round(n/sum(n)*100, 2)) %&gt;% \n  slice_head(n = 10) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() \n\n\n\n\n\n\n\nuser_name\nn\nprop\n\n\n\n\nbuuuuuuuuuuuuuuuuuuuuuut\n59\n1.83\n\n\nsauvignoncitizen\n50\n1.55\n\n\nSay What\n49\n1.52\n\n\nJules Winnfeild üè≥Ô∏è‚Äç‚ößÔ∏è\n47\n1.45\n\n\nasiak\n46\n1.42\n\n\nhardradajm\n40\n1.24\n\n\nBob Carmody\n34\n1.05\n\n\nT.R.\n33\n1.02\n\n\nmaj_k1bbles\n31\n0.96\n\n\nogdimwit\n31\n0.96"
  },
  {
    "objectID": "tutorials/tutorial-09.html#pr√§valenz-vs.-h√§ufigkeit",
    "href": "tutorials/tutorial-09.html#pr√§valenz-vs.-h√§ufigkeit",
    "title": "üî® Topic Modeling in R",
    "section": "Pr√§valenz vs.¬†H√§ufigkeit",
    "text": "Pr√§valenz vs.¬†H√§ufigkeit\n\ntop_gamma_k14 %&gt;% \n  ggplot(aes(as.factor(topic), gamma)) +\n  geom_col(fill = \"#F57350\") +\n  labs(\n    x = \"Topic\",\n    y = \"Mean gamma\"\n  ) +\n  coord_flip() +\n  scale_y_reverse() +\n  scale_x_discrete(position = \"top\") +\n  theme_pubr()\n\n\n\n\n\n\n\n\n\nchats_topics %&gt;% \n  mutate(across(top_topic, as.factor)) %&gt;% \n  ggplot(aes(top_topic, y = after_stat(prop), group = 1)) +\n  geom_bar(fill = \"#1DA1F2\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    x = \"\", \n    y = \"Relative frequency\"\n  ) +\n  coord_flip() +\n  theme_pubr()"
  },
  {
    "objectID": "tutorials/tutorial-09.html#einfluss-von-meta-variablen",
    "href": "tutorials/tutorial-09.html#einfluss-von-meta-variablen",
    "title": "üî® Topic Modeling in R",
    "section": "Einfluss von Meta-Variablen",
    "text": "Einfluss von Meta-Variablen\n\neffects &lt;- estimateEffect(\n  formula =~ platform + debate + message_during_debate,\n  stmobj = tpm_k14, \n  metadata = chats_topics)\n\n\nsummary(effects, topics = 12)\n\n\nCall:\nestimateEffect(formula = ~platform + debate + message_during_debate, \n    stmobj = tpm_k14, metadata = chats_topics)\n\n\nTopic 12:\n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)             -0.128058  28.831457  -0.004  0.99646   \nplatformyoutube          0.214453  28.831531   0.007  0.99407   \ndebatevice presidential  0.203569  28.831506   0.007  0.99437   \nmessage_during_debate    0.011889   0.004316   2.755  0.00588 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsummary(effects, topics = 8)\n\n\nCall:\nestimateEffect(formula = ~platform + debate + message_during_debate, \n    stmobj = tpm_k14, metadata = chats_topics)\n\n\nTopic 8:\n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)              0.1234719 29.5028653   0.004    0.997\nplatformyoutube         -0.0333377 29.5028958  -0.001    0.999\ndebatevice presidential -0.0134738 29.5028788   0.000    1.000\nmessage_during_debate    0.0006216  0.0044960   0.138    0.890"
  },
  {
    "objectID": "computing/computing-useful_links.html",
    "href": "computing/computing-useful_links.html",
    "title": "Useful sources",
    "section": "",
    "text": "This is selection of useful R sources:\n\n Quarto tutorials by Andy Field\n Automatisierte Inhaltsanalyse mit R by Kornelius Puschmann\nBasiskurs R/RStudio by the Chair of Statistics and Econometrics at Friedrich-Alexander-Universit√§t Erlangen-N√ºrnberg",
    "crumbs": [
      "Working with R",
      "Useful R sources"
    ]
  },
  {
    "objectID": "computing/computing-cheatsheets.html",
    "href": "computing/computing-cheatsheets.html",
    "title": "R cheatsheets",
    "section": "",
    "text": "The following cheatsheets come from https://posit.co/resources/cheatsheets. We haven‚Äôt covered every function and functionality listed on them, but you might still find them useful as references.",
    "crumbs": [
      "Working with R",
      "R Cheatsheets"
    ]
  },
  {
    "objectID": "course-assignments.html",
    "href": "course-assignments.html",
    "title": "Assingments",
    "section": "",
    "text": "In order to be able to adapt the assignments flexibly to the requirements of the course (e.g.¬†different number of students, projects changing from semester to semester or several projects in one semester), this seminar uses a portfolio assessment. Even though the seminar aims at testing new forms of assignments due to its practical and empirical orientation, traditional assignments such as presentations or written reports will still be an integral part. However, these will be expanded to include the special features of working with digital behavioral data.\nAssignments can be group (üë•) or individual (üë§) work and are marked as such. The expected group size is 2-4 persons.\nThe following list contains an overview of the assignments to be completed in the course:\n\n\nBreakdown of the final grade\n\n\nAssignment\n100 Pts\n\n\n\n\nüë• Presentation: üìö Theory\n30 Pts\n\n\nüë• Presentation: üìä Research Project\n15 Pts\n\n\nüë§ Peer Review\n15 Pts\n\n\nüë• Written assignment: Final Project Report\n40 Pts\n\n\n\n\n\n\n\n\n\nImportant Disclaimer\n\n\n\n\nThe final structure of the course (e.g.¬†which and how many small projects) will be discussed and determined together with the students in the first session. Accordingly, it is possible that the portfolio or the assignments contained in it will be adjusted subsequently (e.g.¬†the scope, content or deadline of assignments could be changed or even entire assignments could be dropped).",
    "crumbs": [
      "Course information",
      "Assignments"
    ]
  },
  {
    "objectID": "course-assignments.html#sec-portfolio",
    "href": "course-assignments.html#sec-portfolio",
    "title": "Assingments",
    "section": "",
    "text": "In order to be able to adapt the assignments flexibly to the requirements of the course (e.g.¬†different number of students, projects changing from semester to semester or several projects in one semester), this seminar uses a portfolio assessment. Even though the seminar aims at testing new forms of assignments due to its practical and empirical orientation, traditional assignments such as presentations or written reports will still be an integral part. However, these will be expanded to include the special features of working with digital behavioral data.\nAssignments can be group (üë•) or individual (üë§) work and are marked as such. The expected group size is 2-4 persons.\nThe following list contains an overview of the assignments to be completed in the course:\n\n\nBreakdown of the final grade\n\n\nAssignment\n100 Pts\n\n\n\n\nüë• Presentation: üìö Theory\n30 Pts\n\n\nüë• Presentation: üìä Research Project\n15 Pts\n\n\nüë§ Peer Review\n15 Pts\n\n\nüë• Written assignment: Final Project Report\n40 Pts\n\n\n\n\n\n\n\n\n\nImportant Disclaimer\n\n\n\n\nThe final structure of the course (e.g.¬†which and how many small projects) will be discussed and determined together with the students in the first session. Accordingly, it is possible that the portfolio or the assignments contained in it will be adjusted subsequently (e.g.¬†the scope, content or deadline of assignments could be changed or even entire assignments could be dropped).",
    "crumbs": [
      "Course information",
      "Assignments"
    ]
  },
  {
    "objectID": "course-assignments.html#sec-presentation",
    "href": "course-assignments.html#sec-presentation",
    "title": "Assingments",
    "section": "üë• Presentation : üìö Theory (20 Pts)",
    "text": "üë• Presentation : üìö Theory (20 Pts)\nThe topics for the presentation will be assigned at the beginning of the course. The presentation should be 20 to 30 minutes long (including time for questions and discussion). Literature for the preparation of the presentation will be provided. The texts relevant for the respective presentation can be found in the information on the preparation (üìñ) of the respective session. All texts listed in the section ‚ÄúMandatory literature‚Äù constitute the presentation literature for your respective presentation and will be provided. Please include all texts on your topic in your presentation, but feel free to set your own priorities in the presentation. You may also cite other sources, provided they enrich the subject matter.\nThe aim of the presentation is to give the course participants an overview on your topic, e.g., central terms, definitions and features of the respective platform, method and/or tool. The presentation of the state of research (what is the goal of the studies and what do they show?) plays a subordinate role.\n\nFeedback meeting before the presentation (mandatory)\nAdditionally, presenters are required to meet with the instructor in the week before their presentation for a mandatory feedback. My office hours are directly after the session, on Wednesdays from 15:30 to 16:30. If you have scheduling conflicts, we can arrange another meeting time. Meetings can take place in person or via Zoom.\n\nIn advance of the feedback meeting, a first complete draft of the presentation must be submitted as a PowerPoint or PDF file via mail at the latest until 12:00 the day before the meeting, one week before the presentation. During the feedback meeting, students will receive detailed feedback and tips on how to revise their presentation. The revised presentations are then given in presence in the respective sessions. Afterwards, a PDF of the slides is made available to the seminar.\n\n\nFeedback after the presentation (optional)\nIf you would like to receive feedback on your presentation after the session, please let me know during the mandatory feedack meeting. The feedback will take place directly after the event (15:00 to 15:30). The aim of this feedback is to give you a first impression directly after the presentation. However, there will be no (assessment of) the presentation in the discussion.\n\n\n\n\n\n\nImportant information, tasks & deadlines\n\n\n\n\nüí° You will be graded based on your individual contribution, so please clarify which slides are presented by whom (e.g., by adding the initials of the presenting individual in the footnote of the slide).\n‚è∞ One week before your presentation: feedback meeting to discuss the presentation draft during the office hours. Please arrange an alternative date in good time if you are unable to attend the scheduled feedback meeting.\n‚è∞ At the latest at 12:00 the day before the feedback meeting: Send the first complete draft of the presentation slides by e-mail to christoph.adrian@fau.de\n‚è∞ Until 09:00 of the day of the presentation: Send the final draft of the presentation by e-mail to christoph.adrian@fau.de",
    "crumbs": [
      "Course information",
      "Assignments"
    ]
  },
  {
    "objectID": "course-assignments.html#sec-research-project-presentation",
    "href": "course-assignments.html#sec-research-project-presentation",
    "title": "Assingments",
    "section": "üë• Presentation: üìä Research Project (15 Pts)",
    "text": "üë• Presentation: üìä Research Project (15 Pts)\nIn this presentations you will present your research project, including (preliminary) results. The presentation should be 15 to 20 minutes long (with additional 10 minutes for questions and discussion). The aim of the presentation is to give the course an overview of your project and to get feedback on your research question, methodology and analysis, that can be included into the written report. Therefore, you should include the following points in your presentation:\n\nSection 1 - Introduction (2-3 slides)\nThe introduction section includes\n\nan introduction to the subject matter you‚Äôre investigating\nthe motivation for your research question (citing relevant literature)\nthe general research question you wish to explore and/or your hypotheses regarding the research question of interest.\n\n\n\nSection 2 - Method description (2 slides)\nIn this section, you will describe the method and data (sub-)sample you selected. This includes\n\ndescription of the ‚Äúobservations‚Äù included in the data set,\nmotivations for the selection of your specific subsample,\nanalytical approach with which you want to answer the research question/hypothesis,\ndescription of the central method(s) as well as necessary pre-processing steps.\n\n\n\nSection 3 - Results and Discussion (2-3 slides)\nIn this section, you will provide a brief overview of your (preliminary) results. This includes\n\ndescription, visualization and/or summary statistics of the central variables,\n(preliminary) results of the central method(s) you used,\n(optionally) a challenge you face and want to discuss with the course.\n\nThese presentations are the basis for the peer review and the final written report. Therefore, you should use the feedback you receive to improve your project and the final report.\n\n\n\n\n\n\nImportant information, tasks & deadlines\n\n\n\n\nüí° You will be graded as a group (unless explicitly requested otherwise). Not every group member has to present, but all group members should contribute and be able to answer questions about the project.\nüí° The presentations will be the basis for the Peer Review assignment. In order to simplify the process, please use the Google-Slide template provided to your group.\n‚è∞ Until the 15.01.2025: Finalize the presentation (Goolge Slides). On the 16.01., the presentation will be changed to non-editable and make available for peer review. After the peer-review process, the presentation will be ‚Äúunlocked‚Äù and you can make changes again.",
    "crumbs": [
      "Course information",
      "Assignments"
    ]
  },
  {
    "objectID": "course-assignments.html#sec-peer-review",
    "href": "course-assignments.html#sec-peer-review",
    "title": "Assingments",
    "section": "üë§ Peer Review (15 Pts)",
    "text": "üë§ Peer Review (15 Pts)\nCritically reviewing others‚Äô work is a crucial part of the scientific process. Therefore, each person will be assigned two other group‚Äôs research project presentations to review before their presentation. This way, you will be able to get additional feedback before presenting your results to the whole course.\nDuring the peer review process, you will be provided with\n\nread-only access to two group presentations via Google Slides. Although the slides should be non-editable, please to not try to change and do not share the slides.\na link to a digital peer review form. The form contains both short item scales and open-ended questions. The aim is to summarize the content of the presentation very briefly, evaluate it and make suggestions for improvement.\n\n\n\n\n\n\n\n\n\nImportant tasks & deadlines\n\n\n\n\nüí° The peer review will be graded on the extent to which it comprehensively and constructively addresses the components of the other groups presentation: the research context and motivation, data analysis, modeling, interpretations, and conclusions.\n‚è∞ Between 15.01. and 22.01: Fill out the peer review form for the two assigned groups.",
    "crumbs": [
      "Course information",
      "Assignments"
    ]
  },
  {
    "objectID": "course-assignments.html#sec-written-report",
    "href": "course-assignments.html#sec-written-report",
    "title": "Assingments",
    "section": "üë• Written short report (40 Pts)",
    "text": "üë• Written short report (40 Pts)\nThe goal of the written short report is for each group to use at least one of the method or data presented to explore a topic of your own choosing. Choose both data and topic based on your group‚Äôs interests, experience or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis. You are welcome to include an appendix with additional work at the end of the written report document; however, grading will largely be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the word limit.\nThe written report should be 750 to 1000 words per person. However, when written as a group report, the number or words scale with a factor of 0.8 per person (e.g., a group of two should write 1200 to 1600 words, a group of three 1800 to 2400 words). All analyses as well as the written report must be done in RStudio and all components of the project must be reproducible. The mandatory components of the reports are: Introduction, Data/Methodology, Results, Discussion & Conclusion. You are free to add additional sections as necessary. You will be graded based on your individual contribution, so please clarify which part of the report was written by whom (e.g., by adding the initials of the author in the header of the section).\nThe written report is worth 40 points, broken down as follows\n\n\n\nTotal\n40 pts\n\n\n\n\nIntroduction + Theory\n6 pts\n\n\nMethodology/Data\n10 pts\n\n\nResults\n14 pts\n\n\nDiscussion + conclusion\n6 pts\n\n\nOrganization + formatting\n4 pts\n\n\n\n\n\n\n\n\n\nImportant tasks & deadlines\n\n\n\n\nFinal submission of the revised written report is due 02.03.2024, 23:591.",
    "crumbs": [
      "Course information",
      "Assignments"
    ]
  },
  {
    "objectID": "course-assignments.html#footnotes",
    "href": "course-assignments.html#footnotes",
    "title": "Assingments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPlease note that this is a temporary deadline. The final deadline will be adjusted in the course in consultation with the students if necessary.‚Ü©Ô∏é",
    "crumbs": [
      "Course information",
      "Assignments"
    ]
  }
]