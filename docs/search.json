[
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "First session\nSession time\nLocation (Room)\n\n\n\n\n23.10.2023\nWednesdays, 13:15 - 14:45\nFindelgasse, 2.024\n\n\n\n\n\n\n\n\n\nAssignment and course language\n\n\n\n\nThe course can be held in English or German (incl. presentations, pitches, discussions, etc.), depending on the preferences voiced by the students in the survey before the beginning of the course.\nRegardless of the feedback on the course language, the written assignments can be in German or English, depending on what the student/groups prefer.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-information",
    "href": "course-syllabus.html#course-information",
    "title": "Syllabus",
    "section": "",
    "text": "First session\nSession time\nLocation (Room)\n\n\n\n\n23.10.2023\nWednesdays, 13:15 - 14:45\nFindelgasse, 2.024\n\n\n\n\n\n\n\n\n\nAssignment and course language\n\n\n\n\nThe course can be held in English or German (incl. presentations, pitches, discussions, etc.), depending on the preferences voiced by the students in the survey before the beginning of the course.\nRegardless of the feedback on the course language, the written assignments can be in German or English, depending on what the student/groups prefer.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-description",
    "href": "course-syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course description",
    "text": "Course description\nIn this seminar, students are introduced to working with digital behavioral data (DBD). DBD refer to digital traces of human behavior that are knowingly or unknowingly left in online environments (e.g. social media, messengers, entertainment media, or digital collaboration tools). These rich data is increasingly available to social scientific research in the public interest, but can also be used to derive strategic insights for business decisions.\nStudents learn how to work with DBD alongside the entire research process, from data collection, preprocessing and analysis, to reporting and provision (e.g. via open science tools). Students first get a comprehensive overview of the ways in which DBD can be collected (e.g., API scraping, usage logging, mock-up virtual environments, or data donations), as well as the requirements for data protection, research ethics, and data quality. Afterwards, students practice and apply their newly acquired knowledge in small projects on use cases from media and communication research. In doing so, they learn important computer-based methods with which large digital behavioral data sets (e.g. texts, images, usage behavior logs) can be processed and analyzed. By completing this module, participants will get an up-to-date overview and practical insights into how the potential of observational data (digital traces) can be used to better understand the behavior of media users in digital environments.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nStudents will\n\noverview and understand central opportunities of DBD and accompanying challenges for data collection and preprocessing\nevaluate the strengths and weaknesses of different ways of collecting DBD\nget to know and understand central requirements for data protection, research ethics, and data quality\nget to know and overview key computational social science methods to analyze DBD\npractice and apply knowledge on DBD, statistics, and data analysis in small projects of their own",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#recommended-prerequisites",
    "href": "course-syllabus.html#recommended-prerequisites",
    "title": "Syllabus",
    "section": "Recommended prerequisites",
    "text": "Recommended prerequisites\n\nInterest in social scientific perspectives on media, communication, and digital technologies.\nBasic knowledge of working with statistical software such as Stata, R, Python, or SPSS is required.\nStudents are recommended, but not required, to also visit the lecture Data Science: Foundations, Tools, Applications in Socio-Economics and Marketing.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#organization-of-the-course",
    "href": "course-syllabus.html#organization-of-the-course",
    "title": "Syllabus",
    "section": "Organization of the course",
    "text": "Organization of the course\nRegistration for the course takes place via  StudOn. There you will receive the first information and instructions. Please make sure that you complete the short survey before the seminar begins.\nAll slides, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website. I will regularly send out course announcements by e-mail, so please make sure to check your mail address associated with  StudOn regularly.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#preliminary-schedule",
    "href": "course-syllabus.html#preliminary-schedule",
    "title": "Syllabus",
    "section": "(Preliminary) Schedule",
    "text": "(Preliminary) Schedule\n\n\n\n\n\n\nImportant information\n\n\n\n\n⚠️ Please note that this is a provisional timetable which may change, especially after the kick-off meeting and the allocation of topics (see Course schedule for the latest version).\nAll sessions marked with a 🔨 are hands-on sessions actively working with R.\nAll sessions marked with a 📚 are presentation sessions where groups of students will give a detailed presentation (see Assignments for more information).\nAll sessions marked with a 📊 are presentation sessions where groups of students will present the results of their project work (see Assignments for more information).\n\n\n\n\n\n\n\n\n\n\n\nSession\nDatum\nTopic\n\n\n\n\n\n📂 Block 1\nIntroduction\n\n\n1\n23.10.2024\nKick-Off\n\n\n2\n30.10.2024\nDBD: Introduction & Overview\n\n\n3\n06.11.2024\n🔨 Introduction to working with R\n\n\n\n📂 Block 2\nTheoretical background:  & TV election debates\n\n\n4\n13.11.2024\n📚  usage in focus\n\n\n5\n20.11.2024\n📚 Effects of  & TV debates\n\n\n6\n27.11.2024\n📚 Political TV debates & social media\n\n\n\n📂 Block 3\nNatural language processing\n\n\n7\n04.12.2024\n🔨 Text as data I: Introduction\n\n\n8\n11.12.2024\n🔨 Text as data II: Advanced Methods\n\n\n9\n18.12.2024\n🔨 Advanced Method I: Topic Modeling\n\n\n-\n-\n🎄Christmas Break (No Lecture)\n\n\n10\n08.01.2025\n🔨 Advanced Method II: Machine Learning\n\n\n\n📂 Block 5\nProject Work\n\n\n11\n15.01.2025\n🔨 Project work\n\n\n12\n22.01.2025\n🔨 Project work\n\n\n13\n29.01.2025\n📊 Project Presentation (I)\n\n\n14\n05.02.2025\n📊 Project Presentation (II) & 🏁 Evaluation\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor the latest, more detailed version of the course schedule as well as the linked content of the individual sessions (e.g. slides or literature for the respective presentation), please see Schedule.\n\n\nThe course consists of several blocks:\n\nBlock I: Introduction\nThe first four sessions form the (theoretical) basis for the course.\n\nThe kick-off session is mainly for getting to know each other and organizing the course. \nThe second session is to give you an extended introduction DBD, including challenges and important frameworks. \nThe third session is about practical work with R and RStudio.  \n\n\n\nBlock II: Theoretical foundation\nThe second block will contain the presentations by different groups of students about the research relevant for the course.\n\n\nBlock III: Natural Language Processing (NLP)\nThe third block is about working with text data. It is separated into four parts.\n\nThe first session is an introduction to working with text data.\nThe second session is about advanced methods for working with text data and introduces the two methods of the next two sessions.\nThe third session is about topic modeling, a method to extract topics from text data.\nThe fourth session is about machine learning, a method to classify text data.\n\n\n\nBlock IV: Project Work\nThe last block is about working on your project. The goal is to combine the theoretical and practical knowledge you have gained in the previous sessions and apply it to a research project of your choice. This means you have to find a research question, develop a concept of an analysis, run it, analyze it, and present your results (short presentation and written report).",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#sessions",
    "href": "course-syllabus.html#sessions",
    "title": "Syllabus",
    "section": "Sessions",
    "text": "Sessions\nThe goal of the sessions is to be as interactive as possible. In general, the sessions consist of two parts. In the first part (± 30 - 45 minutes) at the beginning of the session, there are usually presentations (including discussion), which are more or less detailed depending on the stage of the project. The second part (± 45 - 60 minutes) consists of a group activity (with concluding discussion), which should either be about deepening the presentation content or about independent work on one’s own or the group project.\nMy role as instructor is to introduce you new tools and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. You are expected to bring a laptop to each class so that you can take part in the in-session exercises. Please make sure your laptop is fully charged before you come to class as the number of outlets in the classroom will not be sufficient to accommodate everyone.\n\nWhere to ask questions\n\nIf you have a question during the lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nAny general questions about session content, assignments or about the project should be posted into the  StudOn-Forum, so that everyone can benefit from the answers. There is a chance another student has already asked a similar question, so please check the other posts before adding a new question. If you know the answer to a question, I encourage you to respond!\nE-mails should be reserved for personal matters.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#assessment",
    "href": "course-syllabus.html#assessment",
    "title": "Syllabus",
    "section": "Assessment",
    "text": "Assessment\nIn order to obtain credits and a grade, participants are required to\n\nattend regularly (at least 80% of the sessions) and participate actively. A maximum of two sessions can be missed without excuse. Absence in further sessions can only be excused in case of illness (i. e. with a medical certificate).\ncomplete various assignments as part of a portfolio. The type and scope of the assignments depends on the number of participants and the project(s). Detailed information can be found in the section Assignments.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#academic-integrity",
    "href": "course-syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic integrity",
    "text": "Academic integrity\n\n\n\n\n\n\nTL;DR\n\n\n\nDo not cheat!\n\n\nFor general information on formatting, style, citation, appendices, wording of the affidavit, etc., see our Guide to Academic Writing.\n\nPolicy on sharing and reusing code\nI am well aware that a huge volume of code is available on the web to solve any number of problems. Unless I explicitly tell you not to use something, the course’s policy is that you may make use of any online resources (e.g. StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\n\n\nPolicy on use of generative artificial intelligence (AI):\nYou should treat generative AI, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course1: (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n✅ AI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\n❌ AI tools for narrative: Unless instructed otherwise, you may not use generative AI to write narrative on assignments. In general, you may use generative AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#recommended-textbooks",
    "href": "course-syllabus.html#recommended-textbooks",
    "title": "Syllabus",
    "section": "Recommended textbooks",
    "text": "Recommended textbooks\n\nAtteveldt, W. van, Trilling, D., & Arcíla, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Handbook of Computational Social Science, Volume 1: Theory, Case Studies and Ethics (1st ed.). Routledge. https://doi.org/10.4324/9781003024583\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Handbook of computational social science, volume 2. Routledge. https://doi.org/10.4324/9781003025245\nHaim, M. (2023). Computational Communication Science: Eine Einführung. Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-40171-9\nSalganik, M. J. (2018). Bit by bit: Social research in the digital age. Princeton University Press.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#footnotes",
    "href": "course-syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D↩︎",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "computing/computing-textbooks.html",
    "href": "computing/computing-textbooks.html",
    "title": "R textbooks",
    "section": "",
    "text": "While there is no official R textbook for the course, there are a few to look at:\n\n🔗 R for Data Science, 2nd Edition\n🔗 Data Visualization: A Practical Introduction\n🔗 Tidy modeling with R\n🔗 Text Mining with R",
    "crumbs": [
      "Working with R",
      "R Textbooks"
    ]
  },
  {
    "objectID": "computing/computing-instructions_R.html",
    "href": "computing/computing-instructions_R.html",
    "title": "Introduction",
    "section": "",
    "text": "Assignment\n\n\n\n\nPlease watch all the videos in the tutorial series on Introduction to R, RStudio & Quarto and Practical Work with R by December 1st, 2024 at the latest. The content conveyed in the videos (e.g., knowledge and application of certain functions to filter, summarize, or edit data) will be assumed as a foundation for session 7 of the seminar.\nIf you have questions and/or problems, please write a post in the forum of the StudOn course so that everyone can benefit from the answers. Use the prefix “Question/Problem R:” in the subject line and tag/add me. If available, please always include the material/video to which your question refers.",
    "crumbs": [
      "Working with R",
      "R Video Tutorials"
    ]
  },
  {
    "objectID": "computing/computing-instructions_R.html#background",
    "href": "computing/computing-instructions_R.html#background",
    "title": "Introduction",
    "section": "Background",
    "text": "Background\nPractical work with R, RStudio, and Quarto is an integral part of the Digital Behavioral Data course. To accommodate different levels of prior knowledge while establishing a common “basic knowledge” for the course, we would like to provide you with a series of introductory videos to facilitate your (re)entry. Specifically, this consists of a mix of YouTube tutorials by  Andy Field, which initially teach the general handling of R, RStudio & Quarto, and the materials from CCS.Amsterdam, which focus on more “substantive” work with R. All tutorials are in English.",
    "crumbs": [
      "Working with R",
      "R Video Tutorials"
    ]
  },
  {
    "objectID": "computing/computing-instructions_R.html#introduction-to-r-rstudio-quarto",
    "href": "computing/computing-instructions_R.html#introduction-to-r-rstudio-quarto",
    "title": "Introduction",
    "section": "Introduction to R, RStudio & Quarto",
    "text": "Introduction to R, RStudio & Quarto\n\nThis video tutorial series was created by  Andy Field to acompany his books An Adventure in Statistics (Field and Iles 2016) and Discovering Statistics Using R (Field, Miles, and Field 2012), and deals with the use of RStudio and Quarto for interacting with R.\nThe videos cover the installation of R, RStudio, and Quarto, the differences between them, a tour of RStudio, good workflows in RStudio, installing and loading packages, and using Quarto.\n\n\n\n\n\n\n\nPlease note …\n\n\n\n\nMost of the exercises shown in the tutorial can be easily reproduced, either by manually entering the data or variables or by using your own files (e.g., when embedding graphics).\nHowever, some data used (e.g., in the session RStudio Working with Code: Part 3) is unfortunately not available publically. In this case, you can either use your own “data” to reproduce the examples or rely on built-in R datasets (such as airquality, mtcars, iris, etc.) by loading them with the data() command (e.g., data(mtcars)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession\nTitel\nDauer\n\n\n\n\n1\n\n\n\n\n Installing R and RStudio\n06:13\n\n\n2\n\n\n\n\n Installing Quarto\n02:55\n\n\n3\n\n\n\n\n R Studio desktop workflow (2022)\n08:16\n\n\n4\n\n\n\n\n Customizing Rstudio\n08:03\n\n\n5\n\n\n\n\n Quarto visual editor [Part 1]\n10:20\n\n\n6\n\n\n\n\n Quarto visual editor [Part 2]\n13:19\n\n\n7\n\n\n\n\n Quarto visual editor [Part 3]\n10:39\n\n\n8\n\n\n\n\n Quarto visual editor [Part 4]\n04:28\n\n\n9\n\n\n\n\n Quarto visual editor [Part 5]\n12:10\n\n\n10\n\n\n\n\n RStudio Working with Code: Part 1\n07:41\n\n\n11\n\n\n\n\n RStudio Working with Code: Part 2\n14:08\n\n\n12\n\n\n\n\n RStudio Working with Code: Part 3\n08:44\n\n\n13\n\n\n\n\n RStudio Working with Code: Part 4\n06:47\n\n\n14\n\n\n\n\n RStudio Working with Code: Part 5\n07:07\n\n\n15\n\n\n\n\n RStudio Working with Code: Part 6\n05:48\n\n\n\nYou can also play the videos as a YouTube playlist.",
    "crumbs": [
      "Working with R",
      "R Video Tutorials"
    ]
  },
  {
    "objectID": "computing/computing-instructions_R.html#practical-work-with-r",
    "href": "computing/computing-instructions_R.html#practical-work-with-r",
    "title": "Introduction",
    "section": "Practical Work with R",
    "text": "Practical Work with R\n\nCCS.Amsterdam is a group of “Computational” communication scientists from the University of Amsterdam and the Vrije Universiteit Amsterdam. In various research projects, these scientists aim to use and develop computational methods to answer social science research questions. This includes, among other things, the study of news streams, polarization, political microtargeting, fake news, and recommender design. A main goal of the group is to disseminate knowledge among a growing community of enthusiastic “Computational” communication scientists.\nThe series of tutorials curated by CCS.Amsterdam aims to teach the use of tidyverse functions for data cleaning, transformation, visualization, etc. The tutorials consist of both handouts, i.e., documents explaining the most important commands, and video tutorials covering the same material.\nThe table also lists chapters from Computational Analysis of Communication [CAC] and R for Data Science [R4DS], two 100% free and openly accessible books that also cover and possibly deepen the material of the respective session.\n\n\n\n\n\n\n\nPlease note\n\n\n\n\nThe video tutorials may be slightly older than the handouts. In case of doubt, follow the content of the handouts rather than the videos.\nPlease note that the CAC offers R and Python examples side by side. You may need to actively select the R code examples.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession\nVideo-Tutorial\nMaterialien\nLiteratur\nDauer\n\n\n\n\n1\n\n\n\n\n R basics: commands, objects, and functions\n\n\n\n\n [Handout]\n\n\n\n\n [CAC] [R4DS]\n29:30\n\n\n2\n\n\n\n\n R Tidyverse: Data transformation\n\n\n\n\n [Handout]\n\n\n\n\n [CAC] [R4DS]\n22:19\n\n\n3\n\n\n\n\n R Tidyverse: Data summarization\n\n\n\n\n [Handout]\n\n\n\n\n [CAC] [R4DS]\n11:00\n\n\n4\n\n\n\n\n R ggplot2: Basics of data visualization\n\n\n\n\n [Handout]\n\n\n\n\n [CAC] [R4DS]\n35:14\n\n\n\nYou can also play the videos as a YouTube playlist.",
    "crumbs": [
      "Working with R",
      "R Video Tutorials"
    ]
  },
  {
    "objectID": "tutorials/tutorial-08.html",
    "href": "tutorials/tutorial-08.html",
    "title": "🔨 Advanced Methods",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "tutorials/tutorial-08.html#background",
    "href": "tutorials/tutorial-08.html#background",
    "title": "🔨 Advanced Methods",
    "section": "Background",
    "text": "Background"
  },
  {
    "objectID": "tutorials/tutorial-08.html#preparation",
    "href": "tutorials/tutorial-08.html#preparation",
    "title": "🔨 Advanced Methods",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, \n    magrittr, janitor,\n    ggpubr, \n    gt, gtExtras,\n    countdown, \n    quanteda, # quanteda text processing\n    quanteda.textplots, quanteda.textstats, quanteda.textmodels,\n    udpipe, spacyr, # POS tagging\n    easystats, tidyverse\n)\n\n\n# Import base data\nchats &lt;- qs::qread(here(\"local_data/chat-debates_full.qs\"))$correct\ntranscripts &lt;- qs::qread(here(\"local_data/transcripts-debates_full.qs\"))$correct\ndict_chat_emotes &lt;- readRDS(here(\"local_data/dictionary_chat_emotes.RDS\"))\n\n# Import corpora\ntranscripts_udpipe &lt;- qs::qread(here(\"local_data/transcripts-corpus_udpipe.qs\"))\ntranscripts_spacyr &lt;- qs::qread(here(\"local_data/transcripts-corpus_spacyr.qs\"))\ntranscripts_pos &lt;- transcripts_udpipe"
  },
  {
    "objectID": "tutorials/tutorial-08.html#codechunks-aus-der-sitzung",
    "href": "tutorials/tutorial-08.html#codechunks-aus-der-sitzung",
    "title": "🔨 Advanced Methods",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nErstellung der Datengrundlage\n\n# Create corpus\ncorp_transcripts &lt;- transcripts %&gt;% \n  quanteda::corpus(\n    docid_field = \"id_sequence\", \n    text_field = \"dialogue\"\n  )\n\n# Tokenize corpus\ntoks_transcripts &lt;- corp_transcripts %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  ) %&gt;% \n  quanteda::tokens_remove(\n    pattern = quanteda::stopwords(\"en\")\n  )\n\n# Add n_grams\ntoks_transcripts_ngrams &lt;- toks_transcripts %&gt;% \n  quanteda::tokens_ngrams(n = 1:3)\n\n# Convert to DFM\ndfm_transcripts &lt;- toks_transcripts_ngrams %&gt;% \n  quanteda::dfm()\n\n\n# Create corpus\ncorp_chats &lt;- chats %&gt;% \n  quanteda::corpus(docid_field = \"message_id\", text_field = \"message_content\")\n\n# Tokenize corpus\ntoks_chats &lt;- corp_chats %&gt;% quanteda::tokens()\n\n# Convert to DFM\ndfm_chats &lt;- toks_chats %&gt;% quanteda::dfm()\n\n\n\nNgrams: Sequenzen von N aufeinanderfolgenden Token\n\ntoks_transcripts %&gt;% \n  quanteda::tokens_ngrams(n = 2) %&gt;% \n  quanteda::dfm() %&gt;%  \n  quanteda.textstats::textstat_frequency() %&gt;% \n  head(25) \n\n             feature frequency rank docfreq group\n1          know_know      1337    1      49   all\n2  t-mobile_t-mobile       864    2       6   all\n3       donald_trump       755    3     461   all\n4          going_say       666    4      30   all\n5          say_going       661    5      35   all\n6         saying_bad       558    6       4   all\n7         bad_saying       553    7       3   all\n8      kamala_harris       494    8     333   all\n9     vice_president       429    9     376   all\n10   curious_curious       373   10       7   all\n11    sekunden_pause       354   11     266   all\n12         right_now       269   12     234   all\n13     united_states       268   13     211   all\n14         feel_like       230   14     164   all\n15             oh_oh       229   15      10   all\n16         like_know       208   16     133   all\n17   president_trump       203   17     178   all\n18         like_like       191   18     144   all\n19  president_harris       186   19     179   all\n20        lot_people       181   20     138   all\n21         know_like       168   21     109   all\n22   american_people       163   22     118   all\n23            oh_god       154   23     139   all\n24         just_like       153   24     129   all\n25  former_president       141   25     115   all\n\n\n\n\nKollokationen: Identifikation von bedeutungsvollen Wortkombinationen\n\ntoks_transcripts %&gt;% \n  quanteda.textstats::textstat_collocations(\n    size = 2, \n    min_count = 5\n  ) %&gt;% \n  head(25)\n\n        collocation count count_nested length    lambda        z\n1         know know  1337            0      2  3.890787 98.31370\n2        saying bad   558            0      2  6.503305 81.19215\n3        bad saying   553            0      2  6.481795 80.98625\n4         going say   666            0      2  4.555737 80.92246\n5         say going   661            0      2  4.562963 80.82524\n6      donald trump   755            0      2  7.422847 75.19267\n7     kamala harris   494            0      2  7.873933 68.88003\n8    vice president   429            0      2  7.258104 56.72753\n9             oh oh   229            0      2  4.679549 54.95066\n10        right now   269            0      2  3.996328 53.27167\n11    senator vance   129            0      2  6.583164 49.48173\n12       little bit   132            0      2  8.268099 45.80914\n13 president harris   186            0      2  4.027681 45.69845\n14           oh god   154            0      2  6.175930 44.94423\n15        years ago   102            0      2  6.436807 43.06424\n16         tim walz    90            0      2  7.588398 43.05596\n17  president trump   203            0      2  3.468589 42.70406\n18       four years   100            0      2  6.381600 42.53221\n19      health care   136            0      2  7.400206 41.83123\n20      white house    85            0      2  8.071701 41.52827\n21   donald trump's   132            0      2  6.408658 41.05938\n22 former president   141            0      2  5.790480 41.04486\n23  curious curious   373            0      2 11.836611 40.77202\n24    governor walz    77            0      2  6.512020 39.65499\n25      two minutes    84            0      2  6.490910 39.39074\n\n\n\n\nArbeiten mit quanteda: corpus\n\n# Create corpus\ncorp_transcripts &lt;- transcripts %&gt;% \n  quanteda::corpus(\n    docid_field = \"id_sequence\", \n    text_field = \"dialogue\"\n  )\n\n# Output\ncorp_transcripts\n\nCorpus consisting of 5,861 documents and 10 docvars.\np1_s0001 :\n\"Tonight, the high-stakes showdown here in Philadelphia betwe...\"\n\np1_s0002 :\n\"A historic race for president upended just weeks ago, Presid...\"\n\np1_s0003 :\n\"The candidates separated by the smallest of margins, essenti...\"\n\np1_s0004 :\n\"This is an ABC News special. The most consequential moment o...\"\n\np1_s0005 :\n\"Together, we'll chart a... (..)\"\n\np1_s0006 :\n\"Donald Trump.\"\n\n[ reached max_ndoc ... 5,855 more documents ]\n\n\n\n\nKeywords-in-Context (KWIC)\n\nUnmittelbarer Wortkontext ohne statistische Gewichtung\n\ntoks_transcripts %&gt;% \n  kwic(\"know\", window = 3) %&gt;% \nhead(10)\n\nKeyword-in-context with 10 matches.                                                                              \n [p1_s0018, 29]  opportunity economy thing | know | shortage homes housing    \n [p1_s0018, 39]            far many people | know | young families need       \n [p1_s0020, 25]  billions billions dollars | know | China fact never          \n [p1_s0022, 44]          done intend build | know | aspirations hopes American\n  [p1_s0024, 2]                    nothing | know | knows better anyone       \n  [p1_s0025, 1]                            | know | everybody else Vice       \n [p1_s0026, 64]        stand issues invite | know | Donald Trump actually     \n [p1_s0028, 38]       goods coming country | know | many economists say       \n [p1_s0029, 24] billions dollars countries | know | like gone immediately     \n [p1_s0031, 90]         Thank President Xi | know | Xi responsible lacking    \n\n\n\n\nEinsatz zur Qualitätskontrolle\n\ntoks_transcripts %&gt;% \n  kwic(\n    phrase(\"know know\"),\n    window = 3) %&gt;%\n  tibble() %&gt;% \n  select(-pattern) %&gt;% \n  slice(35:45) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(\n        table.width = gt::pct(100), \n        table.font.size = \"10px\"\n    )\n\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\n\n\n\n\nvp2_s0723\n94\n95\nkiss just kiss\nknow know\njust kiss kiss\n\n\nvp2_s0732\n119\n120\ndefault press even\nknow know\ndifference campaign strategy\n\n\nvp3_s0151\n32\n33\ncop able assess\nknow know\nJ.D Vance lying\n\n\nvp3_s0332\n3\n4\nreally mean\nknow know\nmany people tune\n\n\nvp3_s0332\n116\n117\ngenerous Sekunden Pause\nknow know\ntype like know\n\n\nvp3_s0332\n120\n121\nknow type like\nknow know\ntype like know\n\n\nvp3_s0332\n124\n125\nknow type like\nknow know\nknow know know\n\n\nvp3_s0332\n125\n126\ntype like know\nknow know\nknow know know\n\n\nvp3_s0332\n126\n127\nlike know know\nknow know\nknow know know\n\n\nvp3_s0332\n127\n128\nknow know know\nknow know\nknow know know\n\n\nvp3_s0332\n128\n129\nknow know know\nknow know\nknow know know\n\n\n\n\n\n\n\n\n\n\nNgrams als Features definieren\n\n# Definition von Features\ncustom_ngrams &lt;- c(\"donald trump\", \"joe biden\", \"kamala harris\")\n\n# Anwendung auf DFM\ndfm_with_custom_ngrams &lt;- toks_transcripts %&gt;% \n  tokens_compound(pattern = phrase(custom_ngrams)) %&gt;% \n  dfm() %&gt;% \n  dfm_trim(min_docfreq = 0.005, max_docfreq = 0.99, docfreq_type = \"prop\") \n\n# Überprüfung\ndfm_with_custom_ngrams %&gt;% \n  convert(to = \"data.frame\") %&gt;% \n  select(doc_id, starts_with(\"donald\")) %&gt;% \n  head()\n\n    doc_id donald_trump donald\n1 p1_s0001            1      0\n2 p1_s0002            1      0\n3 p1_s0003            0      0\n4 p1_s0004            0      0\n5 p1_s0005            0      0\n6 p1_s0006            1      0\n\n\n\n\nSemantische Netzwerke: Visualisierung von Tokenbeziehungen\n\n# Lookup emotes in DFM of chats\ndfm_emotes &lt;- dfm_chats %&gt;% \n  quanteda::dfm_lookup(\n    dictionary = dict_chat_emotes)\n\n# Output frequency of emojis\ntop50_emotes &lt;- dfm_emotes %&gt;% \n  topfeatures(50) %&gt;% \n  names()\n\n# Visualize\ndfm_emotes  %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top50_emotes) %&gt;% \n  textplot_network()\n\n\n\n\n\n\n\n\n\n\nPOS-Tagging & Dependency Parsing\n\nudmodel &lt;- udpipe::udpipe_download_model(language = \"english\")\n\ntranscripts_pos &lt;- transcripts %&gt;%\n  rename(doc_id = id_sequence, text = dialogue) %&gt;% \n  udpipe::udpipe(udmodel)\n\n\ntranscripts_pos %&gt;% \n  select(doc_id, sentence_id, token_id, token, head_token_id, lemma, upos, xpos) %&gt;% \n  head(n = 7) %&gt;% \n  gt() %&gt;% gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(table.width = gt::pct(100), table.font.size = \"12px\")\n\n\n\n\n\n\n\ndoc_id\nsentence_id\ntoken_id\ntoken\nhead_token_id\nlemma\nupos\nxpos\n\n\n\n\np1_s0001\n1\n1\nTonight\n0\ntonight\nNOUN\nNN\n\n\np1_s0001\n1\n2\n,\n1\n,\nPUNCT\n,\n\n\np1_s0001\n1\n3\nthe\n7\nthe\nDET\nDT\n\n\np1_s0001\n1\n4\nhigh\n6\nhigh\nADJ\nJJ\n\n\np1_s0001\n1\n5\n-\n6\n-\nPUNCT\nHYPH\n\n\np1_s0001\n1\n6\nstakes\n7\nstake\nNOUN\nNNS\n\n\np1_s0001\n1\n7\nshowdown\n1\nshowdown\nNOUN\nNN\n\n\n\n\n\n\n\n\n\nMit welchen Wörtern wird Trump beschrieben?\n\ntranscripts_pos %&gt;% \n    filter(\n      upos == \"NOUN\" &\n      lemma == \"trump\") %&gt;%\n    inner_join(\n      transcripts_pos,\n      by = c(\n        \"doc_id\",\n        \"sentence_id\"),\n      relationship = \n        \"many-to-many\") %&gt;%\n    filter(\n      upos.y == \"ADJ\" &\n      head_token_id.y == token_id.x) %&gt;% \n    rename(\n      token_id = token_id.y,\n      token = token.y) %&gt;% \n    select(\n      doc_id, sentence_id,\n      token_id, token) %&gt;%\n    sjmisc::frq(token, sort.frq = \"desc\") \n\ntoken &lt;character&gt; \n# total N=161 valid N=161 mean=3.72 sd=4.67\n\nValue        |   N | Raw % | Valid % | Cum. %\n---------------------------------------------\ndonald       | 132 | 81.99 |   81.99 |  81.99\nDonald       |   4 |  2.48 |    2.48 |  84.47\num           |   3 |  1.86 |    1.86 |  86.34\nformer       |   2 |  1.24 |    1.24 |  87.58\nnarcissistic |   2 |  1.24 |    1.24 |  88.82\nbad          |   1 |  0.62 |    0.62 |  89.44\ngood         |   1 |  0.62 |    0.62 |  90.06\ngreat        |   1 |  0.62 |    0.62 |  90.68\niran         |   1 |  0.62 |    0.62 |  91.30\nlaura        |   1 |  0.62 |    0.62 |  91.93\nmuch         |   1 |  0.62 |    0.62 |  92.55\nokay         |   1 |  0.62 |    0.62 |  93.17\nother        |   1 |  0.62 |    0.62 |  93.79\npast         |   1 |  0.62 |    0.62 |  94.41\nSaid         |   1 |  0.62 |    0.62 |  95.03\nselfish      |   1 |  0.62 |    0.62 |  95.65\nsocial       |   1 |  0.62 |    0.62 |  96.27\ntighter      |   1 |  0.62 |    0.62 |  96.89\ntotal        |   1 |  0.62 |    0.62 |  97.52\nunfit        |   1 |  0.62 |    0.62 |  98.14\nunseat       |   1 |  0.62 |    0.62 |  98.76\nweaker       |   1 |  0.62 |    0.62 |  99.38\nweird        |   1 |  0.62 |    0.62 | 100.00\n&lt;NA&gt;         |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\ntranscripts_spacyr %&gt;%  \n    filter(\n      pos == \"NOUN\" &\n      lemma == \"trump\") %&gt;%\n    inner_join(\n      transcripts_spacyr,\n      by = c(\n        \"doc_id\",\n        \"sentence_id\"),\n      relationship = \n        \"many-to-many\") %&gt;%\n    filter(\n      pos.y == \"ADJ\" &\n      head_token_id.y == token_id.x) %&gt;% \n    rename(\n      token_id = token_id.y,\n      token = token.y) %&gt;% \n    select(\n      doc_id, sentence_id,\n      token_id, token) %&gt;%\n    sjmisc::frq(token, sort.frq = \"desc\") \n\ntoken &lt;character&gt; \n# total N=10 valid N=10 mean=5.40 sd=2.88\n\nValue        | N | Raw % | Valid % | Cum. %\n-------------------------------------------\nunfit        | 2 |    20 |      20 |     20\nbad          | 1 |    10 |      10 |     30\ndonald       | 1 |    10 |      10 |     40\nfucking      | 1 |    10 |      10 |     50\nnarcissistic | 1 |    10 |      10 |     60\nother        | 1 |    10 |      10 |     70\nsame         | 1 |    10 |      10 |     80\ntighter      | 1 |    10 |      10 |     90\ntotal        | 1 |    10 |      10 |    100\n&lt;NA&gt;         | 0 |     0 |    &lt;NA&gt; |   &lt;NA&gt;"
  },
  {
    "objectID": "tutorials/tutorial-07.html",
    "href": "tutorials/tutorial-07.html",
    "title": "🔨 Text as data in R",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "tutorials/tutorial-07.html#background",
    "href": "tutorials/tutorial-07.html#background",
    "title": "🔨 Text as data in R",
    "section": "Background",
    "text": "Background"
  },
  {
    "objectID": "tutorials/tutorial-07.html#preparation",
    "href": "tutorials/tutorial-07.html#preparation",
    "title": "🔨 Text as data in R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, taylor,\n    magrittr, janitor,\n    ggpubr, \n    gt, gtExtras,\n    countdown, \n    quanteda, # quanteda text processing\n    quanteda.textplots, \n    easystats, tidyverse\n)\n\n\n# Import data from URL\nchats &lt;- qs::qread(here(\"local_data/chat-debates_full.qs\"))$correct\ntranscripts &lt;- qs::qread(here(\"local_data/transcripts-debates_full.qs\"))$correct\nstreamer_stats &lt;- qs::qread(here(\"local_data/twitch_streamer_stats.qs\"))"
  },
  {
    "objectID": "tutorials/tutorial-07.html#codechunks-aus-der-sitzung",
    "href": "tutorials/tutorial-07.html#codechunks-aus-der-sitzung",
    "title": "🔨 Text as data in R",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nÜberblick über verschiedenen Statistiken der betrachteten Streamer\n\nstreamer_stats %&gt;% \n  pivot_longer(cols = c(avg_viewers, followers, hours_streamed), names_to = \"statistic\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = month, y = value, fill = streamer)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_grid(statistic ~ ., scales = \"free_y\", labeller = as_labeller(c(\n    avg_viewers = \"Average Viewers\",\n    followers = \"Followers\",\n    hours_streamed = \"Hours Streamed\"))) +\n  theme_minimal() +\n  labs(\n    x = \"Month\",\n    y = \"\",\n    title = \"Streamer Statistics Over Time\", \n    fill = \"Streamer\") +\n  scale_y_continuous(labels = scales::comma) +\n  ggsci::scale_fill_cosmic()\n\n\n\n\n\n\n\n\n\n\nÜberblick über den chats-Datensatz\n\nchats %&gt;% glimpse\n\nRows: 913,375\nColumns: 33\n$ streamer              &lt;chr&gt; \"hasanabi\", \"hasanabi\", \"hasanabi\", \"hasanabi\", …\n$ url                   &lt;chr&gt; \"https://www.twitch.tv/videos/2247664726\", \"http…\n$ platform              &lt;chr&gt; \"twitch\", \"twitch\", \"twitch\", \"twitch\", \"twitch\"…\n$ debate                &lt;chr&gt; \"presidential\", \"presidential\", \"presidential\", …\n$ user_name             &lt;chr&gt; \"bendaspur\", \"spackle_pirate\", \"texaschollima\", …\n$ user_id               &lt;chr&gt; \"54058406\", \"182041182\", \"185502300\", \"159018462…\n$ user_display_name     &lt;chr&gt; \"BenDaSpur\", \"spackle_pirate\", \"TexasChollima\", …\n$ user_badges           &lt;list&gt; [], [], [], [[\"twitch_recap_2023\", 1, \"Twitch R…\n$ message_timestamp     &lt;dbl&gt; 19, 19, 20, 20, 21, 21, 22, 22, 24, 25, 25, 25, …\n$ message_id            &lt;chr&gt; \"dc03b89a-722d-4eaa-a895-736533a68aca\", \"6be50e1…\n$ message_type          &lt;chr&gt; \"text_message\", \"text_message\", \"text_message\", …\n$ message_content       &lt;chr&gt; \"60fps LETSGO 60fps LETSGO 60fps LETSGO 60fps LE…\n$ message_emotes        &lt;list&gt; [], [], [], [], [], [], [], [], [], [], [], [[\"…\n$ message_length        &lt;int&gt; 51, 17, 20, 27, 35, 14, 20, 5, 10, 9, 106, 97, 3…\n$ message_timecode      &lt;Period&gt; 19S, 19S, 20S, 20S, 21S, 21S, 22S, 22S, 24S, …\n$ message_time          &lt;chr&gt; \"00:00:19\", \"00:00:19\", \"00:00:20\", \"00:00:20\", …\n$ message_during_debate &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_has_badge        &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, …\n$ user_is_premium       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, …\n$ user_is_subscriber    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ user_is_turbo         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_moderator     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …\n$ user_is_partner       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …\n$ user_is_subgifter     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_broadcaster   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_vip           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_twitchdj      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_founder       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_staff         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_game_dev      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_ambassador    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_no_audio         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_no_video         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\nchats %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n913375\n\n\nNumber of columns\n33\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n11\n\n\nlist\n2\n\n\nnumeric\n19\n\n\nTimespan\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nstreamer\n0\n1\n8\n19\n0\n3\n0\n\n\nurl\n0\n1\n39\n43\n0\n6\n0\n\n\nplatform\n0\n1\n6\n7\n0\n2\n0\n\n\ndebate\n0\n1\n12\n17\n0\n2\n0\n\n\nuser_name\n0\n1\n0\n36\n9\n89201\n0\n\n\nuser_id\n0\n1\n2\n24\n0\n89055\n0\n\n\nuser_display_name\n0\n1\n0\n36\n9\n89217\n0\n\n\nmessage_id\n0\n1\n36\n40\n0\n913375\n0\n\n\nmessage_type\n0\n1\n12\n12\n0\n1\n0\n\n\nmessage_content\n0\n1\n0\n601\n121\n589125\n1\n\n\nmessage_time\n0\n1\n8\n8\n0\n38911\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\nuser_badges\n0\n1\n707\n0\n3\n\n\nmessage_emotes\n0\n1\n15840\n0\n13\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmessage_timestamp\n0\n1\n20052.35\n11102.30\n-152\n9498\n22129\n29762\n38954\n▆▅▅▇▆\n\n\nmessage_length\n0\n1\n26.71\n31.68\n0\n7\n16\n34\n601\n▇▁▁▁▁\n\n\nmessage_during_debate\n0\n1\n0.28\n0.45\n0\n0\n0\n1\n1\n▇▁▁▁▃\n\n\nuser_has_badge\n0\n1\n0.64\n0.48\n0\n0\n1\n1\n1\n▅▁▁▁▇\n\n\nuser_is_premium\n42\n1\n0.21\n0.41\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\nuser_is_subscriber\n42\n1\n0.12\n0.32\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nuser_is_turbo\n42\n1\n0.03\n0.17\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nuser_is_moderator\n42\n1\n0.02\n0.14\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nuser_is_partner\n42\n1\n0.02\n0.13\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nuser_is_subgifter\n42\n1\n0.00\n0.00\n0\n0\n0\n0\n0\n▁▁▇▁▁\n\n\nuser_is_broadcaster\n42\n1\n0.00\n0.03\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nuser_is_vip\n42\n1\n0.00\n0.03\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nuser_is_twitchdj\n42\n1\n0.00\n0.02\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nuser_is_founder\n42\n1\n0.00\n0.02\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nuser_is_staff\n42\n1\n0.00\n0.01\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nuser_is_game_dev\n42\n1\n0.00\n0.01\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nuser_is_ambassador\n42\n1\n0.00\n0.00\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nuser_no_audio\n42\n1\n0.02\n0.14\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nuser_no_video\n42\n1\n0.01\n0.12\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\n\nVariable type: Timespan\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nmessage_timecode\n0\n1\n-59\n60\n6H 8M 49S\n17167\n\n\n\n\n\n\n\nKurzer Überblick über den transcripts-Datensatz\n\ntranscripts %&gt;% glimpse \n\nRows: 5,861\nColumns: 12\n$ id_sequence            &lt;chr&gt; \"p1_s0001\", \"p1_s0002\", \"p1_s0003\", \"p1_s0004\",…\n$ source                 &lt;chr&gt; \"presidential_debate-abc\", \"presidential_debate…\n$ speaker                &lt;chr&gt; \"S27\", \"S35\", \"S27\", \"S55\", \"S61\", \"S55\", \"S43\"…\n$ timestamp              &lt;time&gt; 00:00:00, 00:00:11, 00:00:20, 00:00:34, 00:00:…\n$ dialogue               &lt;chr&gt; \"Tonight, the high-stakes showdown here in Phil…\n$ dialogue_length        &lt;int&gt; 229, 148, 245, 91, 31, 13, 37, 102, 316, 409, 6…\n$ duration               &lt;dbl&gt; 11, 9, 14, 6, 4, 1, 4, 10, 17, 21, 28, 8, 13, 4…\n$ debate                 &lt;chr&gt; \"presidential\", \"presidential\", \"presidential\",…\n$ streamer               &lt;chr&gt; \"tv_station\", \"tv_station\", \"tv_station\", \"tv_s…\n$ id_streamer            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ id_speaker             &lt;chr&gt; \"p1_s27\", \"p1_s35\", \"p1_s27\", \"p1_s55\", \"p1_s61…\n$ sequence_during_debate &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\ntranscripts %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n5861\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n7\n\n\ndifftime\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nid_sequence\n0\n1\n8\n9\n0\n5861\n0\n\n\nsource\n0\n1\n23\n44\n0\n8\n0\n\n\nspeaker\n0\n1\n3\n4\n0\n152\n0\n\n\ndialogue\n0\n1\n2\n16523\n0\n5697\n0\n\n\ndebate\n0\n1\n12\n17\n0\n2\n0\n\n\nstreamer\n0\n1\n8\n19\n0\n4\n0\n\n\nid_speaker\n0\n1\n6\n8\n0\n640\n0\n\n\n\nVariable type: difftime\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ntimestamp\n0\n1\n0 secs\n38738 secs\n03:33:08\n5433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndialogue_length\n0\n1\n460.75\n887.09\n2\n66\n163\n507\n16523\n▇▁▁▁▁\n\n\nduration\n8\n1\n30.72\n59.80\n0\n5\n12\n33\n1079\n▇▁▁▁▁\n\n\nid_streamer\n0\n1\n2.73\n1.11\n1\n2\n2\n4\n4\n▃▇▁▂▇\n\n\nsequence_during_debate\n0\n1\n0.29\n0.45\n0\n0\n0\n1\n1\n▇▁▁▁▃\n\n\n\n\n\n\n\nArbeiten mit quanteda: corpus\n\n# Create corpus\ncorp_transcripts &lt;- transcripts %&gt;% \n  quanteda::corpus(\n    docid_field = \"id_sequence\", \n    text_field = \"dialogue\"\n  )\n\n# Output\ncorp_transcripts\n\nCorpus consisting of 5,861 documents and 10 docvars.\np1_s0001 :\n\"Tonight, the high-stakes showdown here in Philadelphia betwe...\"\n\np1_s0002 :\n\"A historic race for president upended just weeks ago, Presid...\"\n\np1_s0003 :\n\"The candidates separated by the smallest of margins, essenti...\"\n\np1_s0004 :\n\"This is an ABC News special. The most consequential moment o...\"\n\np1_s0005 :\n\"Together, we'll chart a... (..)\"\n\np1_s0006 :\n\"Donald Trump.\"\n\n[ reached max_ndoc ... 5,855 more documents ]\n\n\n\n\nEinfluss der Preporcessing-Schritte am Beispiel\n\nEinfache Tokenisierung\n\n# Tokenize corpus\ntoks_simple &lt;- corp_transcripts %&gt;% \n  quanteda::tokens() \n\n# Output\nhead(toks_simple[[1]], 100)\n\n [1] \"Tonight\"      \",\"            \"the\"          \"high-stakes\"  \"showdown\"    \n [6] \"here\"         \"in\"           \"Philadelphia\" \"between\"      \"Vice\"        \n[11] \"President\"    \"Kamala\"       \"Harris\"       \"and\"          \"former\"      \n[16] \"President\"    \"Donald\"       \"Trump\"        \".\"            \"Their\"       \n[21] \"first\"        \"face-to-face\" \"meeting\"      \"in\"           \"this\"        \n[26] \"presidential\" \"election\"     \",\"            \"their\"        \"first\"       \n[31] \"face-to-face\" \"meeting\"      \"ever\"         \".\"           \n\n\n\n\nmit Entfernung von Satz- und Sonderzeichen\n\ntoks_nopunct &lt;- corp_transcripts %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  )\n\nhead(toks_nopunct[[1]], 100)\n\n [1] \"Tonight\"      \"the\"          \"high-stakes\"  \"showdown\"     \"here\"        \n [6] \"in\"           \"Philadelphia\" \"between\"      \"Vice\"         \"President\"   \n[11] \"Kamala\"       \"Harris\"       \"and\"          \"former\"       \"President\"   \n[16] \"Donald\"       \"Trump\"        \"Their\"        \"first\"        \"face-to-face\"\n[21] \"meeting\"      \"in\"           \"this\"         \"presidential\" \"election\"    \n[26] \"their\"        \"first\"        \"face-to-face\" \"meeting\"      \"ever\"        \n\n\n\n\nund ohne Stopwörter\n\ntoks_nostopw &lt;- corp_transcripts %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  ) %&gt;% \n  quanteda::tokens_remove(\n    pattern = quanteda::stopwords(\"en\")\n  )\n\nhead(toks_nostopw[[1]], 100)\n\n [1] \"Tonight\"      \"high-stakes\"  \"showdown\"     \"Philadelphia\" \"Vice\"        \n [6] \"President\"    \"Kamala\"       \"Harris\"       \"former\"       \"President\"   \n[11] \"Donald\"       \"Trump\"        \"first\"        \"face-to-face\" \"meeting\"     \n[16] \"presidential\" \"election\"     \"first\"        \"face-to-face\" \"meeting\"     \n[21] \"ever\"        \n\n\n\n\nDirekter Vergleich\n\nhead(toks_simple[[1]], 100)\n\n [1] \"Tonight\"      \",\"            \"the\"          \"high-stakes\"  \"showdown\"    \n [6] \"here\"         \"in\"           \"Philadelphia\" \"between\"      \"Vice\"        \n[11] \"President\"    \"Kamala\"       \"Harris\"       \"and\"          \"former\"      \n[16] \"President\"    \"Donald\"       \"Trump\"        \".\"            \"Their\"       \n[21] \"first\"        \"face-to-face\" \"meeting\"      \"in\"           \"this\"        \n[26] \"presidential\" \"election\"     \",\"            \"their\"        \"first\"       \n[31] \"face-to-face\" \"meeting\"      \"ever\"         \".\"           \n\nhead(toks_nopunct[[1]], 100)\n\n [1] \"Tonight\"      \"the\"          \"high-stakes\"  \"showdown\"     \"here\"        \n [6] \"in\"           \"Philadelphia\" \"between\"      \"Vice\"         \"President\"   \n[11] \"Kamala\"       \"Harris\"       \"and\"          \"former\"       \"President\"   \n[16] \"Donald\"       \"Trump\"        \"Their\"        \"first\"        \"face-to-face\"\n[21] \"meeting\"      \"in\"           \"this\"         \"presidential\" \"election\"    \n[26] \"their\"        \"first\"        \"face-to-face\" \"meeting\"      \"ever\"        \n\nhead(toks_nostopw[[1]], 100)\n\n [1] \"Tonight\"      \"high-stakes\"  \"showdown\"     \"Philadelphia\" \"Vice\"        \n [6] \"President\"    \"Kamala\"       \"Harris\"       \"former\"       \"President\"   \n[11] \"Donald\"       \"Trump\"        \"first\"        \"face-to-face\" \"meeting\"     \n[16] \"presidential\" \"election\"     \"first\"        \"face-to-face\" \"meeting\"     \n[21] \"ever\"        \n\n\n\n\n\nTokenisierung von Bi & Skipgrams\n\n# Bigrams\ntoks_nostopw %&gt;% \n  tokens_ngrams(n = 2) %&gt;% \n  .[[1]]\n\n [1] \"Tonight_high-stakes\"   \"high-stakes_showdown\"  \"showdown_Philadelphia\"\n [4] \"Philadelphia_Vice\"     \"Vice_President\"        \"President_Kamala\"     \n [7] \"Kamala_Harris\"         \"Harris_former\"         \"former_President\"     \n[10] \"President_Donald\"      \"Donald_Trump\"          \"Trump_first\"          \n[13] \"first_face-to-face\"    \"face-to-face_meeting\"  \"meeting_presidential\" \n[16] \"presidential_election\" \"election_first\"        \"first_face-to-face\"   \n[19] \"face-to-face_meeting\"  \"meeting_ever\"         \n\n\n\n# Skipgrams\ntoks_nostopw %&gt;% \n  tokens_ngrams(n = 2, skip = 0:1) %&gt;% \n  .[[1]]\n\n [1] \"Tonight_high-stakes\"       \"Tonight_showdown\"         \n [3] \"high-stakes_showdown\"      \"high-stakes_Philadelphia\" \n [5] \"showdown_Philadelphia\"     \"showdown_Vice\"            \n [7] \"Philadelphia_Vice\"         \"Philadelphia_President\"   \n [9] \"Vice_President\"            \"Vice_Kamala\"              \n[11] \"President_Kamala\"          \"President_Harris\"         \n[13] \"Kamala_Harris\"             \"Kamala_former\"            \n[15] \"Harris_former\"             \"Harris_President\"         \n[17] \"former_President\"          \"former_Donald\"            \n[19] \"President_Donald\"          \"President_Trump\"          \n[21] \"Donald_Trump\"              \"Donald_first\"             \n[23] \"Trump_first\"               \"Trump_face-to-face\"       \n[25] \"first_face-to-face\"        \"first_meeting\"            \n[27] \"face-to-face_meeting\"      \"face-to-face_presidential\"\n[29] \"meeting_presidential\"      \"meeting_election\"         \n[31] \"presidential_election\"     \"presidential_first\"       \n[33] \"election_first\"            \"election_face-to-face\"    \n[35] \"first_face-to-face\"        \"first_meeting\"            \n[37] \"face-to-face_meeting\"      \"face-to-face_ever\"        \n[39] \"meeting_ever\"             \n\n\n\n\nKollokationen für Identifkation prominenter Bigramme\n\ntoks_nostopw %&gt;% \n  quanteda.textstats::textstat_collocations(\n    size = 2, \n    min_count = 5\n  ) %&gt;% \n  head(25)\n\n        collocation count count_nested length    lambda        z\n1         know know  1337            0      2  3.890787 98.31370\n2        saying bad   558            0      2  6.503305 81.19215\n3        bad saying   553            0      2  6.481795 80.98625\n4         going say   666            0      2  4.555737 80.92246\n5         say going   661            0      2  4.562963 80.82524\n6      donald trump   755            0      2  7.422847 75.19267\n7     kamala harris   494            0      2  7.873933 68.88003\n8    vice president   429            0      2  7.258104 56.72753\n9             oh oh   229            0      2  4.679549 54.95066\n10        right now   269            0      2  3.996328 53.27167\n11    senator vance   129            0      2  6.583164 49.48173\n12       little bit   132            0      2  8.268099 45.80914\n13 president harris   186            0      2  4.027681 45.69845\n14           oh god   154            0      2  6.175930 44.94423\n15        years ago   102            0      2  6.436807 43.06424\n16         tim walz    90            0      2  7.588398 43.05596\n17  president trump   203            0      2  3.468589 42.70406\n18       four years   100            0      2  6.381600 42.53221\n19      health care   136            0      2  7.400206 41.83123\n20      white house    85            0      2  8.071701 41.52827\n21   donald trump's   132            0      2  6.408658 41.05938\n22 former president   141            0      2  5.790480 41.04486\n23  curious curious   373            0      2 11.836611 40.77202\n24    governor walz    77            0      2  6.512020 39.65499\n25      two minutes    84            0      2  6.490910 39.39074\n\n\n\n\nAnwendung der DFM\n\n# Check top 25 features\ntoks_nostopw %&gt;%\n  quanteda::dfm() %&gt;% \n  quanteda.textstats::textstat_frequency(\n    n = 25) \n\n     feature frequency rank docfreq group\n1       like      6350    1    1617   all\n2       know      3872    2    1325   all\n3      think      2932    3    1202   all\n4     people      2710    4    1158   all\n5       yeah      2551    5    1107   all\n6      going      2364    6     956   all\n7       just      2145    7    1322   all\n8        say      1632    8     715   all\n9      right      1556    9     930   all\n10     trump      1492   10     859   all\n11       one      1461   11     958   all\n12 president      1372   12     787   all\n13       get      1320   13     850   all\n14      said      1262   14     822   all\n15       now      1222   15     883   all\n16      want      1207   16     755   all\n17       can      1137   17     723   all\n18    really      1137   17     620   all\n19        uh      1134   19     421   all\n20   fucking      1074   20     522   all\n21       lot      1049   21     632   all\n22    saying      1042   22     376   all\n23        oh      1003   23     546   all\n24      well       974   24     740   all\n25       bad       963   25     251   all\n\n\n\nBeispiel für den Loop des (Pre-)Processing\n\n# Customize stopwords\ncustom_stopwords &lt;- c(\"uh\", \"oh\")\n\n# Remove custom stopwords\ntoks_no_custom_stopw &lt;- toks_nostopw %&gt;% \n  quanteda::tokens_remove(\n    pattern = custom_stopwords\n  )\n\n# Check top 25 features\ntoks_no_custom_stopw %&gt;%\n  quanteda::dfm() %&gt;% \n  quanteda.textstats::textstat_frequency(\n    n = 25) \n\n     feature frequency rank docfreq group\n1       like      6350    1    1617   all\n2       know      3872    2    1325   all\n3      think      2932    3    1202   all\n4     people      2710    4    1158   all\n5       yeah      2551    5    1107   all\n6      going      2364    6     956   all\n7       just      2145    7    1322   all\n8        say      1632    8     715   all\n9      right      1556    9     930   all\n10     trump      1492   10     859   all\n11       one      1461   11     958   all\n12 president      1372   12     787   all\n13       get      1320   13     850   all\n14      said      1262   14     822   all\n15       now      1222   15     883   all\n16      want      1207   16     755   all\n17       can      1137   17     723   all\n18    really      1137   17     620   all\n19   fucking      1074   19     522   all\n20       lot      1049   20     632   all\n21    saying      1042   21     376   all\n22      well       974   22     740   all\n23       bad       963   23     251   all\n24      mean       935   24     557   all\n25       way       905   25     572   all\n\n\n\n\n\nVerschiedene Analysen auf Basis der DFM\n\nAuswahl bestimmter Muster/Features\n\n# Create corpus\ncorp_chats &lt;- chats %&gt;% \n  quanteda::corpus(\n    docid_field = \"message_id\", \n    text_field = \"message_content\"\n  )\n\n# Create DFM\ndfm_chats &lt;- corp_chats %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  ) %&gt;% \n  quanteda::dfm() \n\n# Output\ndfm_chats %&gt;% \n  quanteda::dfm_select(pattern = \"@*\") %&gt;% \n  quanteda.textstats::textstat_frequency(\n    n = 25) \n\n            feature frequency rank docfreq group\n1         @hasanabi     29173    1   28371   all\n2        @zackrawrr     11430    2   11381   all\n3       @gizmomacks       246    3     243   all\n4         @toxicsjw       167    4     167   all\n5       @beteljuice       158    5     158   all\n6       @megaphonix       154    6     150   all\n7            @hasan        76    7      76   all\n8  @depressedaether        68    8      68   all\n9     @nicebathroom        68    8      68   all\n10    @littlebear36        64   10      61   all\n11   @david_leonard        61   11      61   all\n12     @hasandpiker        58   12      58   all\n13      @sambarty2k        58   12      58   all\n14         @tiamani        55   14      55   all\n15     @matefeedart        50   15      50   all\n16           @wihby        47   16      47   all\n17      @freejam013        44   17      44   all\n18      @austinshow        42   18      42   all\n19         @mf_jewm        41   19      41   all\n20            @chat        41   19      33   all\n21    @lakemcgroove        37   21      37   all\n22            @mhud        37   21      37   all\n23  @thistwitchname        36   23      36   all\n24     @mangobreezy        35   24      35   all\n25        @mijnboot        35   24      35   all\n\n\n\n\n\nGezielte Suche nach spezifischen Worten\n\n# Load custom emoji-dictionary\ndict_chat_emotes &lt;- readRDS(here(\"local_data/dictionary_chat_emotes.RDS\"))\n\n# Output\ndict_chat_emotes\n\nDictionary object with 5546 key entries.\n- [0Unroll]:\n  - 0unroll\n- [1]:\n  - 1\n- [2020Celebrate]:\n  - 2020celebrate\n- [2020Forward]:\n  - 2020forward\n- [2020Glitchy]:\n  - 2020glitchy\n- [2020Pajamas]:\n  - 2020pajamas\n[ reached max_nkey ... 5,540 more keys ]\n\n\n\n# Lookup emojis in DFM of chats\ndfm_emotes &lt;- dfm_chats %&gt;% \n  quanteda::dfm_lookup(\n    dictionary = dict_chat_emotes)\n\n# Output frequency of emojis\ndfm_emotes %&gt;% \n  quanteda.textstats::textstat_frequency(\n    n = 25) \n\n         feature frequency rank docfreq group\n1            LUL     20194    1   14967   all\n2           hasL     12455    2    5856   all\n3    bleedPurple      5188    3    5174   all\n4          Kappa      4971    4    4240   all\n5        hasSlam      2989    5    1002   all\n6    NotLikeThis      2341    6    1354   all\n7             🇵🇸      1968    7     780   all\n8        hasChud      1792    8    1206   all\n9          hasHi      1401    9     851   all\n10          hasO      1375   10     609   all\n11       hasBoot      1209   11     551   all\n12       hasRaid      1092   12     470   all\n13      elbyBlom      1001   13    1001   all\n14       WutFace       964   14     687   all\n15     hasBaited       901   15     396   all\n16       hasMods       853   16     604   all\n17          Guns       755   17     728   all\n18      hasKkona       727   18     384   all\n19     DinoDance       721   19     269   all\n20       PopNemo       709   20     334   all\n21 TwitchConHYPE       630   21     236   all\n22      hasSadge       601   22     481   all\n23      has0head       599   23     301   all\n24       hasFlex       569   24     294   all\n25             e       563   25     496   all"
  },
  {
    "objectID": "tutorials/tutorial-03.html",
    "href": "tutorials/tutorial-03.html",
    "title": "🔨 Working with R",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "tutorials/tutorial-03.html#background",
    "href": "tutorials/tutorial-03.html#background",
    "title": "🔨 Working with R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: Hollywood Age Gaps\n\n\n\n\nAn informational site showing the age gap between movie love interests.\n\nThe data follows certain rules:\n\nThe two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)\nThe youngest of the two actors is at least 17 years old\nNot animated characters\n\n\n\n\nThe best way to learn R is by trying. This document tries to display a version of the “normal” data processing procedure.\nUse tidytuesday data as an example to showcase the potential"
  },
  {
    "objectID": "tutorials/tutorial-03.html#packages",
    "href": "tutorials/tutorial-03.html#packages",
    "title": "🔨 Working with R",
    "section": "Packages",
    "text": "Packages\n\nThe pacman::p_load() package is used to load the packages, which has several advantages over the conventional method with library():\nConcise syntax\nAutomatic installation (if the package is not already installed)\nLoading multiple packages at once\nAutomatic search for dependencies\n\n\npacman::p_load(\n  here, \n  magrittr, \n  tidyverse,\n  janitor,\n  easystats,\n  sjmisc,\n  ggpubr)"
  },
  {
    "objectID": "tutorials/tutorial-03.html#codechunks-aus-der-sitzung",
    "href": "tutorials/tutorial-03.html#codechunks-aus-der-sitzung",
    "title": "🔨 Working with R",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nDie erste “Runde” der Datenaufbereitung\n\nDatenimport via URL\n\nVariablennamen und -beschreibungen\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nmovie_name\nName of the film\n\n\nrelease_year\nRelease year\n\n\ndirector\nDirector of the film\n\n\nage_difference\nAge difference between the characters in whole years\n\n\ncouple_number\nAn identifier for the couple in case multiple couples are listed for this film\n\n\nactor_1_name\nThe name of the older actor in this couple\n\n\nactor_2_name\nThe name of the younger actor in this couple\n\n\nactor_1_birthdate\nThe birthdate of the older member of the couple\n\n\nactor_2_birthdate\nThe birthdate of the younger member of the couple\n\n\nactor_1_age\nThe age of the older actor when the film was released\n\n\nactor_2_age\nThe age of the younger actor when the film was released\n\n\n\n\n# Import data from URL\nage_gaps &lt;- read_csv(\"http://hollywoodagegap.com/movies.csv\") %&gt;% \n  janitor::clean_names()\n\n# Check data set\nage_gaps\n\n# A tibble: 1,199 × 12\n   movie_name   release_year director age_difference actor_1_name actor_1_gender\n   &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;         \n 1 Harold and …         1971 Hal Ash…             52 Bud Cort     man           \n 2 Venus                2006 Roger M…             50 Peter O'Too… man           \n 3 The Quiet A…         2002 Phillip…             49 Michael Cai… man           \n 4 Solitary Man         2009 Brian K…             45 Michael Dou… man           \n 5 The Big Leb…         1998 Joel Co…             45 David Huddl… man           \n 6 Beginners            2010 Mike Mi…             43 Christopher… man           \n 7 Poison Ivy           1992 Katt Sh…             42 Tom Skerritt man           \n 8 Dirty Grand…         2016 Dan Maz…             41 Robert De N… man           \n 9 Whatever Wo…         2009 Woody A…             40 Larry David  man           \n10 Entrapment           1999 Jon Ami…             39 Sean Connery man           \n# ℹ 1,189 more rows\n# ℹ 6 more variables: actor_1_birthdate &lt;date&gt;, actor_1_age &lt;dbl&gt;,\n#   actor_2_name &lt;chr&gt;, actor_2_gender &lt;chr&gt;, actor_2_birthdate &lt;chr&gt;,\n#   actor_2_age &lt;dbl&gt;\n\n\n\n\nInitiale Überprüfung der Daten\n\n\n\n\n\n\nSind die Daten “technisch korrekt”?\n\n\n\n\n✅ Wie viele Fälle sind enthalten? Wie viele Variablen?\n✅ Wie lauten die Variablennamen? Sind sie sinnvoll?\n✅ Welchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\n✅ Wie viele eindeutige Werte hat jede Variable?\n✅ Welcher Wert tritt am häufigsten auf, und wie oft kommt er vor?\n✅ Gibt es fehlende Werte? Wenn ja, wie häufig ist dies der Fall?\n\n\n\n\nÜberblick über die Daten\n\nage_gaps %&gt;% glimpse()\n\nRows: 1,199\nColumns: 12\n$ movie_name        &lt;chr&gt; \"Harold and Maude\", \"Venus\", \"The Quiet American\", \"…\n$ release_year      &lt;dbl&gt; 1971, 2006, 2002, 2009, 1998, 2010, 1992, 2016, 2009…\n$ director          &lt;chr&gt; \"Hal Ashby\", \"Roger Michell\", \"Phillip Noyce\", \"Bria…\n$ age_difference    &lt;dbl&gt; 52, 50, 49, 45, 45, 43, 42, 41, 40, 39, 38, 38, 36, …\n$ actor_1_name      &lt;chr&gt; \"Bud Cort\", \"Peter O'Toole\", \"Michael Caine\", \"Micha…\n$ actor_1_gender    &lt;chr&gt; \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"ma…\n$ actor_1_birthdate &lt;date&gt; 1948-03-29, 1932-08-02, 1933-03-14, 1944-09-25, 193…\n$ actor_1_age       &lt;dbl&gt; 23, 74, 69, 65, 68, 81, 59, 73, 62, 69, 57, 77, 59, …\n$ actor_2_name      &lt;chr&gt; \"Ruth Gordon\", \"Jodie Whittaker\", \"Do Thi Hai Yen\", …\n$ actor_2_gender    &lt;chr&gt; \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"man\", …\n$ actor_2_birthdate &lt;chr&gt; \"1896-10-30\", \"1982-06-03\", \"1982-10-01\", \"1989-01-0…\n$ actor_2_age       &lt;dbl&gt; 75, 24, 20, 20, 23, 38, 17, 32, 22, 30, 19, 39, 23, …\n\n\n\n\nKorrekturen\n\nage_gaps_correct &lt;- age_gaps %&gt;% \n  mutate(\n    across(ends_with(\"_birthdate\"), ~as.Date(.)) # set dates to dates\n  )\n\n\n\nÜberprüfung Lageparameter\n\nage_gaps_correct %&gt;% descr()\n\n\n## Basic descriptive statistics\n\n            var    type          label    n NA.prc    mean    sd   se   md\n   release_year numeric   release_year 1199      0 2000.53 17.07 0.49 2004\n age_difference numeric age_difference 1199      0   10.62  8.62 0.25    8\n    actor_1_age numeric    actor_1_age 1199      0   40.07 10.93 0.32   39\n    actor_2_age numeric    actor_2_age 1199      0   31.22  8.47 0.24   30\n trimmed          range iqr  skew\n 2003.48 89 (1935-2024)  15 -1.62\n    9.55      52 (0-52)  12  1.19\n   39.51     64 (17-81)  15  0.54\n   30.38     64 (17-81)  10  1.39\n\n\n\n\n\n\nDie ersten Datenexplorationen\n\nWie sind die Altersunterschiede verteilt?\n\nage_gaps_correct %&gt;% \n    ggplot(aes(x = age_difference)) +\n    geom_bar() +\n    theme_pubr()\n\n\n\n\n\n\n\n\n\n\nIn welchen Filmen ist der Altersunterschied am höchsten?\n\nage_gaps_correct %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(movie_name, age_difference, release_year) \n\n# A tibble: 1,199 × 3\n   movie_name         age_difference release_year\n   &lt;chr&gt;                       &lt;dbl&gt;        &lt;dbl&gt;\n 1 Harold and Maude               52         1971\n 2 Venus                          50         2006\n 3 The Quiet American             49         2002\n 4 Solitary Man                   45         2009\n 5 The Big Lebowski               45         1998\n 6 Beginners                      43         2010\n 7 Poison Ivy                     42         1992\n 8 Dirty Grandpa                  41         2016\n 9 Whatever Works                 40         2009\n10 Entrapment                     39         1999\n# ℹ 1,189 more rows\n\n\n\nage_gaps_correct %&gt;% \n    filter(release_year &gt;= 2022) %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(\n        movie_name, age_difference, release_year, \n        actor_1_name, actor_2_name) \n\n# A tibble: 19 × 5\n   movie_name              age_difference release_year actor_1_name actor_2_name\n   &lt;chr&gt;                            &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       \n 1 Poor Things                         21         2023 Mark Ruffalo Emma Stone  \n 2 The Bubble                          21         2022 Pedro Pascal Maria Bakal…\n 3 Oppenheimer                         20         2023 Cillian Mur… Florence Pu…\n 4 The Northman                        20         2022 Alexander S… Anya Taylor…\n 5 Spaceman                            19         2024 Adam Sandler Carey Mulli…\n 6 The Lost City                       16         2022 Channing Ta… Sandra Bull…\n 7 We Live in Time                     13         2024 Andrew Garf… Florence Pu…\n 8 The Idea of You                     12         2024 Nicholas Ga… Anne Hathaw…\n 9 Barbie                              10         2023 Ryan Gosling Margot Robb…\n10 Twisters                            10         2024 Glen Powell  Daisy Edgar…\n11 Anyone but You                       9         2023 Glen Powell  Sydney Swee…\n12 Everything Everywhere …              9         2022 Ke Huy Quan  Michelle Ye…\n13 Top Gun: Maverick                    8         2022 Tom Cruise   Jennifer Co…\n14 Oppenheimer                          7         2023 Cillian Mur… Emily Blunt \n15 Your Place or Mine                   7         2023 Ashton Kutc… Zoë Chao    \n16 Your Place or Mine                   5         2023 Jesse Willi… Reese Withe…\n17 Poor Things                          2         2023 Christopher… Emma Stone  \n18 Your Place or Mine                   2         2023 Ashton Kutc… Reese Withe…\n19 You People                           1         2023 Jonah Hill   Lauren Lond…\n\n\n\n\nGibt es einen Zusammenhang zwischen Altersunterschied und Release?\n\n(Durchschnitts-)Unterschied nach Jahren\n\nage_gaps_correct %&gt;% \n    group_by(release_year) %&gt;% \n    summarise(age_difference_mean = mean(age_difference)) %&gt;% \n    ggplot(aes(release_year, age_difference_mean)) +\n    geom_col() +\n    theme_pubr()\n\n\n\n\n\n\n\n\n\n\nVerteilung nach Jahren\n\nggpubr::ggboxplot(\n    data = age_gaps_correct, \n    x = \"release_year\", \n    y = \"age_difference\", \n  ) + \n   # Rotate x-axis labels by 90 degrees\n  theme(\n    axis.text.x = element_text(\n        angle = 90,\n        vjust = 0.5,\n         hjust=1))  \n\n\n\n\n\n\n\n\n\n\nÜberprüfung der Korrelation\n\nage_gaps %&gt;%\n  select(release_year, age_difference) %&gt;% \n  correlation::correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1   |     Parameter2 |     r |         95% CI | t(1197) |         p\n----------------------------------------------------------------------------\nrelease_year | age_difference | -0.22 | [-0.27, -0.17] |   -7.83 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 1199\n\n\n\n\nSchätzung OLS\n\n# Schätzung des Models\nmdl &lt;- lm(age_difference ~ release_year, data = age_gaps_correct)\n\n# Output\nmdl %&gt;% parameters::parameters()\n\nParameter    | Coefficient |    SE |           95% CI | t(1197) |      p\n------------------------------------------------------------------------\n(Intercept)  |      233.69 | 28.48 | [177.82, 289.57] |    8.21 | &lt; .001\nrelease year |       -0.11 |  0.01 | [ -0.14,  -0.08] |   -7.83 | &lt; .001\n\nmdl %&gt;% performance::model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n8512.891 | 8512.911 | 8528.159 | 0.049 |     0.048 | 8.403 | 8.410\n\n\n\nmdl %&gt;% report::report()\n\nWe fitted a linear model (estimated using OLS) to predict age_difference with\nrelease_year (formula: age_difference ~ release_year). The model explains a\nstatistically significant and weak proportion of variance (R2 = 0.05, F(1,\n1197) = 61.35, p &lt; .001, adj. R2 = 0.05). The model's intercept, corresponding\nto release_year = 0, is at 233.69 (95% CI [177.82, 289.57], t(1197) = 8.21, p &lt;\n.001). Within this model:\n\n  - The effect of release year is statistically significant and negative (beta =\n-0.11, 95% CI [-0.14, -0.08], t(1197) = -7.83, p &lt; .001; Std. beta = -0.22, 95%\nCI [-0.28, -0.17])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "exercises/exercise-07.html",
    "href": "exercises/exercise-07.html",
    "title": "Twitch Chat Analysis",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "exercises/exercise-07.html#background",
    "href": "exercises/exercise-07.html#background",
    "title": "Twitch Chat Analysis",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: Twitch Chat & Transcripts\n\n\n\nTranscripts & Chats of the Live-Streams from  hasanabi and  zackrawrr and | TheMajorityReport for the Presidential (Harris vs. Trump) and Vice-Presidential (Vance vs. Walz) Debates 2024\n\n\n\nThe best way to learn R is by trying. This document tries to display a version of the “normal” data processing procedure.\nUse tidytuesday data as an example to showcase the potential"
  },
  {
    "objectID": "exercises/exercise-07.html#preparation",
    "href": "exercises/exercise-07.html#preparation",
    "title": "Twitch Chat Analysis",
    "section": "Preparation",
    "text": "Preparation\n\nPackages\nThe pacman::p_load() function from the pacman package is used to load the packages, which has several advantages over the conventional method with library():\n\nConcise syntax\nAutomatic installation (if the package is not already installed)\nLoading multiple packages at once\nAutomatic search for dependencies\n\n\npacman::p_load(\n    here, taylor,\n    magrittr, janitor,\n    ggpubr, \n    gt, gtExtras,\n    countdown, \n    quanteda, # quanteda text processing\n    quanteda.textplots, \n    easystats, tidyverse\n)\n\n\n\nImport und Vorverarbeitung der Daten\n\nchats &lt;- qs::qread(here(\"local_data/chat-debates_full.qs\"))$correct\ntranscripts &lt;- qs::qread(here(\"local_data/transcripts-debates_full.qs\"))$correct"
  },
  {
    "objectID": "exercises/exercise-07.html#praktische-übung",
    "href": "exercises/exercise-07.html#praktische-übung",
    "title": "Twitch Chat Analysis",
    "section": "🛠️ Praktische Übung",
    "text": "🛠️ Praktische Übung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation ausgeführt haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf  des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in das Tutorial (.qmd oder .html). Beim Tutorial handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Tutorial also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden."
  },
  {
    "objectID": "exercises/exercise-07.html#kennenlernen-des-chat-datensatzes",
    "href": "exercises/exercise-07.html#kennenlernen-des-chat-datensatzes",
    "title": "Twitch Chat Analysis",
    "section": "🔎 Kennenlernen des Chat-Datensatzes",
    "text": "🔎 Kennenlernen des Chat-Datensatzes\n\n📋 Exercise 1: Create corpus\n\nCreate new dataset corp_chats\n\nBased on the dataset chats, create a corpus object with the quanteda package.\nUse the corpus() function with the docid_field argument set to “message_id” and the text_field argument set to “message_content”.\nCheck if the transformation was successful by using the summary() function.\n\n\n\n# INSERT CODE HERE\n\n\n\n📋 Exercise 2: Tokenization & DFM conversion\n\nCreate new datasets toks_chats & dfm_chats\n\n\nBased on the dataset corp_chats, create tokens using the tokens() function from the quanteda package.\nConvert the tokens to a document-feature matrix (DFM) using the dfm() function from the quanteda package.\nCheck if the transformations were successful (e.g. by using the print() function).\n\n\n# INSERT CODE HERE\n\n\n\n📋 Exercise 3: Analyse DFM\n\nBased on dfm_chats\n\nUse the textstat_frequency() function from the quanteda package to get the top 50 tokens.\nDisplay the results.\n\nBased on the results, what preprocessing steps could be useful?\n\n\n# INSERT CODE HERE\n\n\n\n📋 Exercise 4: Preprocessing\n\nCreate a new dataset dfm_chats_preprocessed\n\nBased on corp_chats, preprocess the data according to the steps you think are necessary (e.g. removing punctuation, symbols, numbers, URLs, and stopwords).\nDepending on the steps you choose, you might need to use the tokens_remove() function from the quanteda package.\nCreate a new DFM object dfm_chats_preprocessed.\nUse the textstat_frequency() function from the quanteda package on the newly created dataset to get the top 50 tokens and compare the result with the results of Exercise 3.\n\n\n\n# INSERT CODE HERE"
  },
  {
    "objectID": "exercises/exercise-08_solution.html",
    "href": "exercises/exercise-08_solution.html",
    "title": "Advanced Twitch Chat Analysis",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "exercises/exercise-08_solution.html#background",
    "href": "exercises/exercise-08_solution.html#background",
    "title": "Advanced Twitch Chat Analysis",
    "section": "Background",
    "text": "Background"
  },
  {
    "objectID": "exercises/exercise-08_solution.html#todayss-data-basis-advanced-text-analysis",
    "href": "exercises/exercise-08_solution.html#todayss-data-basis-advanced-text-analysis",
    "title": "Advanced Twitch Chat Analysis",
    "section": "Todays’s data basis: Advanced Text Analysis",
    "text": "Todays’s data basis: Advanced Text Analysis\nTranscripts & Chats of the Live-Streams from  hasanabi and  zackrawrr and | TheMajorityReport for the Presidential (Harris vs. Trump) and Vice-Presidential (Vance vs. Walz) Debates 2024"
  },
  {
    "objectID": "exercises/exercise-08_solution.html#preparation",
    "href": "exercises/exercise-08_solution.html#preparation",
    "title": "Advanced Twitch Chat Analysis",
    "section": "Preparation",
    "text": "Preparation\n\nPackages\nThe pacman::p_load() function from the pacman package is used to load the packages, which has several advantages over the conventional method with library():\n\nConcise syntax\nAutomatic installation (if the package is not already installed)\nLoading multiple packages at once\nAutomatic search for dependencies\n\n\npacman::p_load(\n    here, taylor,\n    magrittr, janitor,\n    ggpubr, \n    gt, gtExtras,\n    countdown, \n    quanteda, # quanteda text processing\n    quanteda.textplots, \n    easystats, tidyverse\n)\n\n\n\nImport und Vorverarbeitung der Daten\n\nchats &lt;- qs::qread(here(\"local_data/chats.qs\"))\ntranscripts &lt;- qs::qread(here(\"local_data/transcripts.qs\"))\nchats_spacyr &lt;- qs::qread(here(\"local_data/chat-corpus_spacyr.qs\"))"
  },
  {
    "objectID": "exercises/exercise-08_solution.html#praktische-übung",
    "href": "exercises/exercise-08_solution.html#praktische-übung",
    "title": "Advanced Twitch Chat Analysis",
    "section": "🛠️ Praktische Übung",
    "text": "🛠️ Praktische Übung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation ausgeführt haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf  des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in das Tutorial (.qmd oder .html). Beim Tutorial handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Tutorial also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden."
  },
  {
    "objectID": "exercises/exercise-08_solution.html#kennenlernen-des-chat-datensatzes",
    "href": "exercises/exercise-08_solution.html#kennenlernen-des-chat-datensatzes",
    "title": "Advanced Twitch Chat Analysis",
    "section": "🔎 Kennenlernen des Chat-Datensatzes",
    "text": "🔎 Kennenlernen des Chat-Datensatzes\n\n📋 Exercise 1: Create corpus, token & DFM\n\nCreate new dataset corp_chats\n\nBased on the dataset chats, create a corpus object with the quanteda package.\nUse the corpus() function with the docid_field argument set to “message_id” and the text_field argument set to “message_content”.\n\nCreate new dataset toks_chats\n\nBased on the dataset corp_chats, create tokens using the tokens() function from the quanteda package, including the removal of punctuation, symbols, numbers, URLs, and stopwords.\nUse the tokens_remove() function to remove stopwords (en).\n\nCreate new dataset dfm_chats\n\nConvert the tokens to a document-feature matrix (DFM) using the dfm() function from the quanteda package.\n\n\n\n# Create corpus \ncorp_chats &lt;- chats %&gt;% \n  quanteda::corpus(\n    docid_field = \"message_id\", \n    text_field = \"message_content\"\n  )\n\n# Create tokens\ntoks_chats &lt;- corp_chats %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  ) %&gt;% \n  quanteda::tokens_remove(\n    pattern = quanteda::stopwords(\"en\")\n  ) \n\n# Create DFM\ndfm_chats &lt;- toks_chats %&gt;% \n    quanteda::dfm()\n\n\n\n📋 Exercise 2: Semantisches Netzwerk\n\nCreate a semantic network based on the top 50 tokens from dfm_chats.\n\nBased on dfm_chats, create an object called top_50_tokens by using the topfeatures() & names() function from the quanteda package to get the top 50 tokens.\nBased on dfm_chats, create a feature co-occurrence matrix (FCM) using the fcm() function from the quanteda package.\nSelect the top 50 tokens from the FCM using the fcm_select() function.\nCreate a network plot using the textplot_network() function from the quanteda package.\n\n\n\ntop50_tokens &lt;- dfm_chats %&gt;% \n    topfeatures(n = 50) %&gt;% \n    names()\n\ndfm_chats %&gt;% \n    fcm() %&gt;% \n    fcm_select(pattern = top50_tokens) %&gt;% \n    textplot_network()\n\n\n\n📋 Exercise 3: Analyse auf Basis von POS-Tagging\n\nBased on chats_spacyr, analyse the adjectives associated with Trump.\n\nFilter the dataset by using filter() and the arguments pos == \"NOUN\" and lemma == \"trump\".\nJoin the dataset with itself by using inner_join() and the arguments doc_id, sentence_id, and relationship = \"many-to-many\".\nFilter the dataset again for adjectives with the head token id equal to the token id of the noun. To do that, use filter() and the arguments pos.y == \"ADJ\" and head_token_id.y == token_id.x.\nRename the columns and select the relevant columns.\nDisplay the results using the sjmisc::frq() function.\n\n\n\nchats_spacyr %&gt;% \n    filter(\n      pos == \"NOUN\" &\n      lemma == \"trump\") %&gt;%\n    inner_join(\n      chats_spacyr,\n      by = c(\n        \"doc_id\",\n        \"sentence_id\"),\n      relationship = \n        \"many-to-many\") %&gt;% \n    filter(\n      pos.y == \"ADJ\" &\n      head_token_id.y == token_id.x) %&gt;% \n    rename(\n      token_id = token_id.y,\n      token = token.y) %&gt;% \n    select(\n      doc_id, sentence_id,\n      token_id, token) %&gt;%\n    sjmisc::frq(token, sort.frq = \"desc\") \n\n\n\n📋 Exercise 6: Named Entity Recognition (NER)\n\nAnalyse the named entities in the chat data.\n\nBased on chats_spacyr, use the frq() function from the sjmisc package to get the frequency of named entities.\nAgain based on chats_spacyr, filter the dataset for named entities of that indicate a person is mentioned (by using filter and the varialbe entity). Use the output of the previous step to identify the correct entity. Additionally, base all further analysis only on nouns, by using filter and the variable pos== \"NOUN.\nUse the frq() function from the sjmisc package to get the frequency. To avoid display errors, use the min.frq = 10 argument to only display tokens with a frequency of at least 10.\n\n\n\n# Identify named entities\nchats_spacyr %&gt;% \n    sjmisc::frq(entity, sort.frq = \"desc\")\n\n# Analyse named entities\nchats_spacyr %&gt;% \n    filter(entity == \"PERSON_B\") %&gt;%\n    filter(pos == \"NOUN\") %&gt;% \n    sjmisc::frq(token, sort.frq = \"desc\", min.frq = 10)"
  },
  {
    "objectID": "exercises/exercise-08.html",
    "href": "exercises/exercise-08.html",
    "title": "Advanced Twitch Chat Analysis",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "exercises/exercise-08.html#background",
    "href": "exercises/exercise-08.html#background",
    "title": "Advanced Twitch Chat Analysis",
    "section": "Background",
    "text": "Background"
  },
  {
    "objectID": "exercises/exercise-08.html#todayss-data-basis-advanced-text-analysis",
    "href": "exercises/exercise-08.html#todayss-data-basis-advanced-text-analysis",
    "title": "Advanced Twitch Chat Analysis",
    "section": "Todays’s data basis: Advanced Text Analysis",
    "text": "Todays’s data basis: Advanced Text Analysis\nTranscripts & Chats of the Live-Streams from  hasanabi and  zackrawrr and | TheMajorityReport for the Presidential (Harris vs. Trump) and Vice-Presidential (Vance vs. Walz) Debates 2024"
  },
  {
    "objectID": "exercises/exercise-08.html#preparation",
    "href": "exercises/exercise-08.html#preparation",
    "title": "Advanced Twitch Chat Analysis",
    "section": "Preparation",
    "text": "Preparation\n\nPackages\nThe pacman::p_load() function from the pacman package is used to load the packages, which has several advantages over the conventional method with library():\n\nConcise syntax\nAutomatic installation (if the package is not already installed)\nLoading multiple packages at once\nAutomatic search for dependencies\n\n\npacman::p_load(\n    here, taylor,\n    magrittr, janitor,\n    ggpubr, \n    gt, gtExtras,\n    countdown, \n    quanteda, # quanteda text processing\n    quanteda.textplots, \n    easystats, tidyverse\n)\n\n\n\nImport und Vorverarbeitung der Daten\n\nchats &lt;- qs::qread(here(\"local_data/chats.qs\"))\ntranscripts &lt;- qs::qread(here(\"local_data/transcripts.qs\"))\nchats_spacyr &lt;- qs::qread(here(\"local_data/chat-corpus_spacyr.qs\"))"
  },
  {
    "objectID": "exercises/exercise-08.html#praktische-übung",
    "href": "exercises/exercise-08.html#praktische-übung",
    "title": "Advanced Twitch Chat Analysis",
    "section": "🛠️ Praktische Übung",
    "text": "🛠️ Praktische Übung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation ausgeführt haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf  des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in das Tutorial (.qmd oder .html). Beim Tutorial handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Tutorial also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden."
  },
  {
    "objectID": "exercises/exercise-08.html#kennenlernen-des-chat-datensatzes",
    "href": "exercises/exercise-08.html#kennenlernen-des-chat-datensatzes",
    "title": "Advanced Twitch Chat Analysis",
    "section": "🔎 Kennenlernen des Chat-Datensatzes",
    "text": "🔎 Kennenlernen des Chat-Datensatzes\n\n📋 Exercise 1: Create corpus, token & DFM\n\nCreate new dataset corp_chats\n\nBased on the dataset chats, create a corpus object with the quanteda package.\nUse the corpus() function with the docid_field argument set to “message_id” and the text_field argument set to “message_content”.\n\nCreate new dataset toks_chats\n\nBased on the dataset corp_chats, create tokens using the tokens() function from the quanteda package, including the removal of punctuation, symbols, numbers, URLs, and stopwords.\nUse the tokens_remove() function to remove stopwords (en).\n\nCreate new dataset dfm_chats\n\nConvert the tokens to a document-feature matrix (DFM) using the dfm() function from the quanteda package.\n\n\n\n# INSERT CODE HERE\n\n\n\n📋 Exercise 2: Semantisches Netzwerk\n\nCreate a semantic network based on the top 50 tokens from dfm_chats.\n\nBased on dfm_chats, create an object called top_50_tokens by using the topfeatures() & names() function from the quanteda package to get the top 50 tokens.\nBased on dfm_chats, create a feature co-occurrence matrix (FCM) using the fcm() function from the quanteda package.\nSelect the top 50 tokens from the FCM using the fcm_select() function.\nCreate a network plot using the textplot_network() function from the quanteda package.\n\n\n\n# INSERT CODE HERE\n\n\n\n📋 Exercise 3: Analyse auf Basis von POS-Tagging\n\nBased on chats_spacyr, analyse the adjectives associated with Trump.\n\nFilter the dataset by using filter() and the arguments pos == \"NOUN\" and lemma == \"trump\".\nJoin the dataset with itself by using inner_join() and the arguments doc_id, sentence_id, and relationship = \"many-to-many\".\nFilter the dataset again for adjectives with the head token id equal to the token id of the noun. To do that, use filter() and the arguments pos.y == \"ADJ\" and head_token_id.y == token_id.x.\nRename the columns and select the relevant columns.\nDisplay the results using the sjmisc::frq() function.\n\n\n\n# INSERT CODE HERE\n\n\n\n📋 Exercise 6: Named Entity Recognition (NER)\n\nAnalyse the named entities in the chat data.\n\nBased on chats_spacyr, use the frq() function from the sjmisc package to get the frequency of named entities.\nAgain based on chats_spacyr, filter the dataset for named entities of that indicate a person is mentioned (by using filter and the varialbe entity). Use the output of the previous step to identify the correct entity. Additionally, base all further analysis only on nouns, by using filter and the variable pos== \"NOUN.\nUse the frq() function from the sjmisc package to get the frequency. To avoid display errors, use the min.frq = 10 argument to only display tokens with a frequency of at least 10.\n\n\n\n# INSERT CODE HERE"
  },
  {
    "objectID": "exercises/exercise-03_solution.html",
    "href": "exercises/exercise-03_solution.html",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "",
    "text": "Link to slides\n Download source file"
  },
  {
    "objectID": "exercises/exercise-03_solution.html#background",
    "href": "exercises/exercise-03_solution.html#background",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: Hollywood Age Gaps\n\n\n\n\nAn informational site showing the age gap between movie love interests.\n\nThe data follows certain rules:\n\nThe two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)\nThe youngest of the two actors is at least 17 years old\nNot animated characters\n\n\n\n\nThe best way to learn R is by trying. This document tries to display a version of the “normal” data processing procedure.\nUse tidytuesday data as an example to showcase the potential"
  },
  {
    "objectID": "exercises/exercise-03_solution.html#preparation",
    "href": "exercises/exercise-03_solution.html#preparation",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "Preparation",
    "text": "Preparation\n\nPackages\nThe pacman::p_load() package is used to load the packages, which has several advantages over the conventional method with library():\n\nConcise syntax\nAutomatic installation (if the package is not already installed)\nLoading multiple packages at once\nAutomatic search for dependencies\n\n\npacman::p_load(\n  here, \n  magrittr, \n  tidyverse,\n  janitor,\n  easystats,\n  sjmisc,\n  ggpubr)\n\n\n\nImport und Vorverarbeitung der Daten\n\nVariablennamen und -beschreibungen\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nmovie_name\nName of the film\n\n\nrelease_year\nRelease year\n\n\ndirector\nDirector of the film\n\n\nage_difference\nAge difference between the characters in whole years\n\n\ncouple_number\nAn identifier for the couple in case multiple couples are listed for this film\n\n\nactor_1_name\nThe name of the older actor in this couple\n\n\nactor_2_name\nThe name of the younger actor in this couple\n\n\nactor_1_birthdate\nThe birthdate of the older member of the couple\n\n\nactor_2_birthdate\nThe birthdate of the younger member of the couple\n\n\nactor_1_age\nThe age of the older actor when the film was released\n\n\nactor_2_age\nThe age of the younger actor when the film was released\n\n\n\n\n# Import data from URL\nage_gaps &lt;- read_csv(\"http://hollywoodagegap.com/movies.csv\") %&gt;% \n  janitor::clean_names()\n\n\n# Correct data\nage_gaps_correct &lt;- age_gaps %&gt;% \n  mutate(\n    across(ends_with(\"_birthdate\"), ~as.Date(.)) # set dates to dates\n  )"
  },
  {
    "objectID": "exercises/exercise-03_solution.html#praktische-übung",
    "href": "exercises/exercise-03_solution.html#praktische-übung",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "🛠️ Praktische Übung",
    "text": "🛠️ Praktische Übung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation ausgeführt haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf  des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in das Tutorial (.qmd oder .html). Beim Tutorial handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Tutorial also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\n🔎 Welche Rolle spielt das Geschlecht?\n\n\n\n\n\n\nSpielt das Geschlecht eine Rolle?\n\n\n\n\nDer folgende Abschitt befasst sich nun ergänzend mit der Frage, welche Rolle das Geschlecht mit Blick auf die “Gültigkeit” der vorherigen Ergebnisse spielt\nDazu sind jedoch weitere Explorations- und Überarbeitungsschritte notwendig\n\n\n\n\n\n📋 Exercise 1: Übeprüfung der _gender-Variablen\n\n\n\n\n\n\nArbeitsauftrag 1.1\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_correct die Variablen actor_1_gender und actor_2_gender an.\n\n\n\nage_gaps_correct %&gt;% \n  sjmisc::frq(actor_1_gender, actor_2_gender)\n\nactor_1_gender &lt;character&gt; \n# total N=1199 valid N=1199 mean=1.01 sd=0.11\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\nman   | 1184 | 98.75 |   98.75 |  98.75\nwoman |   15 |  1.25 |    1.25 | 100.00\n&lt;NA&gt;  |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nactor_2_gender &lt;character&gt; \n# total N=1199 valid N=1199 mean=1.99 sd=0.11\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\nman   |   16 |  1.33 |    1.33 |   1.33\nwoman | 1183 | 98.67 |   98.67 | 100.00\n&lt;NA&gt;  |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n\n\n\n\n\nArbeitsauftrag 1.2\n\n\n\nNutzen Sie die Funktion sjmisc::flat_talbe() und den Datensatz age_gaps_correct um eine Kreuztabelle der Variablen actor_1_gender und actor_2_gender zu erstellen.\n\n\n\nage_gaps_correct %&gt;% \n  select(actor_1_gender, actor_2_gender) %&gt;% \n  sjmisc::flat_table()\n\n               actor_2_gender  man woman\nactor_1_gender                          \nman                             12  1172\nwoman                            4    11\n\n\n\n\n🔎 Sind die Daten “konsistent”?\n\nÜberprüfung der Sortierung\n\nage_gaps_correct %&gt;% \n  summarise(\n      p1_older = mean(actor_1_age &gt; actor_2_age), # older person first?\n      p1_male  = mean(actor_1_gender == \"man\"),  # male person first? \n      p_1_first_alpha = mean(actor_1_name &lt; actor_2_name) # alphabetical order?\n  )\n\n# A tibble: 1 × 3\n  p1_older p1_male p_1_first_alpha\n     &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;\n1    0.813   0.987           0.495\n\n\n\n\n\nÜberprüfung der Anzahl pro Paare pro Film\n\n# Create data\ncouples &lt;- age_gaps_correct %&gt;% \n  group_by(movie_name) %&gt;% \n  summarise(n = n()) \n\n# Distribution\ncouples %&gt;% frq(n)\n\nn &lt;integer&gt; \n# total N=864 valid N=864 mean=1.39 sd=0.75\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    1 | 629 | 72.80 |   72.80 |  72.80\n    2 | 162 | 18.75 |   18.75 |  91.55\n    3 |  54 |  6.25 |    6.25 |  97.80\n    4 |  14 |  1.62 |    1.62 |  99.42\n    5 |   3 |  0.35 |    0.35 |  99.77\n    6 |   1 |  0.12 |    0.12 |  99.88\n    7 |   1 |  0.12 |    0.12 | 100.00\n &lt;NA&gt; |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n# Movies with a loot of couples \ncouples %&gt;% \n  filter(n &gt; 3) %&gt;% \n  arrange(desc(n))\n\n# A tibble: 19 × 2\n   movie_name                      n\n   &lt;chr&gt;                       &lt;int&gt;\n 1 Love Actually                   7\n 2 The Family Stone                6\n 3 A View to a Kill                5\n 4 He's Just Not That Into You     5\n 5 Mona Lisa Smile                 5\n 6 A Star Is Born                  4\n 7 American Pie                    4\n 8 Boogie Nights                   4\n 9 Book Club                       4\n10 Closer                          4\n11 Pushing Tin                     4\n12 Sex and the City                4\n13 Soul Food                       4\n14 Tag                             4\n15 The Favourite                   4\n16 The Girl on the Train           4\n17 The Other Woman                 4\n18 Tomorrow Never Dies             4\n19 Twilight                        4\n\n\n\n\nKorrekturen\n\nage_gaps_consistent &lt;- age_gaps_correct %&gt;% \n  # If multiple couples, assign couple number by movie\n  mutate(\n      couple_number = row_number(),\n      .by = \"movie_name\"\n  ) %&gt;% \n  # Change data structure (one line per actor in a coulpe of a movie)\n  pivot_longer(\n    cols = starts_with(c(\"actor_1_\", \"actor_2_\")),\n    names_to = c(NA, NA, \".value\"),\n    names_sep = \"_\"\n  ) %&gt;% \n  # Put older actor first\n  arrange(desc(age_difference), movie_name, birthdate) %&gt;% \n    mutate(\n    position = row_number(),\n    .by = c(\"movie_name\", \"couple_number\")\n  ) %&gt;% \n  pivot_wider(\n    names_from = \"position\",\n    names_glue = \"actor_{position}_{.value}\",\n    values_from = c(\"name\", \"gender\", \"birthdate\", \"age\")\n  ) %&gt;% \n  mutate(\n    couple_structure = case_when(\n      actor_1_gender == \"woman\" & actor_2_gender == \"woman\" ~ 1,\n      actor_1_gender == \"man\" & actor_2_gender == \"man\" ~ 2,\n      actor_1_gender != \"man\" ~ 3, \n      actor_1_gender == \"man\" ~ 4,\n    ),\n    older_male_hetero  = sjmisc::rec(\n      couple_structure, \n      rec=\"3=0; 4=1; ELSE=NA\", \n      to.factor = TRUE\n    )\n  )\n\n\n\n🔎 Die zweite Datenexploration\n\n\n📋 Exercise 2: Alterskombinationen im Überblick\n\n\n\n\n\n\nArbeitauftrag 2\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_consistent die Variablen couple_structure und older_male_hetero an.\n\n\n\nage_gaps_consistent %&gt;% \n  frq(couple_structure, older_male_hetero)\n\ncouple_structure &lt;numeric&gt; \n# total N=1199 valid N=1199 mean=3.78 sd=0.50\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    1 |  11 |  0.92 |    0.92 |   0.92\n    2 |  12 |  1.00 |    1.00 |   1.92\n    3 | 210 | 17.51 |   17.51 |  19.43\n    4 | 966 | 80.57 |   80.57 | 100.00\n &lt;NA&gt; |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nolder_male_hetero &lt;categorical&gt; \n# total N=1199 valid N=1176 mean=0.82 sd=0.38\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    0 | 210 | 17.51 |   17.86 |  17.86\n    1 | 966 | 80.57 |   82.14 | 100.00\n &lt;NA&gt; |  23 |  1.92 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n\n📋 Exercise 3: Wie sind die Altersunterschiede unterteilt, unter Berücksichtiung des Geschlechts?\n\n\n\n\n\n\nArbeitsauftrag 3.1 (graphische Überprüfung)\n\n\n\n\nErstellen Sie, auf Basis des Datensatzes age_gaps_consistent, einen ggplot.\nNutzen Sie im Argument aes() die Variable age_difference als x-Variable und older_male_hetero für das Argument fill.\nNutzen Sie geom_bar zur Erzeugung des Plots.\nOptional: Verwenden Sie theme_pubr\n\n\n\n\n# Simple \nage_gaps_consistent %&gt;% \n  ggplot(aes(age_difference, fill = older_male_hetero)) +\n  geom_bar() +\n  theme_pubr()\n\n\n\n\n\n\n\n# Detailed\nage_gaps_consistent %&gt;% \n  ggplot(aes(age_difference, fill = older_male_hetero)) +\n  geom_bar() +\n  labs(\n    x = \"Altersdifferenz (in Jahren)\",\n    y = 'Anzahl der \"Beziehungen\"'\n  ) +\n   scale_fill_manual(\n    name = \"Older partner in couple\",\n    values = c(\"0\" = \"#F8766D\", \"1\" = \"#00BFC4\", \"NA\" = \"grey\"),\n    labels = c(\"0\" = \"Woman\", \"1\" = \"Man\", \"NA\" = \"Same sex couples\")\n  ) +\n  theme_pubr() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArbeitsauftrag 3.2 (Überprüfung durch Modellierung)\n\n\n\n\nErstellen Sie ein lineares Modell (lm), das die Variable age_difference als abhängige Variable und die Variablen release_year und older_male_hetero als unabhängige Variablen verwendet. Nutzen Sie dazu den Datensatz age_gaps_consistent.\nGeben Sie die Parameter des Modells mit der Funktion parameters::parameters() aus.\nBewerten Sie die Modellleistung mit der Funktion performance::model_performance().\nErstellen Sie einen Bericht über das Modell mit der Funktion report::report().\n\n\n\n\nmdl &lt;- lm(age_difference ~ release_year + older_male_hetero, data = age_gaps_consistent)\n\n# Output\nmdl %&gt;% parameters::parameters()\n\nParameter             | Coefficient |    SE |           95% CI | t(1173) |      p\n---------------------------------------------------------------------------------\n(Intercept)           |      202.27 | 27.53 | [148.25, 256.29] |    7.35 | &lt; .001\nrelease year          |       -0.10 |  0.01 | [ -0.13,  -0.07] |   -7.16 | &lt; .001\nolder male hetero [1] |        6.11 |  0.61 | [  4.90,   7.32] |    9.94 | &lt; .001\n\nmdl %&gt;% performance::model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n8236.693 | 8236.727 | 8256.972 | 0.127 |     0.125 | 8.002 | 8.012\n\nmdl %&gt;% report::report()\n\nWe fitted a linear model (estimated using OLS) to predict age_difference with\nrelease_year and older_male_hetero (formula: age_difference ~ release_year +\nolder_male_hetero). The model explains a statistically significant and weak\nproportion of variance (R2 = 0.13, F(2, 1173) = 85.10, p &lt; .001, adj. R2 =\n0.13). The model's intercept, corresponding to release_year = 0 and\nolder_male_hetero = 0, is at 202.27 (95% CI [148.25, 256.29], t(1173) = 7.35, p\n&lt; .001). Within this model:\n\n  - The effect of release year is statistically significant and negative (beta =\n-0.10, 95% CI [-0.13, -0.07], t(1173) = -7.16, p &lt; .001; Std. beta = -0.20, 95%\nCI [-0.25, -0.14])\n  - The effect of older male hetero [1] is statistically significant and positive\n(beta = 6.11, 95% CI [4.90, 7.32], t(1173) = 9.94, p &lt; .001; Std. beta = 0.71,\n95% CI [0.57, 0.85])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "sessions/session-09.html",
    "href": "sessions/session-09.html",
    "title": "Session 9",
    "section": "",
    "text": "🖥️ Session 09"
  },
  {
    "objectID": "sessions/session-09.html#participate",
    "href": "sessions/session-09.html#participate",
    "title": "Session 9",
    "section": "",
    "text": "🖥️ Session 09"
  },
  {
    "objectID": "sessions/session-09.html#suggested-readings",
    "href": "sessions/session-09.html#suggested-readings",
    "title": "Session 9",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nChen, Y., Peng, Z., Kim, S.-H., & Choi, C. W. (2023). What We Can Do and Cannot Do with Topic Modeling: A Systematic Review. Communication Methods and Measures, 17(2), 111–130. https://doi.org/10.1080/19312458.2023.2167965\nHase, V. (2023). Automated Content Analysis (F. Oehmer-Pedrazzi, S. H. Kessler, E. Humprecht, K. Sommer, & L. Castro, Eds.; pp. 23–36). Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-36179-2_3\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., Häussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93–118. https://doi.org/10.1080/19312458.2018.1430754"
  },
  {
    "objectID": "sessions/session-09.html#additional-readings",
    "href": "sessions/session-09.html#additional-readings",
    "title": "Session 9",
    "section": "Additional readings",
    "text": "Additional readings\n\nBernhard, J., Teuffenbach, M., & Boomgaarden, H. G. (2023). Topic Model Validation Methods and their Impact on Model Selection and Evaluation. Computational Communication Research, 5(1), 1. https://doi.org/10.5117/CCR2023.1.13.BERN\nChurchill, R., & Singh, L. (2022). The Evolution of Topic Modeling. ACM Computing Surveys, 54(10s), 1–35. https://doi.org/10.1145/3507900\nDenny, M. J., & Spirling, A. (2018). Text Preprocessing For Unsupervised Learning: Why It Matters, When It Misleads, And What To Do About It. Political Analysis, 26(2), 168–189. https://doi.org/10/gdjsqk\nMaier, D., Niekler, A., Wiedemann, G., & Stoltenberg, D. (2020). How Document Sampling and Vocabulary Pruning Affect the Results of Topic Models. Computational Communication Research, 2(2), 139–152. https://doi.org/10.5117/CCR2020.2.001.MAIE\nQuinn, K. M., Monroe, B. L., Colaresi, M., Crespin, M. H., & Radev, D. R. (2010). How to Analyze Political Attention with Minimal Assumptions and Costs. American Journal of Political Science, 54(1), 209–228. https://doi.org/10.1111/j.1540-5907.2009.00427.x\nRoberts, M. E., Stewart, B. M., Tingley, D., Lucas, C., Leder-Luis, J., Gadarian, S. K., Albertson, B., & Rand, D. G. (2014). Structural Topic Models for Open-Ended Survey Responses. American Journal of Political Science, 58(4), 1064–1082. https://doi.org/10.1111/ajps.12103"
  },
  {
    "objectID": "sessions/session-09.html#useful-packages",
    "href": "sessions/session-09.html#useful-packages",
    "title": "Session 9",
    "section": "Useful packages",
    "text": "Useful packages\n\nquanteda 🌐 | \nBenoit, K., Watanabe, K., Wang, H., Nulty, P., Obeng, A., Müller, S., & Matsuo, A. (2018). Quanteda: An r package for the quantitative analysis of textual data. Journal of Open Source Software, 3(30), 774. https://doi.org/10.21105/joss.00774\nstm 🌐 |  für Structural Topic Modeling\nRoberts, M. E., Stewart, B. M., & Tingley, D. (2019). stm: An R Package for Structural Topic Models. Journal of Statistical Software, 91(1), 1–40. https://doi.org/10.18637/jss.v091.i02\nkeyATM 🌐 |  für Keyword Assisted Topic Modeling\nEsser, F. (2019). Comparative international studies of election campaign communication: What should happen next? Journalism, 20(8), 1124–1138. https://doi.org/10.1177/1464884919845450\ntopicmodels 🌐 für LDA basiertes Verfahren\nGrün, B., & Hornik, K. (n.d.). topicmodels: Topic Models. https://doi.org/10.32614/CRAN.package.topicmodels\ntidytext 🌐 |  für Extraktion z.B. der Theta- oder Phi-Matrix\nSilge, J., & Robinson, D. (2016). Tidytext: Text mining and analysis using tidy data principles in r. The Journal of Open Source Software, 1(3), 37. https://doi.org/10.21105/joss.00037\nLDAvis zur Visualisierung\nSievert, C., & Shirley, K. (n.d.). LDAvis: Interactive Visualization of Topic Models. https://doi.org/10.32614/CRAN.package.LDAvis\nstminsights 🌐 |  zur Visualisierung\nSchwemmer, C. (2021). Stminsights: A shiny application for inspecting structural topic models. https://github.com/cschwem2er/stminsights\noolong 🌐 |  für Validierungen\nChan, C., & Sältzer, M. (2020). Oolong: An r package for validating automated content analysis tools. Journal of Open Source Software, 5(55), 2461. https://doi.org/10.21105/joss.02461\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/session-06.html",
    "href": "sessions/session-06.html",
    "title": "Session 6",
    "section": "",
    "text": "The following sections list the mandatory articles for the each presenting group. Additional optional articles are provided under Further reading.\n\n\n\n\n\n\nTrilling, D. (2015). Two Different Debates? Investigating the Relationship Between a Political Debate on TV and Simultaneous Comments on Twitter. Social Science Computer Review, 33(3), 259–276. https://doi.org/10.1177/0894439314537886\nRobertson, C. T., Dutton, W. H., Ackland, R., & Peng, T.-Q. (2019). The democratic role of social media in political debates: The use of Twitter in the first televised US presidential debate of 2016. Journal of Information Technology & Politics, 16(2), 105–118. https://doi.org/10.1080/19331681.2019.1590283\nJennings, F. J., Warner, B. R., McKinney, M. S., Kearney, C. C., Funk, M. E., & Bramlett, J. C. (2020). Learning from Presidential Debates: Who Learns the Most and Why? Communication Studies, 71(5), 896–910. https://doi.org/10.1080/10510974.2020.1807377\n\n\n\n\nCoddington, M., Molyneux, L., & Lawrence, R. G. (2014). Fact Checking the Campaign: How Political Reporters Use Twitter to Set the Record Straight (or Not). The International Journal of Press/Politics, 19(4), 391–409. https://doi.org/10.1177/1940161214540942\nGoovaerts, I., & Turkenburg, E. (2023). How Contextual Features Shape Incivility Over Time: An Analysis of the Evolution and Determinants of Political Incivility in Televised Election Debates (19852019). Communication Research, 50(4), 480–507. https://doi.org/10.1177/00936502221135694\n\n\n\n\n\n\nAsbury-Kimmel, V., Chang, K.-C., McCabe, K. T., Munger, K., & Ventura, T. (2021). The effect of streaming chat on perceptions of political debates. Journal of Communication, 71(6), 947–974. https://doi.org/10.1093/joc/jqab041\nRuiz-Bravo, N., Selander, L., & Roshan, M. (2022). The Political Turn of Twitch  Understanding Live Chat as an Emergent Political Space. http://hdl.handle.net/10125/79723\nRiddick, S., & Shivener, R. (2022). Affective spamming on twitch: Rhetorics of an emote-only audience in a presidential inauguration livestream. Computers and Composition, 64. https://doi.org/10.1016/j.compcom.2022.102711\nEaton, J. (2024). From the comments section: Analyzing online public discourse on the first 2020 presidential debate. Research & Politics, 11(3), 20531680241271758. https://doi.org/10.1177/20531680241271758\n\n\n\n\nPowell, A., & Williams-Johnson, D. (2023). “You dumb cracker b*tch”: The legitimizing of White supremacy during a Twitch ban of HasanAbi. New Media & Society, 14614448231191776. https://doi.org/10.1177/14614448231191776\n\n\n\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/session-06.html#literature",
    "href": "sessions/session-06.html#literature",
    "title": "Session 6",
    "section": "",
    "text": "The following sections list the mandatory articles for the each presenting group. Additional optional articles are provided under Further reading.\n\n\n\n\n\n\nTrilling, D. (2015). Two Different Debates? Investigating the Relationship Between a Political Debate on TV and Simultaneous Comments on Twitter. Social Science Computer Review, 33(3), 259–276. https://doi.org/10.1177/0894439314537886\nRobertson, C. T., Dutton, W. H., Ackland, R., & Peng, T.-Q. (2019). The democratic role of social media in political debates: The use of Twitter in the first televised US presidential debate of 2016. Journal of Information Technology & Politics, 16(2), 105–118. https://doi.org/10.1080/19331681.2019.1590283\nJennings, F. J., Warner, B. R., McKinney, M. S., Kearney, C. C., Funk, M. E., & Bramlett, J. C. (2020). Learning from Presidential Debates: Who Learns the Most and Why? Communication Studies, 71(5), 896–910. https://doi.org/10.1080/10510974.2020.1807377\n\n\n\n\nCoddington, M., Molyneux, L., & Lawrence, R. G. (2014). Fact Checking the Campaign: How Political Reporters Use Twitter to Set the Record Straight (or Not). The International Journal of Press/Politics, 19(4), 391–409. https://doi.org/10.1177/1940161214540942\nGoovaerts, I., & Turkenburg, E. (2023). How Contextual Features Shape Incivility Over Time: An Analysis of the Evolution and Determinants of Political Incivility in Televised Election Debates (19852019). Communication Research, 50(4), 480–507. https://doi.org/10.1177/00936502221135694\n\n\n\n\n\n\nAsbury-Kimmel, V., Chang, K.-C., McCabe, K. T., Munger, K., & Ventura, T. (2021). The effect of streaming chat on perceptions of political debates. Journal of Communication, 71(6), 947–974. https://doi.org/10.1093/joc/jqab041\nRuiz-Bravo, N., Selander, L., & Roshan, M. (2022). The Political Turn of Twitch  Understanding Live Chat as an Emergent Political Space. http://hdl.handle.net/10125/79723\nRiddick, S., & Shivener, R. (2022). Affective spamming on twitch: Rhetorics of an emote-only audience in a presidential inauguration livestream. Computers and Composition, 64. https://doi.org/10.1016/j.compcom.2022.102711\nEaton, J. (2024). From the comments section: Analyzing online public discourse on the first 2020 presidential debate. Research & Politics, 11(3), 20531680241271758. https://doi.org/10.1177/20531680241271758\n\n\n\n\nPowell, A., & Williams-Johnson, D. (2023). “You dumb cracker b*tch”: The legitimizing of White supremacy during a Twitch ban of HasanAbi. New Media & Society, 14614448231191776. https://doi.org/10.1177/14614448231191776\n\n\n\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/session-04.html",
    "href": "sessions/session-04.html",
    "title": "Session 4",
    "section": "",
    "text": "The following sections list the mandatory articles for the each presenting group. Additional optional articles are provided under Further reading.\n\n\n\n\n\n\nHilvert-Bruce, Z., Neill, J. T., Sjöblom, M., & Hamari, J. (2018). Social motivations of live-streaming viewer engagement on twitch. Computers in Human Behavior, 84, 58–67. https://doi.org/10.1016/j.chb.2018.02.013\nCastro-Agirre, I., & Martínez-Fernández, G. (2024). From gamer niche to mainstream media: Twitch’s most popular media figures and content. Communication & Society, 179–196. https://doi.org/10.15581/003.37.2.179-196\nNguyen, T. T., & Veer, E. (2024). Why people watch user-generated videos? A systematic review and meta-analysis. International Journal of Human-Computer Studies, 181, 103144. https://doi.org/10.1016/j.ijhcs.2023.103144\n\n\n\n\nGros, D., Hackenholt, A., Zawadzki, P., & Wanner, B. (2018). Interactions of Twitch Users and Their Usage Behavior (G. Meiselwitz, Ed.; pp. 201–213). Springer International Publishing. https://doi.org/10.1007/978-3-319-91485-5_15\nZimmer, F., Scheibe, K., & Stock, W. G. (2018). A Model for Information Behavior Research on Social Live Streaming Services (SLSSs) (G. Meiselwitz, Ed.; pp. 429–448). Springer International Publishing. https://doi.org/10.1007/978-3-319-91485-5_33\nXu, X.-Y., Niu, W.-B., Jia, Q.-D., Nthoiwa, L., & Li, L.-W. (2021). Why Do Viewers Engage in Video Game Streaming? The Perspective of Cognitive Emotion Theory and the Moderation Effect of Personal Characteristics. Sustainability, 13(21), 11990. https://doi.org/10.3390/su132111990\nKneisel, A., & Sternadori, M. (2023). Effects of parasocial affinity and gender on live streaming fans’ motivations. Convergence, 29(2), 322–341. https://doi.org/10.1177/13548565221114461\n\n\n\n\n\n\nJacobs, N., & Booth, P. (2021). Converging experiences, converging audiences: An analysis of doctor who on Twitch. Convergence: The International Journal of Research into New Media Technologies, 27(5), 1324–1342. https://doi.org/10.1177/1354856520976447\nDutt, S., & Graham, S. (2023). Video, talk and text: How do parties communicate coherently across modalities in live videostreams? Discourse, Context and Media, 55. https://doi.org/10.1016/j.dcm.2023.100726\nNavarro, A., & Tapiador, F. J. (2023). Twitch as a privileged locus to analyze young people’s attitudes in the climate change debate: a quantitative analysis. Humanities and Social Sciences Communications, 10(1), 1–13. https://doi.org/10.1057/s41599-023-02377-4\n\n\n\n\nYoung, A., & Wiedenfeld, G. (2022). A Motivation Analysis of Video Game Microstreamers: “Finding My People and Myself” on YouTube and Twitch. Journal of Broadcasting & Electronic Media, 66(2), 381–399. https://doi.org/10.1080/08838151.2022.2086549\nLessel, P., Altmeyer, M., Sahner, J., & Krüger, A. (2022). Streamer’s hell - investigating audience influence in live-streams beyond the game. Proc. ACM Hum.-Comput. Interact., 6(CHI PLAY), 252:1252:27. https://doi.org/10.1145/3549515\nMao, E. (2022). How live stream content types impact viewers’ support behaviors? Mediational analysis on psychological and social gratifications. Frontiers in Psychology, 13. https://doi.org/10.3389/fpsyg.2022.951055\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/session-04.html#literature",
    "href": "sessions/session-04.html#literature",
    "title": "Session 4",
    "section": "",
    "text": "The following sections list the mandatory articles for the each presenting group. Additional optional articles are provided under Further reading.\n\n\n\n\n\n\nHilvert-Bruce, Z., Neill, J. T., Sjöblom, M., & Hamari, J. (2018). Social motivations of live-streaming viewer engagement on twitch. Computers in Human Behavior, 84, 58–67. https://doi.org/10.1016/j.chb.2018.02.013\nCastro-Agirre, I., & Martínez-Fernández, G. (2024). From gamer niche to mainstream media: Twitch’s most popular media figures and content. Communication & Society, 179–196. https://doi.org/10.15581/003.37.2.179-196\nNguyen, T. T., & Veer, E. (2024). Why people watch user-generated videos? A systematic review and meta-analysis. International Journal of Human-Computer Studies, 181, 103144. https://doi.org/10.1016/j.ijhcs.2023.103144\n\n\n\n\nGros, D., Hackenholt, A., Zawadzki, P., & Wanner, B. (2018). Interactions of Twitch Users and Their Usage Behavior (G. Meiselwitz, Ed.; pp. 201–213). Springer International Publishing. https://doi.org/10.1007/978-3-319-91485-5_15\nZimmer, F., Scheibe, K., & Stock, W. G. (2018). A Model for Information Behavior Research on Social Live Streaming Services (SLSSs) (G. Meiselwitz, Ed.; pp. 429–448). Springer International Publishing. https://doi.org/10.1007/978-3-319-91485-5_33\nXu, X.-Y., Niu, W.-B., Jia, Q.-D., Nthoiwa, L., & Li, L.-W. (2021). Why Do Viewers Engage in Video Game Streaming? The Perspective of Cognitive Emotion Theory and the Moderation Effect of Personal Characteristics. Sustainability, 13(21), 11990. https://doi.org/10.3390/su132111990\nKneisel, A., & Sternadori, M. (2023). Effects of parasocial affinity and gender on live streaming fans’ motivations. Convergence, 29(2), 322–341. https://doi.org/10.1177/13548565221114461\n\n\n\n\n\n\nJacobs, N., & Booth, P. (2021). Converging experiences, converging audiences: An analysis of doctor who on Twitch. Convergence: The International Journal of Research into New Media Technologies, 27(5), 1324–1342. https://doi.org/10.1177/1354856520976447\nDutt, S., & Graham, S. (2023). Video, talk and text: How do parties communicate coherently across modalities in live videostreams? Discourse, Context and Media, 55. https://doi.org/10.1016/j.dcm.2023.100726\nNavarro, A., & Tapiador, F. J. (2023). Twitch as a privileged locus to analyze young people’s attitudes in the climate change debate: a quantitative analysis. Humanities and Social Sciences Communications, 10(1), 1–13. https://doi.org/10.1057/s41599-023-02377-4\n\n\n\n\nYoung, A., & Wiedenfeld, G. (2022). A Motivation Analysis of Video Game Microstreamers: “Finding My People and Myself” on YouTube and Twitch. Journal of Broadcasting & Electronic Media, 66(2), 381–399. https://doi.org/10.1080/08838151.2022.2086549\nLessel, P., Altmeyer, M., Sahner, J., & Krüger, A. (2022). Streamer’s hell - investigating audience influence in live-streams beyond the game. Proc. ACM Hum.-Comput. Interact., 6(CHI PLAY), 252:1252:27. https://doi.org/10.1145/3549515\nMao, E. (2022). How live stream content types impact viewers’ support behaviors? Mediational analysis on psychological and social gratifications. Frontiers in Psychology, 13. https://doi.org/10.3389/fpsyg.2022.951055\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/session-01.html",
    "href": "sessions/session-01.html",
    "title": "Session 1",
    "section": "",
    "text": "Important\n\n\n\nPlease make sure that you answered the survey send to you by (StudOn) mail."
  },
  {
    "objectID": "sessions/session-01.html#prepare",
    "href": "sessions/session-01.html#prepare",
    "title": "Session 1",
    "section": "Prepare",
    "text": "Prepare\n✍️ Fill out the short survey before the 22.10.2024 (see  StudOn for Link)\n📖 Read the syllabus"
  },
  {
    "objectID": "sessions/session-01.html#participate",
    "href": "sessions/session-01.html#participate",
    "title": "Session 1",
    "section": "Participate",
    "text": "Participate\n🖥️ Session 01\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/session-03.html",
    "href": "sessions/session-03.html",
    "title": "Session 3",
    "section": "",
    "text": "✍️ Keep working on R-Video-Tutorials.\n🔍 Check out the resources in the section “Working with R” (z.B. R Textbooks & Useful R sources)"
  },
  {
    "objectID": "sessions/session-03.html#prepare",
    "href": "sessions/session-03.html#prepare",
    "title": "Session 3",
    "section": "",
    "text": "✍️ Keep working on R-Video-Tutorials.\n🔍 Check out the resources in the section “Working with R” (z.B. R Textbooks & Useful R sources)"
  },
  {
    "objectID": "sessions/session-03.html#participate",
    "href": "sessions/session-03.html#participate",
    "title": "Session 3",
    "section": "Participate",
    "text": "Participate\n🖥️ Session 03"
  },
  {
    "objectID": "sessions/session-03.html#suggested-readings",
    "href": "sessions/session-03.html#suggested-readings",
    "title": "Session 3",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nBauer, P. C., & Landesvatter, C. (2023). Writing a reproducible paper with RStudio and quarto. https://osf.io/ur4xn\nJonge, E. de, & Loo, M. van der. (2013). An introduction to data cleaning with R.\nPearson, R. K. (2018). Exploratory data analysis using r. CRC Press/Taylor & Francis Group."
  },
  {
    "objectID": "sessions/session-03.html#useful-tools-resources",
    "href": "sessions/session-03.html#useful-tools-resources",
    "title": "Session 3",
    "section": "Useful tools & resources",
    "text": "Useful tools & resources\n\n📖 Useful and detailed tutorial to use git/github with RStudio\n📖 Happy Git and GitHub for the useR\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "slides/slides-08.html#seminarplan",
    "href": "slides/slides-08.html#seminarplan",
    "title": "🔨 Advanced Methods",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\nSession\nDatum\nTopic\nPresenter\n\n\n\n\n\n📂 Block 1\nIntroduction\n\n\n\n1\n23.10.2024\nKick-Off\nChristoph Adrian\n\n\n2\n30.10.2024\nDBD: Overview & Introduction\nChristoph Adrian\n\n\n3\n06.11.2024\n🔨 Introduction to working with R\nChristoph Adrian\n\n\n\n📂 Block 2\nTheoretical Background: Twitch & TV Election Debates\n\n\n\n4\n13.11.2024\n📚 Twitch-Nutzung im Fokus\nStudent groups\n\n\n5\n20.11.2024\n📚 (Wirkungs-)Effekte von Twitch & TV-Debatten\nStudent groups\n\n\n6\n27.11.2024\n📚 Politische Debatten & Social Media\nStudent groups\n\n\n\n📂 Block 3\nMethod: Natural Language Processing\n\n\n\n7\n04.12.2024\n🔨 Text as data I: Introduction\nChristoph Adrian\n\n\n8\n11.12.2024\n🔨 Text as data II: Advanced Methods\nChristoph Adrian\n\n\n9\n18.12.2024\n🔨 Advanced Method I: Topic Modeling\nChristoph Adrian\n\n\n\nNo lecture\n🎄Christmas Break\n\n\n\n10\n08.01.2025\n🔨 Advanced Method II: Machine Learning\nChristoph Adrian\n\n\n\n📂 Block 4\nProject Work\n\n\n\n11\n15.01.2025\n🔨 Project work\nStudent groups\n\n\n12\n22.01.2025\n🔨 Project work\nStudent groups\n\n\n13\n29.01.2025\n📊 Project Presentation I\nStudent groups (TBD)\n\n\n14\n05.02.2025\n📊 Project Presentation & 🏁 Evaluation\nStudentds (TBD) & Christoph Adrian"
  },
  {
    "objectID": "slides/slides-08.html#possibilities-over-possibilities",
    "href": "slides/slides-08.html#possibilities-over-possibilities",
    "title": "🔨 Advanced Methods",
    "section": "Possibilities over possibilities",
    "text": "Possibilities over possibilities\nÜberblick über verschiedene Methoden der Textanalyse (Grimmer & Stewart, 2013)"
  },
  {
    "objectID": "slides/slides-08.html#clusteranalyse-on",
    "href": "slides/slides-08.html#clusteranalyse-on",
    "title": "🔨 Advanced Methods",
    "section": "Clusteranalyse on 💉",
    "text": "Clusteranalyse on 💉\nGrundidee des Topic Modelings\n\n„computational content-analysis technique […] used to investigate the “hidden” thematic structure of […] texts” (Maier et al., 2018, p. S.93)\n\n\nVerfahren des unüberwachten maschinellen Lernens, das sich daher insbesondere zur Exploration und Deskription großer Textmengen eignet\nThemen werden strikt auf Basis von Worthäufigkeiten in den einzelnen Dokumenten vermeintlich objektiv berechnet, ganz ohne subjektive Einschätzungen und damit einhergehenden etwaigen Verzerrungen\nBekanntesten dieser Verfahren sind LDA (Latent Dirichlet Allocation) sowie die darauf aufbauenden CTM (Correlated Topic Models) und STM (Structural Topic Models)"
  },
  {
    "objectID": "slides/slides-08.html#vom-korpus-zum-themenmodell",
    "href": "slides/slides-08.html#vom-korpus-zum-themenmodell",
    "title": "🔨 Advanced Methods",
    "section": "Vom Korpus zum Themenmodell",
    "text": "Vom Korpus zum Themenmodell\nProzess des Topic Modelings nach Maier et al. (2018)"
  },
  {
    "objectID": "slides/slides-08.html#prominente-wörter-eines-themas",
    "href": "slides/slides-08.html#prominente-wörter-eines-themas",
    "title": "🔨 Advanced Methods",
    "section": "Prominente Wörter eines Themas",
    "text": "Prominente Wörter eines Themas\nVorstellung der Word-topic oder Phi-Matrix\n\n\n\n\nBedingte Wahrscheinlichkeit (beta), mit der Features in Themen prävalent sind\nWortlisten, die Themen beschreiben (“Top Features”)"
  },
  {
    "objectID": "slides/slides-08.html#zugehörige-dokumente-eines-themas",
    "href": "slides/slides-08.html#zugehörige-dokumente-eines-themas",
    "title": "🔨 Advanced Methods",
    "section": "Zugehörige Dokumente eines Themas",
    "text": "Zugehörige Dokumente eines Themas\nVorstellung der Document-topic oder Theta-Matrix\n\n\n\n\nBedingte Wahrscheinlichkeit (gamma), mit der Themen in Dokumenten prävalent sind\nDokumentenlisten, die Themen beschreiben (“Top Documents”)"
  },
  {
    "objectID": "slides/slides-08.html#themenmodelle-sind",
    "href": "slides/slides-08.html#themenmodelle-sind",
    "title": "🔨 Advanced Methods",
    "section": "Themenmodelle sind …",
    "text": "Themenmodelle sind …\nPromises & Pitfalls von Themenmodellen\n\nprobabilistisch ➜ Zuordnung von Wahrscheinlichkeiten, nicht eindeutigen Klassen\n\nModell sagt nicht eindeutig, welches das „eine“ Thema je Dokument ist oder wie ein Thema zu interpretieren ist ➜ es gibt nur (probabilistische) Hinweise.\n\ngenerative ➜ Prozess findet das statistische „passendste“ Modell, um unseren Korpus zu „generieren“\n\nModell läuft in iterativen Schlaufen immer und immer wieder durch, bis eine “optimale” Lösung gefunden wurde\nAber: Es gibt z.T. nicht-deterministische (d.h. je nach Einstellungen unterschiedliche) Lösungen.\n\n\n\nProababilistisches Modell: - Features haben eine Wahrscheinlichkeit von größer gleich 0 je Thema (ϕ-matrix) - Themen haben eine Wahrscheinlichkeit von größer gleich 0 je Dokument (θ-matrix) Generatvies Modell: - Gemeinsame Modellierung der beobachteten Variablen (Features i in den Dokumenten d) & der latenten Variablen (ϕ, θ)"
  },
  {
    "objectID": "slides/slides-08.html#beyond-lda",
    "href": "slides/slides-08.html#beyond-lda",
    "title": "🔨 Advanced Methods",
    "section": "Beyond LDA",
    "text": "Beyond LDA\nVerschiedene Ansätze der Themenmodellierung\n\n\nLatent Dirichlet Allocation [LDA] (Blei et al., 2003) ist ein probabilistisches generatives Modell, das davon ausgeht, dass jedes Dokumentin einem Korpus eine Mischung von Themen ist und jedes Wort im Dokument einem der Themen des Dokuments zuzuordnen ist.\nStructural Topic Modeling [STM] (Roberts et al., 2016; Roberts et al., 2019) erweitert LDA durch die Einbeziehung von Kovariaten auf Dokumentenebene und ermöglicht die Modellierung des Einflusses externer Faktoren auf die Themenprävalenz.\nWord embeddings (Word2Vec (Mikolov et al., 2013) , Glove (Pennington et al., 2014)) stellen Wörter als kontinuierliche Vektoren in einem hochdimensionalen Raum dar und erfassen semantische Beziehungen zwischen Wörtern basierend auf ihrem Kontext in den Daten.\nTopic Modeling mit Neural Networks (BERTopic(Devlin et al., 2019), Doc2Vec(Le & Mikolov, 2014)) nutzt Deep Learning-Architekturen, um automatisch latente Themen aus Textdaten zu lernen"
  },
  {
    "objectID": "slides/slides-08.html#opinion-matters",
    "href": "slides/slides-08.html#opinion-matters",
    "title": "🔨 Advanced Methods",
    "section": "Opinion matters",
    "text": "Opinion matters\nSentimentanalyse: Einführung und Anwedungsfälle\n\nAnwendung von Natural Language Processing (NLP), Textanalyse und Computational Linguistics, um\n\nsubjektive Informationen aus Texten zu extrahieren\nMeinung, Einstellung oder Emotionen zu bestimmten Themen oder Entitäten zu bestimmen \n\nWichtige Anwendungsgebiete sind Marketinganalysen, Produktbewertungen, politische Meinungsforschung und soziale Medien."
  },
  {
    "objectID": "slides/slides-08.html#the-very-good-the-good-and-the-ugly",
    "href": "slides/slides-08.html#the-very-good-the-good-and-the-ugly",
    "title": "🔨 Advanced Methods",
    "section": "The very good, the good and the ugly",
    "text": "The very good, the good and the ugly\nVerschiedene Methode der Sentimentanalyse\n\nRegelbasierte Ansätze: Verwenden von definierten Regeln und Wörterbüchern.\nMaschinelles Lernen: Verwendung von Klassifikatoren wie Naive Bayes, SVM.\nDeep Learning: Einsatz von neuronalen Netzen wie RNNs oder Transformers.\n\nWelche Aspekte werden untersucht?\n\nPolarität: positive, negative, neutrale.\nSubjektfunktion: Wer spricht? Wessen Meinung?\nIntensität: Stärke des Ausdrucks der Meinung."
  },
  {
    "objectID": "slides/slides-08.html#out-of-the-box-or-diy",
    "href": "slides/slides-08.html#out-of-the-box-or-diy",
    "title": "🔨 Advanced Methods",
    "section": "Out of the box or DIY?",
    "text": "Out of the box or DIY?\nWerkzeuge und Tools für Sentimentanalyse\n\nLinguistic Inquiry and Word Count [LIWC] (Tausczik & Pennebaker, 2009): Textanalysesoftware-Tool & Off-the-shelf-Dictionary.\nValence Aware Dictionary and sEntiment Reasoner [VADER] (Hutto & Gilbert, 2014): Regelbasierte Sentimentanalyse-Tool, das speziell für Social Media Texte entwickelt wurde\nTextBlob: Python-Bibliothek für Textverarbeitung, die auch Sentimentanalyse unterstützt.\nCommercial Tools: IBM Watson, Google Cloud Natural Language API, Microsoft Text Analytics.\n\nABER: Zunehmender Einsatz von Transformer-Modellen wie BERT (Devlin et al., 2019) und GPT für genauere Analysen."
  },
  {
    "objectID": "slides/slides-08.html#quick-reminder",
    "href": "slides/slides-08.html#quick-reminder",
    "title": "🔨 Advanced Methods",
    "section": "Quick reminder",
    "text": "Quick reminder\nDatengrundlage für die heutige Sitzung\n\n\n\n# Create corpus\ncorp_transcripts &lt;- transcripts %&gt;% \n  quanteda::corpus(\n    docid_field = \"id_sequence\", \n    text_field = \"dialogue\"\n  )\n\n# Tokenize corpus\ntoks_transcripts &lt;- corp_transcripts %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  ) %&gt;% \n  quanteda::tokens_remove(\n    pattern = quanteda::stopwords(\"en\")\n  )\n\n\n\n# Add n_grams\ntoks_transcripts_ngrams &lt;- toks_transcripts %&gt;% \n  quanteda::tokens_ngrams(n = 1:3)\n\n# Convert to DFM\ndfm_transcripts &lt;- toks_transcripts_ngrams %&gt;% \n  quanteda::dfm()"
  },
  {
    "objectID": "slides/slides-08.html#und-für-die-chats",
    "href": "slides/slides-08.html#und-für-die-chats",
    "title": "🔨 Advanced Methods",
    "section": "Und für die Chats",
    "text": "Und für die Chats\nDatengrundlage für die heutige Sitzung\n\n# Create corpus\ncorp_chats &lt;- chats %&gt;% \n  quanteda::corpus(docid_field = \"message_id\", text_field = \"message_content\")\n\n# Tokenize corpus\ntoks_chats &lt;- corp_chats %&gt;% quanteda::tokens()\n\n# Convert to DFM\ndfm_chats &lt;- toks_chats %&gt;% quanteda::dfm()"
  },
  {
    "objectID": "slides/slides-08.html#better-bag-of-words",
    "href": "slides/slides-08.html#better-bag-of-words",
    "title": "🔨 Advanced Methods",
    "section": "Better bag-of-words",
    "text": "Better bag-of-words\nCo-Occurence Verfahren und Ihre Einsatzgebiet\nAnnahme von bag-of-words Modellen (Inhalte eines Text lassen sich vollständig durch die Häufigkeit der in ihm vorkommenden Wörter beschreiben) problematisch\n\nVerbessung durch komplexere Verfahren bzw. Repräsentationen, wie z.B. Ngrams, Keywords-in-Context, Collocations, Semantische Netzwerke etc.\nPotentielle Anwendungsfälle:\n\nTextbereinigung, z.B. Entfernung von Duplikaten via ngram-shingling (Nicholls, 2019)\nTextanalyse, z.B. Stereotypen (Arendt & Karadas, 2017), Labeling, Frames (Ruigrok & Atteveldt, 2007)"
  },
  {
    "objectID": "slides/slides-08.html#häufige-wortkombinationen",
    "href": "slides/slides-08.html#häufige-wortkombinationen",
    "title": "🔨 Advanced Methods",
    "section": "Häufige Wortkombinationen",
    "text": "Häufige Wortkombinationen\nNgrams: Sequenzen von N aufeinanderfolgenden Token\n\ntoks_transcripts %&gt;% \n  quanteda::tokens_ngrams(n = 2) %&gt;% \n  quanteda::dfm() %&gt;%  \n  quanteda.textstats::textstat_frequency() %&gt;% \n  head(25) \n\n             feature frequency rank docfreq group\n1          know_know      1337    1      49   all\n2  t-mobile_t-mobile       864    2       6   all\n3       donald_trump       755    3     461   all\n4          going_say       666    4      30   all\n5          say_going       661    5      35   all\n6         saying_bad       558    6       4   all\n7         bad_saying       553    7       3   all\n8      kamala_harris       494    8     333   all\n9     vice_president       429    9     376   all\n10   curious_curious       373   10       7   all\n11    sekunden_pause       354   11     266   all\n12         right_now       269   12     234   all\n13     united_states       268   13     211   all\n14         feel_like       230   14     164   all\n15             oh_oh       229   15      10   all\n16         like_know       208   16     133   all\n17   president_trump       203   17     178   all\n18         like_like       191   18     144   all\n19  president_harris       186   19     179   all\n20        lot_people       181   20     138   all\n21         know_like       168   21     109   all\n22   american_people       163   22     118   all\n23            oh_god       154   23     139   all\n24         just_like       153   24     129   all\n25  former_president       141   25     115   all"
  },
  {
    "objectID": "slides/slides-08.html#statistisch-häufige-wortkombinationen",
    "href": "slides/slides-08.html#statistisch-häufige-wortkombinationen",
    "title": "🔨 Advanced Methods",
    "section": "Statistisch häufige Wortkombinationen",
    "text": "Statistisch häufige Wortkombinationen\nKollokationen: Identifikation von bedeutungsvollen Wortkombinationen\n\ntoks_transcripts %&gt;% \n  quanteda.textstats::textstat_collocations(\n    size = 2, \n    min_count = 5\n  ) %&gt;% \n  head(25)\n\n        collocation count count_nested length    lambda        z\n1         know know  1337            0      2  3.890787 98.31370\n2        saying bad   558            0      2  6.503305 81.19215\n3        bad saying   553            0      2  6.481795 80.98625\n4         going say   666            0      2  4.555737 80.92246\n5         say going   661            0      2  4.562963 80.82524\n6      donald trump   755            0      2  7.422847 75.19267\n7     kamala harris   494            0      2  7.873933 68.88003\n8    vice president   429            0      2  7.258104 56.72753\n9             oh oh   229            0      2  4.679549 54.95066\n10        right now   269            0      2  3.996328 53.27167\n11    senator vance   129            0      2  6.583164 49.48173\n12       little bit   132            0      2  8.268099 45.80914\n13 president harris   186            0      2  4.027681 45.69845\n14           oh god   154            0      2  6.175930 44.94423\n15        years ago   102            0      2  6.436807 43.06424\n16         tim walz    90            0      2  7.588398 43.05596\n17  president trump   203            0      2  3.468589 42.70406\n18       four years   100            0      2  6.381600 42.53221\n19      health care   136            0      2  7.400206 41.83123\n20      white house    85            0      2  8.071701 41.52827\n21   donald trump's   132            0      2  6.408658 41.05938\n22 former president   141            0      2  5.790480 41.04486\n23  curious curious   373            0      2 11.836611 40.77202\n24    governor walz    77            0      2  6.512020 39.65499\n25      two minutes    84            0      2  6.490910 39.39074\n\n\n\nEs wird die Häufigkeit von Wortkombinationen analysiert und mit erwarteten Häufigkeiten verglichen, um besonders signifikante Kollokationen zu identifizieren."
  },
  {
    "objectID": "slides/slides-08.html#spezifische-token-plus-kontext",
    "href": "slides/slides-08.html#spezifische-token-plus-kontext",
    "title": "🔨 Advanced Methods",
    "section": "Spezifische Token plus Kontext",
    "text": "Spezifische Token plus Kontext\nKeywords-in-Context (KWIC): Unmittelbarer Wortkontext ohne statistische Gewichtung\n\ntoks_transcripts %&gt;% \n  kwic(\"know\", window = 3) %&gt;% \nhead(10)\n\nKeyword-in-context with 10 matches.                                                                              \n [p1_s0018, 29]  opportunity economy thing | know | shortage homes housing    \n [p1_s0018, 39]            far many people | know | young families need       \n [p1_s0020, 25]  billions billions dollars | know | China fact never          \n [p1_s0022, 44]          done intend build | know | aspirations hopes American\n  [p1_s0024, 2]                    nothing | know | knows better anyone       \n  [p1_s0025, 1]                            | know | everybody else Vice       \n [p1_s0026, 64]        stand issues invite | know | Donald Trump actually     \n [p1_s0028, 38]       goods coming country | know | many economists say       \n [p1_s0029, 24] billions dollars countries | know | like gone immediately     \n [p1_s0031, 90]         Thank President Xi | know | Xi responsible lacking"
  },
  {
    "objectID": "slides/slides-08.html#schlüsselphrase-plus-kontext",
    "href": "slides/slides-08.html#schlüsselphrase-plus-kontext",
    "title": "🔨 Advanced Methods",
    "section": "Schlüsselphrase plus Kontext",
    "text": "Schlüsselphrase plus Kontext\nEinsatz von Keywords-in-Context (KWIC) zur Qualitätskontrolle\n\n\ntoks_transcripts %&gt;% \n  kwic(\n    phrase(\"know know\"),\n    window = 3) %&gt;%\n  tibble() %&gt;% \n  select(-pattern) %&gt;% \n  slice(35:45) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(\n        table.width = gt::pct(100), \n        table.font.size = \"10px\"\n    )\n\n\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\n\n\n\n\nvp2_s0723\n94\n95\nkiss just kiss\nknow know\njust kiss kiss\n\n\nvp2_s0732\n119\n120\ndefault press even\nknow know\ndifference campaign strategy\n\n\nvp3_s0151\n32\n33\ncop able assess\nknow know\nJ.D Vance lying\n\n\nvp3_s0332\n3\n4\nreally mean\nknow know\nmany people tune\n\n\nvp3_s0332\n116\n117\ngenerous Sekunden Pause\nknow know\ntype like know\n\n\nvp3_s0332\n120\n121\nknow type like\nknow know\ntype like know\n\n\nvp3_s0332\n124\n125\nknow type like\nknow know\nknow know know\n\n\nvp3_s0332\n125\n126\ntype like know\nknow know\nknow know know\n\n\nvp3_s0332\n126\n127\nlike know know\nknow know\nknow know know\n\n\nvp3_s0332\n127\n128\nknow know know\nknow know\nknow know know\n\n\nvp3_s0332\n128\n129\nknow know know\nknow know\nknow know know"
  },
  {
    "objectID": "slides/slides-08.html#ngrams-als-features-definieren",
    "href": "slides/slides-08.html#ngrams-als-features-definieren",
    "title": "🔨 Advanced Methods",
    "section": "Ngrams als Features definieren",
    "text": "Ngrams als Features definieren\nSteigerung der Datenqualität durch Berücksichtigung von Ngrams-Features\n\n# Definition von Features\ncustom_ngrams &lt;- c(\"donald trump\", \"joe biden\", \"kamala harris\")\n\n# Anwendung auf DFM\ndfm_with_custom_ngrams &lt;- toks_transcripts %&gt;% \n  tokens_compound(pattern = phrase(custom_ngrams)) %&gt;% \n  dfm() %&gt;% \n  dfm_trim(min_docfreq = 0.005, max_docfreq = 0.99, docfreq_type = \"prop\") \n\n# Überprüfung\ndfm_with_custom_ngrams %&gt;% \n  convert(to = \"data.frame\") %&gt;% \n  select(doc_id, starts_with(\"donald\")) %&gt;% \n  head()\n\n    doc_id donald_trump donald\n1 p1_s0001            1      0\n2 p1_s0002            1      0\n3 p1_s0003            0      0\n4 p1_s0004            0      0\n5 p1_s0005            0      0\n6 p1_s0006            1      0"
  },
  {
    "objectID": "slides/slides-08.html#häufige-zusammen-verwendete-emotes",
    "href": "slides/slides-08.html#häufige-zusammen-verwendete-emotes",
    "title": "🔨 Advanced Methods",
    "section": "Häufige zusammen verwendete Emotes",
    "text": "Häufige zusammen verwendete Emotes\nSemantische Netzwerke: Visualisierung von Tokenbeziehungen\n\n\n# Lookup emotes in DFM of chats\ndfm_emotes &lt;- dfm_chats %&gt;% \n  quanteda::dfm_lookup(\n    dictionary = dict_chat_emotes)\n\n# Output frequency of emojis\ntop50_emotes &lt;- dfm_emotes %&gt;% \n  topfeatures(50) %&gt;% \n  names()\n\n# Visualize\ndfm_emotes  %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top50_emotes) %&gt;% \n  textplot_network()"
  },
  {
    "objectID": "slides/slides-08.html#berücksichtigung-der-syntax",
    "href": "slides/slides-08.html#berücksichtigung-der-syntax",
    "title": "🔨 Advanced Methods",
    "section": "Berücksichtigung der Syntax",
    "text": "Berücksichtigung der Syntax\nPart-of-Speech Tagging: Hintergrund & Anwendungsbeispiele\n\n(Jurafsky & Martin, 2024, p. S.366)\n“process of assigning a part-of-speech to each word in a text” (Jurafsky & Martin, 2024, p. S.365)\n\n\nBeispiele für Anwendungsfälle:\n\nanalysieren, ob es sich bei einem Feature um ein Adjektiv handelt, das sich auf ein bestimmtes Substantiv bezieht\nzwischen gleichen Features mit unterschiedlichen Bedeutungen unterscheiden („Sound solution“ vs. „What is that sound“?)"
  },
  {
    "objectID": "slides/slides-08.html#dependency-parsing",
    "href": "slides/slides-08.html#dependency-parsing",
    "title": "🔨 Advanced Methods",
    "section": "Dependency Parsing",
    "text": "Dependency Parsing\nHintegrund und Anwendungsfälle\n\n\n“the syntactic structure of a sentence […] in terms of directed binary grammatical relations between the words” (Jurafsky & Martin, 2024, p. S.411)\n\n\nBeispiele für Anwendungsfälle:\n\nanalysieren, ob es sich bei einem Feature um ein Adjektiv handelt, das sich auf ein bestimmtes Substantiv bezieht\nzwischen gleichen Features mit unterschiedlichen Bedeutungen unterscheiden („Sound solution“ vs. „What is that sound“?)"
  },
  {
    "objectID": "slides/slides-08.html#praktische-umsetzung-mit-udpipe-in-r",
    "href": "slides/slides-08.html#praktische-umsetzung-mit-udpipe-in-r",
    "title": "🔨 Advanced Methods",
    "section": "Praktische Umsetzung mit udpipe in R",
    "text": "Praktische Umsetzung mit udpipe in R\nBeispiele für POS-Tagging & Dependency Parsing\n\nudmodel &lt;- udpipe::udpipe_download_model(language = \"english\")\n\ntranscripts_pos &lt;- transcripts %&gt;%\n  rename(doc_id = id_sequence, text = dialogue) %&gt;% \n  udpipe::udpipe(udmodel)\n\n\ntranscripts_pos %&gt;% \n  select(doc_id, sentence_id, token_id, token, head_token_id, lemma, upos, xpos) %&gt;% \n  head(n = 7) %&gt;% \n  gt() %&gt;% gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(table.width = gt::pct(100), table.font.size = \"12px\")\n\n\n\n\n\n\n\ndoc_id\nsentence_id\ntoken_id\ntoken\nhead_token_id\nlemma\nupos\nxpos\n\n\n\n\np1_s0001\n1\n1\nTonight\n0\ntonight\nNOUN\nNN\n\n\np1_s0001\n1\n2\n,\n1\n,\nPUNCT\n,\n\n\np1_s0001\n1\n3\nthe\n7\nthe\nDET\nDT\n\n\np1_s0001\n1\n4\nhigh\n6\nhigh\nADJ\nJJ\n\n\np1_s0001\n1\n5\n-\n6\n-\nPUNCT\nHYPH\n\n\np1_s0001\n1\n6\nstakes\n7\nstake\nNOUN\nNNS\n\n\np1_s0001\n1\n7\nshowdown\n1\nshowdown\nNOUN\nNN"
  },
  {
    "objectID": "slides/slides-08.html#mit-welchen-wörtern-wird-trump-beschrieben",
    "href": "slides/slides-08.html#mit-welchen-wörtern-wird-trump-beschrieben",
    "title": "🔨 Advanced Methods",
    "section": "Mit welchen Wörtern wird Trump beschrieben?",
    "text": "Mit welchen Wörtern wird Trump beschrieben?\nAnwendung & Probleme von POS-Tagging\n\n\ntranscripts_pos %&gt;% \n    filter(\n      upos == \"NOUN\" &\n      lemma == \"trump\") %&gt;%\n    inner_join(\n      transcripts_pos,\n      by = c(\n        \"doc_id\",\n        \"sentence_id\"),\n      relationship = \n        \"many-to-many\") %&gt;%\n    filter(\n      upos.y == \"ADJ\" &\n      head_token_id.y == token_id.x) %&gt;% \n    rename(\n      token_id = token_id.y,\n      token = token.y) %&gt;% \n    select(\n      doc_id, sentence_id,\n      token_id, token) %&gt;%\n    sjmisc::frq(token, sort.frq = \"desc\") \n\n\ntoken &lt;character&gt; \n# total N=161 valid N=161 mean=3.72 sd=4.67\n\nValue        |   N | Raw % | Valid % | Cum. %\n---------------------------------------------\ndonald       | 132 | 81.99 |   81.99 |  81.99\nDonald       |   4 |  2.48 |    2.48 |  84.47\num           |   3 |  1.86 |    1.86 |  86.34\nformer       |   2 |  1.24 |    1.24 |  87.58\nnarcissistic |   2 |  1.24 |    1.24 |  88.82\nbad          |   1 |  0.62 |    0.62 |  89.44\ngood         |   1 |  0.62 |    0.62 |  90.06\ngreat        |   1 |  0.62 |    0.62 |  90.68\niran         |   1 |  0.62 |    0.62 |  91.30\nlaura        |   1 |  0.62 |    0.62 |  91.93\nmuch         |   1 |  0.62 |    0.62 |  92.55\nokay         |   1 |  0.62 |    0.62 |  93.17\nother        |   1 |  0.62 |    0.62 |  93.79\npast         |   1 |  0.62 |    0.62 |  94.41\nSaid         |   1 |  0.62 |    0.62 |  95.03\nselfish      |   1 |  0.62 |    0.62 |  95.65\nsocial       |   1 |  0.62 |    0.62 |  96.27\ntighter      |   1 |  0.62 |    0.62 |  96.89\ntotal        |   1 |  0.62 |    0.62 |  97.52\nunfit        |   1 |  0.62 |    0.62 |  98.14\nunseat       |   1 |  0.62 |    0.62 |  98.76\nweaker       |   1 |  0.62 |    0.62 |  99.38\nweird        |   1 |  0.62 |    0.62 | 100.00\n&lt;NA&gt;         |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;"
  },
  {
    "objectID": "slides/slides-08.html#besser-mit-spacyr-aber-noch-nicht-gut",
    "href": "slides/slides-08.html#besser-mit-spacyr-aber-noch-nicht-gut",
    "title": "🔨 Advanced Methods",
    "section": "Besser mit spacyr, aber noch nicht gut",
    "text": "Besser mit spacyr, aber noch nicht gut\nAnwendung & Auswertung von POS-Tagging\n\n\ntranscripts_spacyr %&gt;%  \n    filter(\n      pos == \"NOUN\" &\n      lemma == \"trump\") %&gt;%\n    inner_join(\n      transcripts_spacyr,\n      by = c(\n        \"doc_id\",\n        \"sentence_id\"),\n      relationship = \n        \"many-to-many\") %&gt;%\n    filter(\n      pos.y == \"ADJ\" &\n      head_token_id.y == token_id.x) %&gt;% \n    rename(\n      token_id = token_id.y,\n      token = token.y) %&gt;% \n    select(\n      doc_id, sentence_id,\n      token_id, token) %&gt;%\n    sjmisc::frq(token, sort.frq = \"desc\") \n\n\ntoken &lt;character&gt; \n# total N=10 valid N=10 mean=5.40 sd=2.88\n\nValue        | N | Raw % | Valid % | Cum. %\n-------------------------------------------\nunfit        | 2 |    20 |      20 |     20\nbad          | 1 |    10 |      10 |     30\ndonald       | 1 |    10 |      10 |     40\nfucking      | 1 |    10 |      10 |     50\nnarcissistic | 1 |    10 |      10 |     60\nother        | 1 |    10 |      10 |     70\nsame         | 1 |    10 |      10 |     80\ntighter      | 1 |    10 |      10 |     90\ntotal        | 1 |    10 |      10 |    100\n&lt;NA&gt;         | 0 |     0 |    &lt;NA&gt; |   &lt;NA&gt;"
  },
  {
    "objectID": "slides/slides-08.html#welche-neue-daten-stehen-zur-verfügung",
    "href": "slides/slides-08.html#welche-neue-daten-stehen-zur-verfügung",
    "title": "🔨 Advanced Methods",
    "section": "Welche neue Daten stehen zur Verfügung?",
    "text": "Welche neue Daten stehen zur Verfügung?\nÜberblick über die neuen Datensätze\nSowohl für die Chats & Transkripte werden mehrere Korpora hinzugefügt\n\nDatensatz ...-corpus_udpipe.qs enthält einen mit dem Paket udpipe (v0.8.11, Wijffels, 2023) verarbeiteten Datensatz\nDatensatz ...-corpus_spacy.qsenthält einen mit dem Paket spacyr (v1.3.0, Benoit & Matsuo, 2023) verarbeiteten Datensatz\n\nCode für die Erstellung der Datensätze in der Sektion “Data colletion” auf der Homepage"
  },
  {
    "objectID": "slides/slides-08.html#quick-overview",
    "href": "slides/slides-08.html#quick-overview",
    "title": "🔨 Advanced Methods",
    "section": "Quick overview",
    "text": "Quick overview\nudpipe-Korpus\n\n# Corpus processed with udpipe\ntranscripts_udpipe %&gt;% glimpse\n\nRows: 596,711\nColumns: 17\n$ doc_id        &lt;chr&gt; \"p1_s0001\", \"p1_s0001\", \"p1_s0001\", \"p1_s0001\", \"p1_s000…\n$ paragraph_id  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ sentence_id   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ sentence      &lt;chr&gt; \"Tonight, the high-stakes showdown here in Philadelphia …\n$ start         &lt;int&gt; 1, 8, 10, 14, 18, 19, 26, 35, 40, 43, 56, 64, 69, 79, 86…\n$ end           &lt;int&gt; 7, 8, 12, 17, 18, 24, 33, 38, 41, 54, 62, 67, 77, 84, 91…\n$ term_id       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ token_id      &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\",…\n$ token         &lt;chr&gt; \"Tonight\", \",\", \"the\", \"high\", \"-\", \"stakes\", \"showdown\"…\n$ lemma         &lt;chr&gt; \"tonight\", \",\", \"the\", \"high\", \"-\", \"stake\", \"showdown\",…\n$ upos          &lt;chr&gt; \"NOUN\", \"PUNCT\", \"DET\", \"ADJ\", \"PUNCT\", \"NOUN\", \"NOUN\", …\n$ xpos          &lt;chr&gt; \"NN\", \",\", \"DT\", \"JJ\", \"HYPH\", \"NNS\", \"NN\", \"RB\", \"IN\", …\n$ feats         &lt;chr&gt; \"Number=Sing\", NA, \"Definite=Def|PronType=Art\", \"Degree=…\n$ head_token_id &lt;chr&gt; \"0\", \"1\", \"7\", \"6\", \"6\", \"7\", \"1\", \"7\", \"13\", \"13\", \"13\"…\n$ dep_rel       &lt;chr&gt; \"root\", \"punct\", \"det\", \"amod\", \"punct\", \"compound\", \"ap…\n$ deps          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ misc          &lt;chr&gt; \"SpaceAfter=No\", NA, NA, \"SpaceAfter=No\", \"SpaceAfter=No…"
  },
  {
    "objectID": "slides/slides-08.html#quick-overview-1",
    "href": "slides/slides-08.html#quick-overview-1",
    "title": "🔨 Advanced Methods",
    "section": "Quick overview",
    "text": "Quick overview\nspacyr-Korpus\n\n# Corpus processed with spacyr\ntranscripts_spacyr %&gt;% glimpse\n\nRows: 596,923\nColumns: 10\n$ doc_id        &lt;chr&gt; \"p1_s0001\", \"p1_s0001\", \"p1_s0001\", \"p1_s0001\", \"p1_s000…\n$ sentence_id   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ token_id      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ token         &lt;chr&gt; \"Tonight\", \",\", \"the\", \"high\", \"-\", \"stakes\", \"showdown\"…\n$ lemma         &lt;chr&gt; \"tonight\", \",\", \"the\", \"high\", \"-\", \"stake\", \"showdown\",…\n$ pos           &lt;chr&gt; \"NOUN\", \"PUNCT\", \"DET\", \"ADJ\", \"PUNCT\", \"NOUN\", \"NOUN\", …\n$ tag           &lt;chr&gt; \"NN\", \",\", \"DT\", \"JJ\", \"HYPH\", \"NNS\", \"NN\", \"RB\", \"IN\", …\n$ head_token_id &lt;dbl&gt; 7, 7, 7, 6, 6, 7, 7, 7, 8, 9, 7, 13, 15, 15, 11, 15, 18,…\n$ dep_rel       &lt;chr&gt; \"npadvmod\", \"punct\", \"det\", \"amod\", \"punct\", \"compound\",…\n$ entity        &lt;chr&gt; \"TIME_B\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"GPE_B\", \"\", \"…"
  },
  {
    "objectID": "slides/slides-08.html#and-now-you",
    "href": "slides/slides-08.html#and-now-you",
    "title": "🔨 Advanced Methods",
    "section": "🧪 And now … you!",
    "text": "🧪 And now … you!\nNext steps\n\nLaden das .zip-Archiv zur Sitzung von StudOn herunter und entpacke die Dateien an einen Ort deiner Wahl.\nDoppelklicke auf die Datei dbd_exercise.Rproj, um das RStudio-Projekt zu öffnen. Dies stellt sicher, dass alle Abhängigkeiten korrekt funktionieren.\nÖffnen die Datei exercise_08.qmd und folge den Anweisungen.\nTipp: Alle im Vortrag verwendeten Code-Schnipsel findest du im der Tutorial-Datei zur Sitzung."
  },
  {
    "objectID": "slides/slides-08.html#references",
    "href": "slides/slides-08.html#references",
    "title": "🔨 Advanced Methods",
    "section": "References",
    "text": "References\n\n\nArendt, F., & Karadas, N. (2017). Content analysis of mediated associations: An automated text-analytic approach. Communication Methods and Measures, 11(2), 105–120. https://doi.org/10.1080/19312458.2016.1276894\n\n\nBenoit, K., & Matsuo, A. (2023). Spacyr: Wrapper to the ’spaCy’ ’NLP’ library. https://spacyr.quanteda.io\n\n\nBlei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. The Journal of Machine Learning Research, 3, 9931022.\n\n\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding (J. Burstein, C. Doran, & T. Solorio, Eds.; p. 41714186). Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1423\n\n\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267–297. https://doi.org/10/f458q9\n\n\nHutto, C., & Gilbert, E. (2014). VADER: A parsimonious rule-based model for sentiment analysis of social media text. Proceedings of the International AAAI Conference on Web and Social Media, 8(1), 216–225. https://doi.org/10.1609/icwsm.v8i1.14550\n\n\nJurafsky, D., & Martin, J. H. (2024). Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition with language models (3rd ed.). https://web.stanford.edu/~jurafsky/slp3/\n\n\nLe, Q., & Mikolov, T. (2014). Distributed representations of sentences and documents (E. P. Xing & T. Jebara, Eds.; Vol. 32, p. 11881196). PMLR. https://proceedings.mlr.press/v32/le14.html\n\n\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., Häussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93–118. https://doi.org/10.1080/19312458.2018.1430754\n\n\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality (C. J. Burges, L. Bottou, M. Welling, Z. Ghahramani, & K. Q. Weinberger, Eds.; Vol. 26). Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n\n\nNicholls, T. (2019). Detecting Textual Reuse in News Stories, At Scale. International Journal of Communication, 13(0), 25. https://ijoc.org/index.php/ijoc/article/view/9904\n\n\nPennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word representation. 15321543. https://doi.org/10.3115/v1/D14-1162\n\n\nRoberts, M. E., Stewart, B. M., & Airoldi, E. M. (2016). A model of text for experimentation in the social sciences. Journal of the American Statistical Association, 111(515), 988–1003. https://doi.org/10/f88tzh\n\n\nRoberts, M. E., Stewart, B. M., & Tingley, D. (2019). stm: An R Package for Structural Topic Models. Journal of Statistical Software, 91(1), 1–40. https://doi.org/10.18637/jss.v091.i02\n\n\nRuigrok, N., & Atteveldt, W. van. (2007). Global Angling with a Local Angle: How U.S., British, and Dutch Newspapers Frame Global and Local Terrorist Attacks. Harvard International Journal of Press/Politics, 12(1), 68–90. https://doi.org/10.1177/1081180X06297436\n\n\nTausczik, Y. R., & Pennebaker, J. W. (2009). The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods. Journal of Language and Social Psychology, 29(1), 24–54. https://doi.org/10.1177/0261927x09351676\n\n\nWijffels, J. (2023). Udpipe: Tokenization, parts of speech tagging, lemmatization and dependency parsing with the ’UDPipe’ ’NLP’ toolkit. https://CRAN.R-project.org/package=udpipe"
  },
  {
    "objectID": "slides/slides-02.html#kurzes-update",
    "href": "slides/slides-02.html#kurzes-update",
    "title": "Überblick & Einführung",
    "section": "Kurzes Update",
    "text": "Kurzes Update\nAllgemeine Infos zum Kurs\n\nHaben alle sich für die Prüfung angemeldet? Gibt es noch Fragen zum Sonderanmeldetermin?\nHaben alle eine Benachrichtung für den Post im  StudOn-Forum bekommen?\nHaben alle die 📖 Basisliteratur gefunden? Gibt es Fragen?\nPräsentationsgruppe 1&2: Denkt bitte an die Zusendung des Entwurf der Präsentationsfolien (bis nächsten Dienstag bis 12:00) und das Feedbackgespräch nächste Woche!"
  },
  {
    "objectID": "slides/slides-02.html#finale-themenvergabe",
    "href": "slides/slides-02.html#finale-themenvergabe",
    "title": "Überblick & Einführung",
    "section": "Finale Themenvergabe",
    "text": "Finale Themenvergabe\nÜberblick über die Gruppenverteilung\n\n\n\n\n\n\n\n\n\nGruppe\nThema\nStudierende\n\n\n\n\n1\nMotivation der Nutzung von Twitch\nAzat, Heimstädt\n\n\n2\nKommunikation und Interaktion auf Twitch\nBurmeister, Fischer, Erdogmus\n\n\n3\n(Wirkungs-)Effekte der Twitch-Nutzung/Interaktion\nDierking, Reineke\n\n\n4\n(Wirkungs-)Effekte von TV-Wahldebatten\nSpickenreuther, Wolf\n\n\n5\nWechselwirkung zwischen TV-Debatten und Twitter\nGierth, Landgraf\n\n\n6\nLive-Chat(-Kommentare) in politischen Debatten\nMach, Stadler, Weiß"
  },
  {
    "objectID": "slides/slides-02.html#semsterplan",
    "href": "slides/slides-02.html#semsterplan",
    "title": "Überblick & Einführung",
    "section": "Semsterplan",
    "text": "Semsterplan\n\n\n\n\n\n\n\n\nSession\nDatum\nTopic\nPresenter\n\n\n\n\n\n📂 Block 1\nIntroduction\n\n\n\n1\n23.10.2024\nKick-Off\nChristoph Adrian\n\n\n2\n30.10.2024\nDBD: Overview & Introduction\nChristoph Adrian\n\n\n3\n06.11.2024\n🔨 Introduction to working with R\nChristoph Adrian\n\n\n\n📂 Block 2\nTheoretical Background: Twitch & TV Election Debates\n\n\n\n4\n13.11.2024\n📚 Twitch-Nutzung im Fokus\nStudent groups\n\n\n5\n20.11.2024\n📚 (Wirkungs-)Effekte von Twitch & TV-Debatten\nStudent groups\n\n\n6\n27.11.2024\n📚 Politische Debatten & Social Media\nStudent groups\n\n\n\n📂 Block 3\nMethod: Natural Language Processing\n\n\n\n7\n04.12.2024\n🔨 Text as data I: Introduction\nChristoph Adrian\n\n\n8\n11.12.2024\n🔨 Text as data I: Advanced Methods\nChristoph Adrian\n\n\n9\n18.12.2024\n🔨 Advanced Method I: Topic Modeling\nChristoph Adrian\n\n\n\nNo lecture\n🎄Christmas Break\n\n\n\n10\n08.01.2025\n🔨 Advanced Method I: Machine Learning\nChristoph Adrian\n\n\n\n📂 Block 4\nProject Work\n\n\n\n11\n15.01.2025\n🔨 Project work\nStudent groups\n\n\n12\n22.01.2025\n🔨 Project work\nStudent groups\n\n\n13\n29.01.2025\n📊 Project Presentation I\nStudent groups (TBD)\n\n\n14\n05.02.2025\n📊 Project Presentation & 🏁 Evaluation\nStudentds (TBD) & Christoph Adrian"
  },
  {
    "objectID": "slides/slides-02.html#was-ist-das-eigentlich",
    "href": "slides/slides-02.html#was-ist-das-eigentlich",
    "title": "Überblick & Einführung",
    "section": "Was ist das eigentlich?",
    "text": "Was ist das eigentlich?\nRückblick auf einen Definitionversuch von Weller (2021)\n\n\n… fasst eine Vielzahl von möglichen Datenquellen zusammen, die verschiedene Arten von Aktivitäten aufzeichnen (häufig sogar “nur” als Nebenprodukt)\n… können dabei helfen, Meinungen, Verhalten und Merkmale der menschlichen Nutzung digitaler Technologien zu erkennen"
  },
  {
    "objectID": "slides/slides-02.html#und-im-kontext-des-seminars",
    "href": "slides/slides-02.html#und-im-kontext-des-seminars",
    "title": "Überblick & Einführung",
    "section": "Und im Kontext des Seminars?",
    "text": "Und im Kontext des Seminars?\nArbeitsdefinition & Kernbereiche (GESIS) von DBD\n\n\n\nDBD umfasst digitale Beobachtungen menschlichen und algorithmischen Verhaltens,\nwie sie z.B. von Online-Plattformen (wie Google, Facebook oder dem World Wide Web) oder\nSensoren (wie Smartphones, RFID-Sensoren, Satelliten oder Street View-Kameras) erfasst werden."
  },
  {
    "objectID": "slides/slides-02.html#die-power-von-social-sensing",
    "href": "slides/slides-02.html#die-power-von-social-sensing",
    "title": "Überblick & Einführung",
    "section": "Die Power von Social Sensing",
    "text": "Die Power von Social Sensing\nForschungsdesign zur Erhebung digitaler Verhaltensdaten (Flöck & Sen, 2022)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Zukunft: Linking"
  },
  {
    "objectID": "slides/slides-02.html#mit-fokus-auf-die-platform",
    "href": "slides/slides-02.html#mit-fokus-auf-die-platform",
    "title": "Überblick & Einführung",
    "section": "Mit Fokus auf die Platform",
    "text": "Mit Fokus auf die Platform\nForschungsdesign zur Erhebung digitaler Verhaltensdaten (Flöck & Sen, 2022)"
  },
  {
    "objectID": "slides/slides-02.html#online-plattformen-prägen-die-gesellschaft",
    "href": "slides/slides-02.html#online-plattformen-prägen-die-gesellschaft",
    "title": "Überblick & Einführung",
    "section": "Online-Plattformen prägen die Gesellschaft",
    "text": "Online-Plattformen prägen die Gesellschaft\nGründe für den Fokus auf Onlineplattformen (Ulloa, 2021)\n\n\nvermitteln & formen menschliche Kommunikation (z.B. Tweet mit 280 Zeichen)\npolitische (Miss-)Nutzung\nGatekeeper für Informationen (z.B. “Dr.Google”)\ntägliche algorithmische Empfehlungen und Werbung: Nachrichten, Produkte, Jobangebote, Bewerbungen, Versicherungen, Hotels, …\n\n\nABER: Berücksichtigung der Art und Weise, wie die Daten gesammelt werden!"
  },
  {
    "objectID": "slides/slides-02.html#eine-kleine-lobeshymne-auf-dbd",
    "href": "slides/slides-02.html#eine-kleine-lobeshymne-auf-dbd",
    "title": "Überblick & Einführung",
    "section": "Eine kleine Lobeshymne auf DBD",
    "text": "Eine kleine Lobeshymne auf DBD\nZwischenfazit\n\nDigitale Geräte oder Sensoren können sich an bestimmte Fakten besser “erinnern” als das menschliche Gedächtnis.\nSensoren sind oft bereits in alltägliche Technologie eingebaut und produzieren digitale Verhaltensdaten als ein “Nebenprodukt”.\nUnaufdringliche Erfassung als potentieller Vorteil bzw. Entlastung für Teilnehmer*Innen\nKombination mit Umfragedaten möglich (und bereichernd!)\n\n\nAber: Berücksichtigung der Rahmenbedingungen!\nZur erfolgreichen Nutzung müssen Forschungsziele & verfügbare Daten in Einklang gebracht, mögliche Biases und methodische Probleme berücksichtigt sowie die Datenqualität evaluiert werden.\n\n\nBietet die Plattform Zugang zu den benötigten Daten? Wenn nicht, gibt es alternative Weg um an die Daten zu gelangen? Wenn ja, ist dies legal/ethisch?"
  },
  {
    "objectID": "slides/slides-02.html#wenn-der-vorteil-zum-nachteil-wird",
    "href": "slides/slides-02.html#wenn-der-vorteil-zum-nachteil-wird",
    "title": "Überblick & Einführung",
    "section": "Wenn der Vorteil zum Nachteil wird",
    "text": "Wenn der Vorteil zum Nachteil wird\nAmbivalenz der Unaufdringlichkeit (Engel et al., 2021)\n\nUnterscheidung zwischen aufdringlichen (z.B. spezielle Research-App & Befragungen) & unaufdringlichen (z.B. Cookies, Browserplugins & APIs) erhobenen Daten\nBewertung und Erwartung an Datensammlung ist abhängig vom Kontext (z.B. Amazon vs. Researchgate)\n\n\nParadoxes Dilemma\nEinerseits bereitwillige (oft unwissende) Abgabe der Daten an Konzerne ohne Wissen um deren Weiterverarbeitung, andererseits häufig Bedenken bezüglich Datenschutz & Privatsphäre bei wissenschaftlichen Studien, die über Verwendung der Daten aufklären.\n\n\nWarum? Persönlicher Nutzen?"
  },
  {
    "objectID": "slides/slides-02.html#eher-konzept-als-begriff",
    "href": "slides/slides-02.html#eher-konzept-als-begriff",
    "title": "Überblick & Einführung",
    "section": "Eher Konzept als Begriff",
    "text": "Eher Konzept als Begriff\nZur Ambigutität des Begriffes bias und dessen Bedeutung im Seminar\n\n\nProblem: keine klare Grenzen zwischen den eher normativen Konnotationen (z.B. confirmation bias) und der statistischen Bedeutung des Begriffs (z.B. selection bias)\nDeswegen: Bewusstsein für Ambiguität des Begriffes\n\nVerwendung in vielen Disziplinen wie der Sozialwissenschaft, der kognitiven Psychologie oder dem Recht\nUntersuchung von verschiedenen Phänomenen, wie kognitive Voreingenommenheiten (Croskerry, 2002) sowie systemische, diskriminierende Ergebnisse (Friedman & Nissenbaum, 1996) oder Schäden (Barocas & Selbst, 2016), aktuell z.B. bei der Verwendung von Machine Learning oder AI.\n\n\n\n\n\n\nVerwendung des Begriff hauptsächlich in seiner statistischen Bedeutung, um auf Verzerrungen in sozialen Daten und deren Analysen hinzuweisen."
  },
  {
    "objectID": "slides/slides-02.html#know-your-bias",
    "href": "slides/slides-02.html#know-your-bias",
    "title": "Überblick & Einführung",
    "section": "Know your bias!",
    "text": "Know your bias!\nFramework zur Minimierung von Fehlern und Problemen (Olteanu et al., 2019)\n\n\nBeschreibung:\n\nDie Analyse sozialer Daten beginnt mit bestimmten Zielen (Abschnitt 2.1), wie dem Verständnis oder der Beeinflussung von Phänomenen, die für soziale Plattformen spezifisch sind (Typ I) und/oder von Phänomenen, die über soziale Plattformen hinausgehen (Typ II).\nDiese Ziele erfordern, dass die Forschung bestimmte Validitätskriterien erfüllt, die weiter oben beschrieben wurden (Abschnitt 2.2).\nDiese Kriterien können ihrerseits durch eine Reihe von allgemeinen Verzerrungen und Problemen beeinträchtigt werden (Abschnitt 3).\nDiese Herausforderungen können von den Merkmalen der einzelnen Datenplattformen (Abschnitt 4) abhängen - die oft nicht unter der Kontrolle der Forschenden stehen - und von den Entscheidungen des Forschungsdesigns entlang einer Datenverarbeitungspipeline (Abschnitte 5 bis 8) - die oft unter der Kontrolle des Forschers stehen.\nPfeile zeigen an, wie sich Komponenten in unserem Rahmenwerk direkt auf andere auswirken"
  },
  {
    "objectID": "slides/slides-02.html#the-biggest-problem-of-them-all",
    "href": "slides/slides-02.html#the-biggest-problem-of-them-all",
    "title": "Überblick & Einführung",
    "section": "The biggest problem of them all",
    "text": "The biggest problem of them all\nPotentielle Probleme mit der Qualität der Daten\n\n\n\nDefinition Data bias (Olteanu et al., 2019)\nA systematic distortion in the sampled data that compromises its representativeness.\n\n\n\n\nSparsity: Häufig Heavy-Tail-Verteilung, was Analyse am “Kopf” (in Bezug auf häufige Elemente oder Phänomene) erleichtert, am “Schwanz” (wie seltene Elemente oder Phänomene) jedoch erschwert (Baeza-Yates, 2013)\nNoise: Unvollständige, beschädigte, unzuverlässige oder unglaubwürdige Inhalte (boyd & Crawford, 2012; Naveed et al., 2011)\n\nAber: Unterscheidung von “Noise” und “Signal” ist oft unklar und hängt von der Forschungsfrage ab (Salganik, 2018)\n\nOrganische vs gemessene Daten: Fragen zur Repräsentativität (vs. Stichprobenbeschreibung), Kausalität (vs. Korrelation) und Vorhersagegüte"
  },
  {
    "objectID": "slides/slides-02.html#bias-at-the-source",
    "href": "slides/slides-02.html#bias-at-the-source",
    "title": "Überblick & Einführung",
    "section": "Bias at the source",
    "text": "Bias at the source\nPotentielle Probleme mit der Datenquelle oder -herkunft\n\nBiases, die auf das Design und die Möglichkeiten der Plattformen zurückzuführen sind (functional biases).\nVerhaltensnormen, die auf den einzelnen Plattformen bestehen oder sich herausbilden (normative biases).\nFaktoren, die außerhalb der sozialen Plattformen liegen, aber das Nutzerverhalten beeinflussen können (external biases)\nVorhandensein von nicht-individuellen Konten ein (non-individuals).\n\n\nfunctional biases:\n- Platform-specific design and features shape user behavior (z.B. Emojis) - Algorithms used for organizing and ranking content influence user behavior - Content presentation influences user behavior (z.B. UI)\nnormative biases:\n\nNorms are shaped by the attitudes and behaviors of online communities, which may be context-dependent (z.B. Partyfotos auf Instagram, aber nicht LinkedIn)\nThe awareness of being observed by others impacts user behavio (Anonymität vs Klarnamen)\nSocial conformity and “herding” happen in social platforms, and such behavioral traits shape user behavior (z.B. Ratings beinflussen eigenes Rating)\n\nexternal biase:\n\nCultural elements and social contexts are reflected in social datasets. (Zeichenlimit Japan vs. Deutschland)\nMisinformation and disinformation.\nContents on different topics are treated differently.\nHigh-impact events, whether anticipated or not, are reflected on social media (z.B. Feiertage)\n\nnon-individual-accounts: Organizational accounts, Bots"
  },
  {
    "objectID": "slides/slides-02.html#gefangen-im-spannungsverhältnis",
    "href": "slides/slides-02.html#gefangen-im-spannungsverhältnis",
    "title": "Überblick & Einführung",
    "section": "Gefangen im Spannungsverhältnis",
    "text": "Gefangen im Spannungsverhältnis\nForschungethik bei digitalen Daten\nHintergrund: Die Herausforderung besteht in der Kombination von zwei extremen Sichtweisen, der Betrachtung der Forschung mit sozialen Daten als “klinische” Forschung oder als Computerforschung\n\nDie Sozialdatenforschung unterscheidet sich von klinischen Versuchen.\nEthische Entscheidungen in der Sozialdatenforschung müssen gut überlegt sein, da oft sind mehrere Werte betroffen, die miteinander in Konflikt stehen können\nDiskussion des Spannungsverhältnisses am Beispiel von drei spezifischer ethischer Kriterien: Autonomie, Wohltätigkeit und Gerechtigkeit\n\n\nHintergrund:\n\nDie Sozialdatenforschung ähnelt klinischen Versuchen und anderen Experimenten am Menschen in ihrer Fähigkeit, Menschen zu schaden, und sollte daher auch als solche reguliert werden\ndie Sozialdatenforschung ähnelt der sonstigen Computerforschung, die sich traditionell auf Methoden, Algorithmen und den Aufbau von Systemen konzentriert, mit minimalen direkten Auswirkungen auf Menschen.\n\nPunkt 2: Schäden, die die üblichen Arten der Sozialdatenforschung ( z. B. die Verletzung der Privatsphäre oder der Anblick verstörender Bilder)verursachen können, oft nicht mit Schäden von klinischen Versuchen gleichzusetzen\nPunkt 3: Datenanalyse beispielsweise erforderlich sein, um wichtige Dienste bereitzustellen, und es sollten Lösungen erwogen werden, die ein Gleichgewicht zwischen Datenschutz und Genauigkeit herstellen (Goroff, 2015)."
  },
  {
    "objectID": "slides/slides-02.html#achtung-der-individuellen-autonomie",
    "href": "slides/slides-02.html#achtung-der-individuellen-autonomie",
    "title": "Überblick & Einführung",
    "section": "Achtung der individuellen Autonomie",
    "text": "Achtung der individuellen Autonomie\nDiskussion der Informierte Zustimmung als Indikator autonomer Entscheidung\n\n\n\n\n\n\nEinwilligung nach Aufklärung setzt voraus, dass\n\n\n\n\ndie Forscher*Innen den potenziellen Teilnehmenden alle relevanten Informationen offenlegen;\ndie potenziellen Teilnehmenden in der Lage sind, diese Informationen zu bewerten;\ndie potenziellen Teilnehmenden freiwillig entscheiden können, ob sie teilnehmen wollen oder nicht;\ndie Teilnehmenden den Forschernden ihre ausdrückliche Erlaubnis erteilen, häufig in schriftlicher Form; und\ndie Teilnehmende die Möglichkeit haben, ihre Einwilligung jederzeit zurückzuziehen.\n\n\n\n\n\nPotentielle Probleme mit Blick auf DBD\n\nDie Zustimmung von Millionen von Nutzern einzuholen ist nicht praktikabel.\nDie Nutzungsbedingungen sozialer Plattformen stellen möglicherweise keine informierte Zustimmung zur Forschung dar.\nDas öffentliche Teilen von Inhalten im Internet bedeutet nicht unbedingt eine Zustimmung zur Forschung."
  },
  {
    "objectID": "slides/slides-02.html#no-no-yes",
    "href": "slides/slides-02.html#no-no-yes",
    "title": "Überblick & Einführung",
    "section": "No “No” ≠ “Yes”!",
    "text": "No “No” ≠ “Yes”!\nEthische Erwägungen bei DBD-Forschung\n\nAus öffentlicher Zugänglich- bzw. Verfügbarkeit von Daten leitet sich nicht automatisch ethische Verwertbarkeit ab (boyd & Crawford, 2012; Zimmer, 2010)\n\nVerletzung der Privatsphäre der Nutzer (Goroff, 2015)\nErmöglichung von rassischem, sozioökonomischem oder geschlechtsspezifischem Profiling (Barocas & Selbst, 2016)\n\nNegative Beispiele\n\nFacebook contagion experiment (2012-2014): Feeds von Nutzer*Innen so manipulierten, dass sie je nach den geäußerten Emotionen mehr oder weniger von bestimmten Inhalten enthielten (Kramer et al., 2014)\nEncore-Forschungsprojekt: Messung der Internetzensur auf der ganzen Welt, bei der Webbrowser angewiesen wurden, zu versuchen, sensible Webinhalte ohne das Wissen oder die Zustimmung der Nutzer herunterzuladen (Burnett & Feamster, 2014)\n\n\n\nHintergrund:\n\nEthische Fragen bisher epistemische Bedenken (Verwendung von nicht schlüssigen oder fehlgeleiteten Beweisen), jetzt normativ Bedenken (Folgen der Forschung)\nForschung grundsätzlich in vielen Ländern gesetztlich geregelt\n\nNegativbeispiele:\n\nFacebook contagion experiment: Das Experiment wurde als ein Eingriff kritisiert, der den emotionalen Zustand von ahnungslosen Nutzern beeinflusste, die keine Zustimmung zur Teilnahme an der Studie gegeben hatten (Hutton und Henderson, 2015a).\nEncore-Forschngsprojekt: Menschen in einigen Ländern durch diese Zugriffsversuche möglicherweise gefährdet wurden\n\nFolgende Abschnitte:\n\nzentrales Spannungsverhältnis in der Forschungsethik digitaler Daten dargestellt.\nAnschließend wird die Diskussion spezifischer ethischer Probleme in der Sozialdatenforschung im Hinblick auf drei grundlegende Kriterien gegliedert, die im Belmont-Bericht (Ryan et al., 1978), einem grundlegenden Werk zur Forschungsethik, vorgebracht wurden: Autonomie (Abschnitt 9.2), Wohltätigkeit (Abschnitt 9.3) und Gerechtigkeit (Abschnitt 9.4)."
  },
  {
    "objectID": "slides/slides-02.html#wohltätigkeit-und-unschädlichkeit-als-ziel",
    "href": "slides/slides-02.html#wohltätigkeit-und-unschädlichkeit-als-ziel",
    "title": "Überblick & Einführung",
    "section": "Wohltätigkeit und Unschädlichkeit als Ziel",
    "text": "Wohltätigkeit und Unschädlichkeit als Ziel\nBewertung von Risken & Nutzen\nHintergrund: Nicht nur Fokus auf den Nutzen der Forschung, sondern auch auf die möglichen Arten von Schäden, die betroffenen Gruppen und die Art und Weise, wie nachteilige Auswirkungen getestet werden können . (Sweeney, 2013)\n\nPotentielle Probleme\n\nDaten über Einzelpersonen können ihnen schaden, wenn sie offengelegt werden.\nForschungsergebnisse können verwendet werden, um Schaden anzurichten.\n“Dual-Use”- und Sekundäranalysen sind in der Sozialdatenforschung immer häufiger anzutreffen.\n\n\nDie Forschung zu sozialen Daten wird mit bestimmten Arten von Schäden in Verbindung gebracht, von denen die Verletzung der Privatsphäre vielleicht die offensichtlichste ist (Zimmer, 2010; Crawford und Finn, 2014).\nBeispiel 1: Einige prominente Beispiele sind die Datenpanne bei Ashley Madison im Jahr 2015, bei der einer Website, die sich als Dating-Netzwerk für betrügerische Ehepartner anpreist, Kontoinformationen (einschließlich der vollständigen Namen der Nutzer) gestohlen und online gestellt wurden (Thomsen, 2015), sowie die jüngsten Datenpannen bei Facebook, bei denen Hunderte Millionen von Datensätzen mit Kommentaren, Likes, Reaktionen, Kontonamen, App-Passwörtern und mehr öffentlich gemacht wurden.\nzu 1: - Stalking, Diskriminierung, Erpressung oder Identitätsdiebstahl (Gross und Acquisti, 2005). - Zu lange Archivierung personenbezogener Daten oder die öffentliche Freigabe schlecht anonymisierter Datensätze kann zu Verletzungen der Privatsphäre führen, da diese Daten mit anderen Quellen kombiniert werden können, um Erkenntnisse über Personen ohne deren Wissen zu gewinnen (Crawford und Finn, 2014; Goroff, 2015; Horvitz und Mulligan, 2015)\nzu 2: Abgesehen von der Tatsache, dass aus sozialen Daten gezogene Rückschlüsse in vielerlei Hinsicht falsch sein können, wie in dieser Studie hervorgehoben wird, können zu präzise Rückschlüsse dazu führen, dass Menschen in immer kleinere Gruppen eingeteilt werden können (Barocas, 2014).\nzu 3: Daten, Instrumente und Schlussfolgerungen, die für einen bestimmten Zweck gewonnen wurden, für einen anderen Zweck verwendet werden (Hovy und Spruit, 2016; Benton et al., 2017)"
  },
  {
    "objectID": "slides/slides-02.html#faire-verteilung-von-risiken-nutzen",
    "href": "slides/slides-02.html#faire-verteilung-von-risiken-nutzen",
    "title": "Überblick & Einführung",
    "section": "Faire Verteilung von Risiken & Nutzen",
    "text": "Faire Verteilung von Risiken & Nutzen\nRecht & Gerechtigkeit\nHintergrund: Häufig wird unterstellt bzw. angenommen, dass es von Anfang an bekannt, wer durch die Forschung belastet und wer von den Ergebnissen profitieren wird.\n\nPotentielle Probleme\n\nDie digitale Kluft kann das Forschungsdesign beeinflussen (z.B. WEIRD Samples)\nAlgorithmen und Forschungsergebnisse können zu Diskriminierung führen.\nForschungsergebnisse sind möglicherweise nicht allgemein zugänglich.\nNicht alle Interessengruppen werden über die Verwendung von Forschungsergebnissen konsultiert.\n\n\nzu 1: Data divide: mangelnde Verfügbarkeit von hochwertigen Daten über Entwicklungsländer und unterprivilegierte Gemeinschaften (Cinnamon und Schuurman, 2013). WEIRD = White, Educated, Industrialized, Rich, and Democratic\nzu 3: Idealerweise sollten die Menschen Zugang zu den Forschungsergebnissen und Artefakten haben, die aus der Untersuchung ihrer persönlichen Daten entstanden sind (Gross und Acquisti, 2005; Crawford und Finn, 2014).\nzu 4: In die Überlegungen darüber, wie, für wen und wann Forschungsergebnisse umgesetzt werden, sollten diejenigen einbezogen werden, die möglicherweise betroffen sind oder deren Daten verwendet werden (Costanza-Chock, 2018; Design Justice, 2018; Green, 2018)"
  },
  {
    "objectID": "slides/slides-02.html#zwei-trends-drei-fragen-vier-empfehlungen",
    "href": "slides/slides-02.html#zwei-trends-drei-fragen-vier-empfehlungen",
    "title": "Überblick & Einführung",
    "section": "Zwei Trends, Drei Fragen, Vier Empfehlungen",
    "text": "Zwei Trends, Drei Fragen, Vier Empfehlungen\nZusammenfassung und Ausblick\nTrend 1: Skepsis gegenüber einfachen Antworten\n\n\nWie einstehen die Daten, was enthalten sie tatsächlich und wie sind die Arbeitsdatensätze zusammengestellt?\nWird deutlich, was ausgewertet wird?\nWird die Verwendung von vorhandenen Datensätzen und Modellen des maschinellen Lernens hinterfragt?\n\n\nTrend 2: Wechsel von der Thematisierung zur Adressieung von Bedenken\n\n\nDetaillierte Dokumentation und kritische Prüfung der Datensatz- und Modellerstellung\nDBD-Studien auf verschiedene Plattformen, Themen, Zeitpunkte und Teilpopulationen auszuweiten, um festzustellen, wie sich die Ergebnisse beispielsweise in verschiedenen kulturellen, demografischen und verhaltensbezogenen Kontexten unterscheiden\nTransparenzmechanismen zu schaffen, die es ermöglichen, Online-Plattformen zu überprüfen und Verzerrungen in Daten an der Quelle zu evaluieren\nForschung zu diesen Leitlinien, Standards, Methoden und Protokollen auszuweiten und ihre Übernahme zu fördern.\n\n\n\nSchließlich gibt es angesichts der Komplexität der inhärent kontextabhängigen, anwendungs- und bereichsabhängigen Verzerrungen und Probleme in sozialen Daten und Analysepipelines, die in diesem Papier behandelt werden, keine Einheitslösungen - bei der Bewertung und Bekämpfung von Verzerrungen ist Nuancierung entscheidend."
  },
  {
    "objectID": "slides/slides-02.html#der-weg-bestimmt-das-ergebnis",
    "href": "slides/slides-02.html#der-weg-bestimmt-das-ergebnis",
    "title": "Überblick & Einführung",
    "section": "Der Weg bestimmt das Ergebnis",
    "text": "Der Weg bestimmt das Ergebnis\nEinfluss der Erhebung auf die Daten(-form) (Davidson et al., 2023)"
  },
  {
    "objectID": "slides/slides-02.html#data-download-packages",
    "href": "slides/slides-02.html#data-download-packages",
    "title": "Überblick & Einführung",
    "section": "Data Download Packages",
    "text": "Data Download Packages\nBeispiel für Data Donations (Driel et al., 2022)\n\n\n\nGeneral Data Protection Regulation erlaubt die von einer Plattform gespeicherten personenbezogenen Daten in einem strukturierten, allgemein gebräuchlichen und maschienenlesbaren Format (Data Download Package) anzufordern.\nNutzer:innen können Forschenden diese Daten spenden, verbunden mit der Möglichkeit, bestimmte Daten (heraus) zu filtern."
  },
  {
    "objectID": "slides/slides-02.html#screenomics-software",
    "href": "slides/slides-02.html#screenomics-software",
    "title": "Überblick & Einführung",
    "section": "Screenomics software",
    "text": "Screenomics software\nBeispiel für Tracking (Reeves et al., 2021)\n\n\n\nErfassung: Alle fünf Sekunden, in denen digitale Geräte aktiviert sind, werden Screenshots erstellt, verschlüssel, komprimiert & an einen Forschungsserver übertragen\nVerarbeitung: Screenomics (App) erkennt und segmentiert Text, Gesichter, Logos und Objekte auf den Screenshots"
  },
  {
    "objectID": "slides/slides-02.html#zeeschuimer-plugin",
    "href": "slides/slides-02.html#zeeschuimer-plugin",
    "title": "Überblick & Einführung",
    "section": "Zeeschuimer Plugin",
    "text": "Zeeschuimer Plugin\nBeispiel für Scraping (Peeters, 2022)\n\n\n\nBrowsererweiterung, die während des Besuchs einer Social-Media-Website Daten über die Elemente sammelt, die in der Weboberfläche einer Plattform zu sehen sind\nDerzeit werden die unter anderem , ,  &  unterstützt\nErgänzung zu 4CAT (Peeters & Hagen, 2022), einem Tool zur Analyse und Verarbeitung von Daten aus Online-Plattformen"
  },
  {
    "objectID": "slides/slides-02.html#and-now-you",
    "href": "slides/slides-02.html#and-now-you",
    "title": "Überblick & Einführung",
    "section": "And now … you!",
    "text": "And now … you!\nGruppenarbeit (ca. 15 Minuten) mit kurzer Ergebnisvorstellung (ca. 15 Min)\n\n\n\n\n\n\nArbeitsauftrag\n\n\nStellt euch vor, Ihr wollt eine der drei vorgestellten Methoden nutzen, um ein Forschungsprojekt durchzuführen.\n\nWas sind mögliche Biases an der Quelle der Daten, die ihr bei der Methode berücksichtigen müsst?\nWelche ethischen und rechtlichen Fragen ergeben sich aus der Nutzung der Methode?\n\n\n\n\n\n\n\n\n\n\nNächste Schritte\n\n\n\nEs gibt für jede Methode (Data Donation, Tracking & Scraping) eine Gruppe. Ihr könnt selber aussuchen, in welche Gruppe ihr möchtet.\nSchreibt eure Ergebnisse in die dafür bereitgestellet Folienvorlage (auf der nächsten Slide)."
  },
  {
    "objectID": "slides/slides-02.html#please-discuss",
    "href": "slides/slides-02.html#please-discuss",
    "title": "Überblick & Einführung",
    "section": "Please discuss!",
    "text": "Please discuss!\nBitte nutzt die jeweilige Folienvorlage für die Dokumentation euerer Ergebnisse\n\n\n\n\n\n     Data Donations\n\n\n\n\n\n     Tracking\n\n\n\n\n\n     Scraping\n\n\n\n\n−+\n15:00"
  },
  {
    "objectID": "slides/slides-02.html#literatur",
    "href": "slides/slides-02.html#literatur",
    "title": "Überblick & Einführung",
    "section": "Literatur",
    "text": "Literatur\n\n\nBaeza-Yates, R. A. (2013). Big data or right data.\n\n\nBarocas, S., & Selbst, A. D. (2016). Big Data’s Disparate Impact. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2477899\n\n\nboyd, danah m., & Crawford, K. (2012). Critical questions for big data: Provocations for a cultural, technological, and scholarly phenomenon. Information, Communication & Society, 15(5), 662–679. https://doi.org/10.1080/1369118X.2012.678878\n\n\nBurnett, S., & Feamster, N. (2014). Encore: Lightweight measurement of web censorship with cross-origin requests. https://doi.org/10.48550/ARXIV.1410.1211\n\n\nCroskerry, P. (2002). Achieving Quality in Clinical Decision Making: Cognitive Strategies and Detection of Bias. Academic Emergency Medicine, 9(11), 1184–1204. https://doi.org/10.1197/aemj.9.11.1184\n\n\nDavidson, B. I., Wischerath, D., Racek, D., Parry, D. A., Godwin, E., Hinds, J., Van Der Linden, D., Roscoe, J. F., Ayravainen, L. E. M., & Cork, A. (2023). Platform-controlled social media APIs threaten open science. https://osf.io/ps32z\n\n\nDriel, I. I. van, Giachanou, A., Pouwels, J. L., Boeschoten, L., Beyens, I., & Valkenburg, P. M. (2022). Promises and Pitfalls of Social Media Data Donations. Communication Methods and Measures, 1–17. https://doi.org/10.1080/19312458.2022.2109608\n\n\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Digital trace data (1st ed., pp. 100–118). Routledge. https://www.taylorfrancis.com/books/9781003024583/chapters/10.4324/9781003024583-8\n\n\nFlöck, F., & Sen, I. (2022). Digital traces of human behaviour in online platforms  research design and error sources. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meet_the_experts_Digitaltraces_humanbehaviour.pdf\n\n\nFriedman, B., & Nissenbaum, H. (1996). Bias in computer systems. ACM Transactions on Information Systems, 14(3), 330–347. https://doi.org/10.1145/230538.230561\n\n\nGoroff, D. L. (2015). Balancing privacy versus accuracy in research protocols. Science, 347(6221), 479–480. https://doi.org/10.1126/science.aaa3483\n\n\nKramer, A. D. I., Guillory, J. E., & Hancock, J. T. (2014). Experimental evidence of massive-scale emotional contagion through social networks. Proceedings of the National Academy of Sciences, 111(24), 8788–8790. https://doi.org/10.1073/pnas.1320040111\n\n\nNaveed, N., Gottron, T., Kunegis, J., & Alhadi, A. C. (2011). the 20th ACM international conference. 183. https://doi.org/10.1145/2063576.2063607\n\n\nOlteanu, A., Castillo, C., Diaz, F., & Kıcıman, E. (2019). Social data: Biases, methodological pitfalls, and ethical boundaries. Frontiers in Big Data, 2, 13. https://doi.org/10.3389/fdata.2019.00013\n\n\nPeeters, S. (2022). Zeeschuimer. Zenodo. https://zenodo.org/record/6826877\n\n\nPeeters, S., & Hagen, S. (2022). The 4CAT Capture and Analysis Toolkit: A Modular Tool for Transparent and Traceable Social Media Research. Computational Communication Research, 4(2), 571–589. https://doi.org/10.5117/ccr2022.2.007.hage\n\n\nReeves, B., Ram, N., Robinson, T. N., Cummings, J. J., Giles, C. L., Pan, J., Chiatti, A., Cho, M., Roehrick, K., Yang, X., Gagneja, A., Brinberg, M., Muise, D., Lu, Y., Luo, M., Fitzgerald, A., & Yeykelis, L. (2021). Screenomics : A Framework to Capture and Analyze Personal Life Experiences and the Ways that Technology Shapes Them. HumanComputer Interaction, 36(2), 150–201. https://doi.org/10.1080/07370024.2019.1578652\n\n\nSalganik, M. J. (2018). Bit by bit: Social research in the digital age. Princeton University Press.\n\n\nSweeney, L. (2013). Discrimination in Online Ad Delivery: Google ads, black names and white names, racial discrimination, and click advertising. Queue, 11(3), 10–29. https://doi.org/10.1145/2460276.2460278\n\n\nUlloa, R. (2021). Introduction to online data acquisition. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meettheexperts_Introductioncss.pdf\n\n\nWeller, K. (2021). A short introduction to computational social science and digital behavioral data. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meettheexperts_Introductioncss.pdf\n\n\nZimmer, M. (2010). “But the data is already public”: on the ethics of research in Facebook. Ethics and Information Technology, 12(4), 313–325. https://doi.org/10.1007/s10676-010-9227-5"
  },
  {
    "objectID": "slides/slides-10.html#seminarplan",
    "href": "slides/slides-10.html#seminarplan",
    "title": "🔨 Sentiment Analysis",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\nSession\nDatum\nTopic\nPresenter\n\n\n\n\n\n📂 Block 1\nIntroduction\n\n\n\n1\n23.10.2024\nKick-Off\nChristoph Adrian\n\n\n2\n30.10.2024\nDBD: Overview & Introduction\nChristoph Adrian\n\n\n3\n06.11.2024\n🔨 Introduction to working with R\nChristoph Adrian\n\n\n\n📂 Block 2\nTheoretical Background: Twitch & TV Election Debates\n\n\n\n4\n13.11.2024\n📚 Twitch-Nutzung im Fokus\nStudent groups\n\n\n5\n20.11.2024\n📚 (Wirkungs-)Effekte von Twitch & TV-Debatten\nStudent groups\n\n\n6\n27.11.2024\n📚 Politische Debatten & Social Media\nStudent groups\n\n\n\n📂 Block 3\nMethod: Natural Language Processing\n\n\n\n7\n04.12.2024\n🔨 Text as data I: Introduction\nChristoph Adrian\n\n\n8\n11.12.2024\n🔨 Text as data II: Advanced Methods\nChristoph Adrian\n\n\n9\n18.12.2024\n🔨 Advanced Method I: Topic Modeling\nChristoph Adrian\n\n\n\nNo lecture\n🎄Christmas Break\n\n\n\n10\n08.01.2025\n🔨 Advanced Method II: Machine Learning\nChristoph Adrian\n\n\n\n📂 Block 4\nProject Work\n\n\n\n11\n15.01.2025\n🔨 Project work\nStudent groups\n\n\n12\n22.01.2025\n🔨 Project work\nStudent groups\n\n\n13\n29.01.2025\n📊 Project Presentation I\nStudent groups (TBD)\n\n\n14\n05.02.2025\n📊 Project Presentation & 🏁 Evaluation\nStudentds (TBD) & Christoph Adrian"
  },
  {
    "objectID": "slides/slides-10.html#kurze-rekapitulation",
    "href": "slides/slides-10.html#kurze-rekapitulation",
    "title": "🔨 Sentiment Analysis",
    "section": "Kurze Rekapitulation",
    "text": "Kurze Rekapitulation\nGrundidee & verschiedene Umsetzungsmöglichkeiten einer Sentimentanalyse\n\nAnwendung von Natural Language Processing (NLP), Textanalyse und Computational Linguistics, um subjektive Informationen aus Texten zu extrahieren bzw. Meinung, Einstellung oder Emotionen zu bestimmten Themen oder Entitäten zu bestimmen\nVerschiede Methoden:\n\nRegelbasierte Ansätze (Dictionaries)\nMaschinelles Lernen & Deep Learning\nLLMs (& KI)"
  },
  {
    "objectID": "slides/slides-10.html#ein-spektrum-an-möglichkeiten",
    "href": "slides/slides-10.html#ein-spektrum-an-möglichkeiten",
    "title": "🔨 Sentiment Analysis",
    "section": "Ein Spektrum an Möglichkeiten",
    "text": "Ein Spektrum an Möglichkeiten\nBeispiele für verschiedene Dictionaries & Pakete zur Umsetzung einer Sentimentanalyse\n\n\nDefinition eines eigenen, “organischen” Dictionaries vs. Off-the-shelf, wie z.B.\n\nLexicoder Sentiment Dictionary (Young & Soroka, 2012) ➜ Das Wörterbuch besteht aus 2.858 “negativen” und 1.709 “positiven” Sentiment-Wörtern sowie 2.860 und 1.721 Negationen von negativen bzw. positiven Wörtern.\nAFINN (Nielsen, 2011) ➜ Bewertung von Wörtern mit Sentiment-Werten von -5 (negativ) bis +5 (positiv)\nValence Aware Dictionary and sEntiment Reasoner (Hutto & Gilbert, 2014) ➜ Sentiment-Tool, dass zusätzlich den Kontext der Wörter mit berücksichtigt und einen Score zwischen -1 (negativ) und +1 (positiv) berechnet\n\nPraktische Umsetzung mit quanteda (v4.1.0, Benoit et al., 2018) bzw. quanteda.sentiment [v0.31] oder vader [v0.2.1, Roehrick (n.d.)]"
  },
  {
    "objectID": "slides/slides-10.html#erweiterung-des-quantedaverse",
    "href": "slides/slides-10.html#erweiterung-des-quantedaverse",
    "title": "🔨 Sentiment Analysis",
    "section": "Erweiterung des quantedaverse",
    "text": "Erweiterung des quantedaverse\nVorstellung von quanteda.sentiment bzw. enthaltenen Funktionen & Diktionäre\n\nquanteda.sentiment erweitert das quanteda Paket um Funktionen zur Berechnung von Sentiment in Texten. Es bietet zwei Hauptfunktionen:\n\ntextstat_polarity() ➜ Sentiment basierend auf positiven und negativen Wörtern (z.B. mit Lexicoder Sentiment Dictionary).\nBeispiel in Bezug auf politische Diskussionen: „War der Ton der Diskussion positiv oder negativ?“\ntextstat_valence() ➜ Sentiment als Durchschnitt der Valenzwerte der Wörter in einem Dokument (z.B. AFINN).\nBeispiel in Bezug auf politische Diskussionen: „Wie intensiv haben die Teilnehmer:innen ihre Emotionen ausgedrückt?“"
  },
  {
    "objectID": "slides/slides-10.html#unterschied-zwischen-polarität-valenz",
    "href": "slides/slides-10.html#unterschied-zwischen-polarität-valenz",
    "title": "🔨 Sentiment Analysis",
    "section": "Unterschied zwischen Polarität & Valenz",
    "text": "Unterschied zwischen Polarität & Valenz\nPraktische Anwedung von quanteda.sentiment\n\n\n\nchats_polarity &lt;- corp_chats %&gt;% \n  textstat_polarity(\n    dictionary = data_dictionary_LSD2015) %&gt;% \n  rename(polarity = sentiment)\n\n\nchats_polarity %&gt;% \n    head(n = 10)\n\n                                 doc_id  polarity\n1  dc03b89a-722d-4eaa-a895-736533a68aca  0.000000\n2  6be50e12-2fd5-436f-b253-b2358b618380  0.000000\n3  f5e41904-7f01-4f03-ad6c-2c0f07d70ed0  1.098612\n4  92dc6519-eb54-4c18-abef-27201314b22f -1.098612\n5  92055088-7067-48c0-aa11-9c6103bdf4c4  0.000000\n6  03ad4706-aa67-4ddc-a1e4-6f8ca981778e  0.000000\n7  00c5dd9c-41b8-4430-8b2e-be67c5e363ac  0.000000\n8  923c7eac-d92e-4cac-876a-07d4fa45cb57  0.000000\n9  6bdfb03d-fdbd-48b6-9b81-2fc56785fd67 -1.098612\n10 7f25fc9f-b000-41fa-91b8-60672cd3e608  0.000000\n\n\n\n\nchats_valence &lt;- corp_chats %&gt;% \n  textstat_valence(\n    dictionary = data_dictionary_AFINN) %&gt;% \n  rename(valence = sentiment)\n\n\nchats_valence %&gt;% \n    head(n = 10)\n\n                                 doc_id valence\n1  dc03b89a-722d-4eaa-a895-736533a68aca       0\n2  6be50e12-2fd5-436f-b253-b2358b618380       0\n3  f5e41904-7f01-4f03-ad6c-2c0f07d70ed0       0\n4  92dc6519-eb54-4c18-abef-27201314b22f      -5\n5  92055088-7067-48c0-aa11-9c6103bdf4c4       0\n6  03ad4706-aa67-4ddc-a1e4-6f8ca981778e       0\n7  00c5dd9c-41b8-4430-8b2e-be67c5e363ac       0\n8  923c7eac-d92e-4cac-876a-07d4fa45cb57       0\n9  6bdfb03d-fdbd-48b6-9b81-2fc56785fd67       0\n10 7f25fc9f-b000-41fa-91b8-60672cd3e608       0"
  },
  {
    "objectID": "slides/slides-10.html#was-vader-anders-macht",
    "href": "slides/slides-10.html#was-vader-anders-macht",
    "title": "🔨 Sentiment Analysis",
    "section": "Was VADER anders macht",
    "text": "Was VADER anders macht\nHintergrundinformationen zu VADER (Hutto & Gilbert, 2014)\n\nBerücksichtigt Valenzverschiebungen mit Kontextbewusstsein\n\nNegationen (z.B. „nicht gut“ ist weniger positiv als „gut“).\nIntensitätsmodifikatoren (z.B. „sehr gut“ ist positiver als „gut“).\nKontrastierende Konjunktionen (z.B. „aber“ signalisiert einen Stimmungswechsel: „gut, aber nicht großartig“).\n\nBerücksichtigt Interpunktion (z.B. „Erstaunlich!!!“ ist positiver als „Erstaunlich“) und Großschreibung (z.B. „ERSTAUNLICH“ hat ein stärkeres Sentiment als „erstaunlich“)\nHandhabt Slang, Emojis und internet-spezifische Sprache (z.B. „LOL“, „:)“, oder „omg“)"
  },
  {
    "objectID": "slides/slides-10.html#mehr-als-nur-ein-score",
    "href": "slides/slides-10.html#mehr-als-nur-ein-score",
    "title": "🔨 Sentiment Analysis",
    "section": "Mehr als nur ein Score",
    "text": "Mehr als nur ein Score\nVorstellung der Funktion vader [v0.2.1, Roehrick (n.d.)] inklusive Output\n\nget_vader()➜ Exportiert folgende Metriken:\n\nWortbezogene Sentiment-Scores: Jedes Wort erhält einen Sentiment-Score, der basierend auf Faktoren wie Interpunktion und Großschreibung angepasst wird.\nGesamtwert (Compound score): Ein einzelner Wert, der das Gesamtsentiment des gesamten Satzes zusammenfasst.\nPositive (pos), neutrale (neu) und negative (neg) Scores: Repräsentieren den Prozentsatz der Wörter, die in jede Sentiment-Kategorie fallen.\nBut count: Zählt das Vorkommen des Wortes „aber“, was auf mögliche Stimmungswechsel innerhalb des Satzes hinweist."
  },
  {
    "objectID": "slides/slides-10.html#erstellung-transformation-des-vader-outputs",
    "href": "slides/slides-10.html#erstellung-transformation-des-vader-outputs",
    "title": "🔨 Sentiment Analysis",
    "section": "Erstellung & Transformation des VADER Outputs",
    "text": "Erstellung & Transformation des VADER Outputs\nPraktische Anwedung von vader\n\nchats_vader &lt;- chats %&gt;% \n  mutate(\n    # Estimate sentiment scores\n    vader_output = map(dialogue, ~vader::get_vader(.x)), \n    # Extract word-level scores\n    word_scores = map(vader_output, ~ .x[\n        names(.x) != \"compound\" &\n        names(.x) != \"pos\" & \n        names(.x) != \"neu\" & \n        names(.x) != \"neg\" & \n        names(.x) != \"but_count\"]),  \n    compound = map_dbl(vader_output, ~ as.numeric(.x[\"compound\"])),\n    pos = map_dbl(vader_output, ~ as.numeric(.x[\"pos\"])),\n    neu = map_dbl(vader_output, ~ as.numeric(.x[\"neu\"])),\n    neg = map_dbl(vader_output, ~ as.numeric(.x[\"neg\"])),\n    but_count = map_dbl(vader_output, ~ as.numeric(.x[\"but_count\"]))\n  )"
  },
  {
    "objectID": "slides/slides-10.html#ein-blick-auf-das-ergebnis",
    "href": "slides/slides-10.html#ein-blick-auf-das-ergebnis",
    "title": "🔨 Sentiment Analysis",
    "section": "Ein Blick auf das Ergebnis",
    "text": "Ein Blick auf das Ergebnis\nPraktische Anwedung von vader\n\nchats_vader %&gt;% \n    select(message_id, compound:but_count) %&gt;% \n    head(n = 20)\n\n# A tibble: 20 × 6\n   message_id                           compound   pos   neu   neg but_count\n   &lt;chr&gt;                                   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 dc03b89a-722d-4eaa-a895-736533a68aca    0         0 1     0             0\n 2 6be50e12-2fd5-436f-b253-b2358b618380    0         0 1     0             0\n 3 f5e41904-7f01-4f03-ad6c-2c0f07d70ed0    0         0 1     0             0\n 4 92dc6519-eb54-4c18-abef-27201314b22f   -0.586     0 0.513 0.487         0\n 5 92055088-7067-48c0-aa11-9c6103bdf4c4    0         0 1     0             0\n 6 03ad4706-aa67-4ddc-a1e4-6f8ca981778e    0         0 1     0             0\n 7 00c5dd9c-41b8-4430-8b2e-be67c5e363ac    0         0 1     0             0\n 8 923c7eac-d92e-4cac-876a-07d4fa45cb57    0         0 1     0             0\n 9 6bdfb03d-fdbd-48b6-9b81-2fc56785fd67    0         0 1     0             0\n10 7f25fc9f-b000-41fa-91b8-60672cd3e608    0         0 1     0             0\n11 a00e2ca8-2e76-4941-b360-b6b311701cba    0         0 1     0             0\n12 637e5e96-9f26-4a87-955e-74f2fb29685a    0         0 1     0             0\n13 4b0a6fbe-54d6-4d06-8d08-875112abcd92    0         0 1     0             0\n14 cf57874e-a239-4bce-a766-4bb7636847b7    0         0 1     0             0\n15 51b66d60-0f6b-43a6-a40c-cb6d51cde1a9    0         0 1     0             0\n16 08d7ae3c-1180-4e26-940e-de763fbe6f18    0         0 1     0             0\n17 72494412-fe24-44ad-9a02-de22e8e54724    0         0 1     0             0\n18 93a9da3e-63ab-4eea-bb51-73bff8dadf13    0         0 1     0             0\n19 3aa667c1-a8b1-4f18-94a6-920b8a9ee37b    0         0 1     0             0\n20 daebee85-4885-48f2-8086-9b9172285792   -0.586     0 0.513 0.487         0"
  },
  {
    "objectID": "slides/slides-10.html#kombinieren-vergleichen",
    "href": "slides/slides-10.html#kombinieren-vergleichen",
    "title": "🔨 Sentiment Analysis",
    "section": "Kombinieren & vergleichen",
    "text": "Kombinieren & vergleichen\nZusammenführung der einzelnen Dictionary-Sentiments mit den Stammdaten\n\nchats_sentiment &lt;- chats %&gt;% \n    left_join(chats_polarity, by = join_by(\"message_id\" == \"doc_id\")) %&gt;%\n    left_join(chats_valence, by = join_by(\"message_id\" == \"doc_id\")) %&gt;% \n    left_join(chats_vader %&gt;% \n        select(message_id, vader_output, word_scores, compound, pos, neu, neg, but_count), \n        by = \"message_id\")\n\n\nchats_sentiment %&gt;% \n    select(message_id, polarity, valence, compound) %&gt;%\n    datawizard::describe_distribution()\n\nVariable |  Mean |   SD | IQR |         Range | Skewness | Kurtosis |      n | n_Missing\n----------------------------------------------------------------------------------------\npolarity | -0.06 | 0.65 |   0 | [-4.44, 3.61] |    -0.18 |     1.30 | 913375 |         0\nvalence  | -0.04 | 1.38 |   0 | [-5.00, 5.00] |    -0.21 |     2.54 | 913375 |         0\ncompound |  0.01 | 0.30 |   0 | [-1.00, 1.00] |    -0.12 |     1.11 | 913170 |       205"
  },
  {
    "objectID": "slides/slides-10.html#neutralität-dominiert",
    "href": "slides/slides-10.html#neutralität-dominiert",
    "title": "🔨 Sentiment Analysis",
    "section": "Neutralität dominiert",
    "text": "Neutralität dominiert\nVergleich der Verteilungsfunktionen der verschiedenen Sentiments\n\n\nExpand for full code\nchats_sentiment %&gt;%\n    pivot_longer(cols = c(polarity, valence, compound), names_to = \"sentiment_type\", values_to = \"sentiment_value\") %&gt;%\n    ggplot(aes(x = sentiment_value, fill = sentiment_type)) +\n    geom_density(alpha = 0.5) +\n    facet_wrap(~ sentiment_type, scales = \"free\") +\n    labs(\n        title = \"Density Plot of Polarity, Valence, and Compound Sentiment\"\n    ) +\n    theme_pubr() +\n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "slides/slides-10.html#sentiment-scores-einer-nachricht",
    "href": "slides/slides-10.html#sentiment-scores-einer-nachricht",
    "title": "🔨 Sentiment Analysis",
    "section": "Sentiment Scores einer Nachricht",
    "text": "Sentiment Scores einer Nachricht\nPraktische Anwendung des compound scores\n\n\nExpand for full code\nchats_vader_sample &lt;- chats_vader %&gt;%\n    filter(message_length &lt; 100) %&gt;%\n    slice_sample(n = 10) \n\nchats_vader_sample %&gt;%\n    ggplot(aes(x = message_content, y = compound, fill = compound &gt; 0)) +\n        geom_bar(stat = \"identity\", width = 0.7) +\n        scale_fill_manual(values = c(\"TRUE\" = \"blue\", \"FALSE\" = \"red\"), labels = c(\"Positive\", \"Negative\")) +\n        labs(\n            title = \"Overall Compound Sentiment for Each Sentence\",\n            x = \"Sentences\",\n            y = \"Compound Sentiment\",\n            fill = \"Sentiment\") +\n        coord_flip() +  # Flip for easier readability\n        theme_minimal() +\n        theme(\n            axis.text.x = element_text(angle = 45, hjust = 1))  # Label wrapping and adjusting angle"
  },
  {
    "objectID": "slides/slides-10.html#anteil-an-positiven-neutralen-und-negativen-wörtern",
    "href": "slides/slides-10.html#anteil-an-positiven-neutralen-und-negativen-wörtern",
    "title": "🔨 Sentiment Analysis",
    "section": "Anteil an positiven, neutralen und negativen Wörtern",
    "text": "Anteil an positiven, neutralen und negativen Wörtern\nPraktische Anwendung der Word-Level Scores\n\n\nExpand for full code\nchats_vader_sample %&gt;% \n    mutate(\n        pos_pct = pos * 100,\n        neu_pct = neu * 100,\n        neg_pct = neg * 100) %&gt;% \n  select(message_content, pos_pct, neu_pct, neg_pct) %&gt;% \n  pivot_longer(\n    cols = c(pos_pct, neu_pct, neg_pct),\n    names_to = \"sentiment\",\n    values_to = \"percentage\") %&gt;% \n  mutate(\n    sentiment = factor(\n        sentiment,\n        levels = c(\"pos_pct\", \"neu_pct\", \"neg_pct\"),\n        labels = c(\"Positive\", \"Neutral\", \"Negative\"))) %&gt;% \n  ggplot(aes(x = message_content, y = percentage, fill = sentiment)) +\n    geom_bar(stat = \"identity\", width = 0.7) +\n    scale_fill_manual(values = c(\"Positive\" = \"blue\", \"Neutral\" = \"gray\", \"Negative\" = \"red\")) +\n    labs(\n        title = \"Proportion of Positive, Neutral, and Negative Sentiment\",\n        x = \"Sentences\",\n        y = \"Percentage\",\n        fill = \"Sentiment\") +\n  coord_flip() +\n  theme_minimal()"
  },
  {
    "objectID": "slides/slides-10.html#the-power-of-machines",
    "href": "slides/slides-10.html#the-power-of-machines",
    "title": "🔨 Sentiment Analysis",
    "section": "The power of machines",
    "text": "The power of machines\nAlternativen zu Dictionary-sbasierten Ansätzen\n\n“Traditionelles” Machine Learning (ML), z.B. durch Feature extraction (z.B. TF-IDF) und Modellierung (z.B. Naive Bayes, Random Forest, SVM, etc.)\nDeep Learning, z.B. durch die Nutzung von Wort-Embeddings (z.B. Word2Vec, GloVe) oder kontextuellen Embeddings (z.B. BERT) und Verwendung von neuronalen Netzwerken wie LSTMs, GRUs oder CNNs.\nLarge Language Models (LLMs), z.B. vortrainierte LLMs (z.B. GPT, BERT), die für Sentiment-Aufgaben feinabgestimmt sind und Kontext und Nuancen besser verstehen als traditionelle Ansätze."
  },
  {
    "objectID": "slides/slides-10.html#the-way-to-use-ml-in-r",
    "href": "slides/slides-10.html#the-way-to-use-ml-in-r",
    "title": "🔨 Sentiment Analysis",
    "section": "The way to use ML in R",
    "text": "The way to use ML in R\nHintergrundinformationen zu Tidymodels\n\n\nKernfunktionen und Merkmale\n\nVorverarbeitung: Umgang mit fehlenden Daten, Transformationen und Feature-Engineering.\nModellierung: Vereinfachte Modellspezifikation, -training und -abstimmung.\nBewertung: Bewertung der Modellleistung mit Metriken, Resampling und Visualisierungen.\nWorkflow: Integration von Vorverarbeitung, Modellierung und Bewertung in reproduzierbare Pipelines.\nKompatibilität: Unterstützt eine Vielzahl von Modellen (z.B. Regression, Klassifikation, Clustering)."
  },
  {
    "objectID": "slides/slides-10.html#ml-the-tidy-way",
    "href": "slides/slides-10.html#ml-the-tidy-way",
    "title": "🔨 Sentiment Analysis",
    "section": "ML the tidy way",
    "text": "ML the tidy way\nWas macht das tidymodels Paket besonders?\n\nIntegration mit Tidyverse: Datenorientiertes Design mit menschenlesbarer Syntax.\nModulares Ökosystem: Spezialisierte Pakete (z.B. recipes, parsnip, rsample, tune), die nahtlos zusammenarbeiten.\nReproduzierbarkeit und Transparenz: Explizite Workflows und Abstimmungsstrategien.\nUmfassende Toolbox: Kreuzvalidierung, Bootstrapping und erweiterte Diagnosen.\nBenutzerfreundlich: Intuitiv für R-Nutzer, Balance zwischen Benutzerfreundlichkeit und fortgeschrittener Funktionalität."
  },
  {
    "objectID": "slides/slides-10.html#viele-vorteile-ein-zentrales-problem",
    "href": "slides/slides-10.html#viele-vorteile-ein-zentrales-problem",
    "title": "🔨 Sentiment Analysis",
    "section": "Viele Vorteile, ein zentrales Problem",
    "text": "Viele Vorteile, ein zentrales Problem\nWarum im Seminar kein Fokus auf supervised ML gelegt wird\n\nZeit: sorgfältige Vorverarbeitung, (Re-)Modellierung und Fine-Tuning\nKomplexität: tiefes Verständnis von Algorithmen und Modellierungstechniken erforderlich.\nFehlende Daten: besonders “supervised” ML benötigt große, saubere und gut annotierte Datensätze\n\nAber:\n\nCode für Umsetzung im Tutorial zur Sitzung enhalten\nDie Umsetzung orientiert sich an einem Blogeintrag (inklusive Screencast) von Julia Silge , der mehr Hintergrundinformationen enthält"
  },
  {
    "objectID": "slides/slides-10.html#the-not-so-distant-future",
    "href": "slides/slides-10.html#the-not-so-distant-future",
    "title": "🔨 Sentiment Analysis",
    "section": "The (not so distant) future",
    "text": "The (not so distant) future\nNutzung lokaler LLMs mit Ollama\n\n\n\n\n\nopen-source project that serves as a powerful and user-friendly platform for running LLMs on your local machine.\nbridge between the complexities of LLM technology and the desire for an accessible and customizable AI experience.\nprovides access to a diverse and continuously expanding library of pre-trained LLM models (e.g.Llama 3, Phi 3, Mistral, Gemma 2)"
  },
  {
    "objectID": "slides/slides-10.html#r-wrapper-für-llm-apis",
    "href": "slides/slides-10.html#r-wrapper-für-llm-apis",
    "title": "🔨 Sentiment Analysis",
    "section": "R-Wrapper für LLM APIs",
    "text": "R-Wrapper für LLM APIs\nVorstellung von Pakten für die Nutzung (lokaler) LLMs in R\n\n\n\n\n\nthe goal of rollama is to wrap the Ollama API, which allows you to run different LLMs locally and create an experience similar to ChatGPT/OpenAI’s API.\n\n\n\n\n\n\n\nellmer makes it easy to use large language models (LLM) from R. It supports a wide variety of LLM providers and implements a rich set of features including streaming outputs, tool/function calling, structured data extraction, and more."
  },
  {
    "objectID": "slides/slides-10.html#chat-mit-llms-in-r",
    "href": "slides/slides-10.html#chat-mit-llms-in-r",
    "title": "🔨 Sentiment Analysis",
    "section": "Chat mit LLMs in R",
    "text": "Chat mit LLMs in R\nPraktische Anwendung von ellmer [v0.0.0.9000, Wickham & Cheng (2024)]\n\n\n\n\nellmer_chat_llama &lt;- ellmer::chat_ollama(\n    model = \"llama3.2\"\n)\n\nellmer_chat_llama$chat(\"Why is the sky blue?\")\n\nThe sky appears blue because of a phenomenon called scattering, which occurs \nwhen sunlight interacts with the tiny molecules of gases in the Earth's \natmosphere.\n\nHere's a simplified explanation:\n\n1. Sunlight consists of all colors of the visible spectrum, including red, \norange, yellow, green, blue, indigo, and violet.\n2. When sunlight enters the Earth's atmosphere, it encounters tiny molecules of\ngases such as nitrogen (N2) and oxygen (O2).\n3. The shorter wavelengths of light, like blue and violet, are scattered more \nthan the longer wavelengths, like red and orange. This is known as Rayleigh \nscattering.\n4. As a result of scattering, the blue light is distributed in all directions, \nreaching our eyes from every part of the sky.\n\nThe reason why the sky appears blue on a clear day is because:\n\n* The blue light scatters in all directions, making it visible to our eyes from\nevery angle.\n* The longer wavelengths of light (like red and orange) continue to travel in \nstraight lines, allowing us to see them as the sun's rays pass through the \natmosphere.\n\nThe color we perceive is a combination of the scattered blue light and any \ndirect sunlight that reaches our eyes. On cloudy days or during sunrise/sunset \nwhen there is more scattering of shorter wavelengths, the sky can appear more \nhazy, red, or orange.\n\nKeep in mind that the exact shade of blue and other atmospheric effects can \nvary depending on factors such as:\n\n* The time of day (sun rise, peak noon, sunset)\n* Atmospheric conditions (humidity, pollution levels, weather patterns)\n* Altitude and latitude\n\nBut for most of us, the classic clear-sky blue will always be our sky's \nsignature hue!\n\n\n\n\nellmer_chat_mistral &lt;- ellmer::chat_ollama(\n    model = \"mistral\"\n)\n\nellmer_chat_mistral$chat(\"Why is the sky blue?\")\n\n The appearance of a blue sky is due to a process called Rayleigh scattering. \nIn simple terms, this is scattering of light by particles in the atmosphere \nthat are much smaller than the wavelength of light (molecules and small air \nmolecules). Blue light is scattered more because it travels in shorter, smaller\nwaves, and has more energy to spare for scattering events. This scattering of \nblue light is what we perceive as a blue sky during a clear day. If you \nremember the mnemonic \"ROYGBIV\" (Red, Orange, Yellow, Green, Blue, Indigo, \nViolet), blue occupies the part of the visible spectrum that scatters most in \nthe atmosphere, so this explains why we don't usually see red or orange hues in\nthe sky. However, as the sun sets and moves towards the horizon, the light has \nto pass through more atmosphere, which means more of shorter wavelengths like \nblue are scattered away. What is left is predominantly red and yellow light, \nwhich results in the colorful sunsets we often witness."
  },
  {
    "objectID": "slides/slides-10.html#chat-mit-llms-in-r-1",
    "href": "slides/slides-10.html#chat-mit-llms-in-r-1",
    "title": "🔨 Sentiment Analysis",
    "section": "Chat mit LLMs in R",
    "text": "Chat mit LLMs in R\nPraktische Anwendung von rollama [v0.2.0, Gruber & Weber (2024)]\n\n\n\n\ndemo_2_llama3_2 &lt;- rollama::query(\n     \"What is the longest five letter word in english?\",\n    model = \"llama3.2\",\n    screen = FALSE,\n    output = \"text\"\n)\n\nglue::glue(demo_2_llama3_2)\n\nThe longest five-letter word in English is \"house\".\n\n\n\n\ndemo_2_mistral &lt;- rollama::query(\n    \"What is the longest five letter word in english?\",\n    model = \"mistral\",\n    screen = FALSE,\n    output = \"text\"\n)\n\nglue::glue(demo_2_mistral)\n\n The longest five-letter word in English that is considered a single word (without a hyphen) is \"smile\" when you consider the past tense form \"smiled\". If we stick to common, everyday words, then there are no five-letter words longer than \"movie\". Words like \"abstemsia,\" \"floccinaucinihilipilification,\" and \"derterminable\" have more letters but consist of multiple words combined."
  },
  {
    "objectID": "slides/slides-10.html#vorsicht-bei-der-auswahl-eines-modells",
    "href": "slides/slides-10.html#vorsicht-bei-der-auswahl-eines-modells",
    "title": "🔨 Sentiment Analysis",
    "section": "Vorsicht bei der Auswahl eines Modells",
    "text": "Vorsicht bei der Auswahl eines Modells\nModelle unterscheiden sich in ihrer Komplexität & Performance\n\n\n\n\ndemo_3_llama3_2 &lt;- rollama::query(\n    \"Is 9677 a prime number?\",\n    model = \"llama3.2\",\n    screen = FALSE,\n    output = \"text\"\n)\n\nglue::glue(demo_3_llama3_2)\n\nTo determine if 9677 is a prime number, I'll need to check its divisibility by other numbers.\n\nAfter checking, I found that 9677 can be divided by 17 and 569, which means it's not a prime number. Therefore, the answer is no, 9677 is not a prime number.\n\n\n\n\ndemo_3_mistral &lt;- rollama::query(\n    \"Is 9677 a prime number?\",\n    model = \"mistral\",\n    screen = FALSE,\n    output = \"text\"\n)\n\nglue::glue(demo_3_mistral)\n\n9677 is not a prime number. A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. In the case of 9677, it can be divided evenly by 1, 7, 1381, and 9677, so it does not meet the criteria for a prime number."
  },
  {
    "objectID": "slides/slides-10.html#sentimentscores-mit-llm",
    "href": "slides/slides-10.html#sentimentscores-mit-llm",
    "title": "🔨 Sentiment Analysis",
    "section": "Sentimentscores mit LLM",
    "text": "Sentimentscores mit LLM\nPrompt-Design für einfache Sentimentanalsye via LLM in R\n\n# Erstellung einer kleinen Stichprobe\nsubsample &lt;- chats_sentiment %&gt;% \n    filter(message_length &gt; 20 & message_length &lt; 50) %&gt;%\n    slice_sample(n = 10) \n\n# Process each review using make_query\nqueries &lt;- rollama::make_query(\n    text = subsample$message_content,\n    prompt = \"Classify the sentiment of the provided text. Provide a sentiment score ranging from -1 (very negative) to 1 (very positive).\",\n    template = \"{prefix}{text}\\n{prompt}\",\n    system = \"Classify the sentiment of this text. Respond with only a numerical sentiment score.\",\n    prefix = \"Text: \"\n)\n\n# Create sentiment score for different models\nmodels &lt;- c(\"llama3.2\", \"gemma2\", \"mistral\")\nnames &lt;- c(\"llama\", \"gemma\", \"mistral\")\nfor (i in seq_along(models)) {\n  subsample[[names[i]]] &lt;- rollama::query(queries, model = models[i], screen = FALSE, output = \"text\")\n}"
  },
  {
    "objectID": "slides/slides-10.html#die-krux-mit-dem-sentiment",
    "href": "slides/slides-10.html#die-krux-mit-dem-sentiment",
    "title": "🔨 Sentiment Analysis",
    "section": "Die Krux mit dem Sentiment",
    "text": "Die Krux mit dem Sentiment\nVergleich der verschiedenen Sentiment Scores\n\nsubsample %&gt;% \n  select(message_content, polarity, valence, compound, llama, gemma, mistral) %&gt;% \n  gt() \n\n\n\n\n\n\n\nmessage_content\npolarity\nvalence\ncompound\nllama\ngemma\nmistral\n\n\n\n\nAc Games never had strong combat.\n0.000000\n0.5\n-0.169\n0\n-0.8\n0.2 (slightly negative)\n\n\nIf you believe Kamala’s lies that is on you\n0.000000\n0.0\n-0.421\n0\n-0.8\n0.35 (Negative)\n\n\nWhat audience you been in?\n0.000000\n0.0\n0.000\n0\n0\n0 (Neutral)\n\n\nshe's evolving into a giga karen\n0.000000\n0.0\n0.000\n0\n-0.8\n0.6 (Moderately Negative)\n\n\n@Megaphonix, Stop one-man spamming [2x]\n0.000000\n-1.5\n-0.649\n0\n-0.8\n0.4 (Negative)\n\n\nEveryone was in bomb shelters\n0.000000\n-1.0\n-0.494\n0\n-0.8\n0.5 (Neutral to slightly negative, indicating a state of fear or concern)\n\n\nChatting \"this rarely happens\"\n0.000000\n0.0\n0.000\n0\n0\n0.3 (Mildly Positive)\n\n\nCurseLit CurseLit CurseLit CurseLit CurseLit\n0.000000\n0.0\n0.000\n0\n-1\n0 (Neutral/Negative)\n\n\nCANT BE SICK IF YOU DONT SEE DOCTOR *big brain*\n-1.098612\n-0.5\n0.501\n0\n0.5\n0.4 (Ambivalent, leaning slightly positive due to the \"big brain\" comment, but overall tone is not clearly positive or negative)\n\n\ncan you taste the 23 flavors\n0.000000\n0.0\n0.000\n0\n0.5\n0.5 (Neutral or Slightly Positive)"
  },
  {
    "objectID": "slides/slides-10.html#und-was-machen-wir-jetzt-damit",
    "href": "slides/slides-10.html#und-was-machen-wir-jetzt-damit",
    "title": "🔨 Sentiment Analysis",
    "section": "Und was machen wir jetzt damit?",
    "text": "Und was machen wir jetzt damit?\n(Weiter-)Arbeit mit dem Sentiments\n\nValidierung, z.B.\n\nQualitativer Vergleich der Scores und dem Inhalt der Nachricht\nÜberprüfung besonders “positiver” oder “negativer” Nachrichten\nggf. Vergleich verschiedener Sentiment Scores\n\nWeiterführende Analysen, z.B.\n\nVerteilung der Sentiment Scores nach Streamer oder Länge der Nachrichten\nWichtig: Bezug zur Forschungsfrage!"
  },
  {
    "objectID": "slides/slides-10.html#unterschiedliche-emotionalität-des-chats",
    "href": "slides/slides-10.html#unterschiedliche-emotionalität-des-chats",
    "title": "🔨 Sentiment Analysis",
    "section": "Unterschiedliche Emotionalität des Chats?",
    "text": "Unterschiedliche Emotionalität des Chats?\nBeispiel für weiterführende Analyse: Sentiment Scores nach Streamer\n\nchats_sentiment %&gt;% \n    ggpubr::ggdensity(\n        x = \"compound\",\n        color = \"streamer\"\n    )"
  },
  {
    "objectID": "slides/slides-10.html#und-was-machen-wir-jetzt-damit-1",
    "href": "slides/slides-10.html#und-was-machen-wir-jetzt-damit-1",
    "title": "🔨 Sentiment Analysis",
    "section": "Und was machen wir jetzt damit?",
    "text": "Und was machen wir jetzt damit?\nBeispiel für weiterführende Analyse: Sentiment Scores nach Länge der Nachrichten\n\n\nExpand for full code\nchats_sentiment %&gt;% \n    mutate(message_length_fct = case_when( \n        message_length &lt;= 7 ~ \"&lt;= 7 words\",\n        message_length &gt; 7 & message_length &lt;= 34 ~ \"8 to 34 words\",\n        message_length &gt;= 34 ~ \"&gt; 34 words\")\n     ) %&gt;%\n    group_by(message_length_fct) %&gt;%\n    mutate(n = n()) %&gt;%\n    ggviolin(\n        x = \"message_length_fct\",\n        y = \"compound\", \n        fill = \"message_length_fct\"\n    ) +\n    stat_summary(\n        fun.data = function(x) data.frame(y = max(x) + 0.15, label = paste0(\"n=\", length(x))),\n        geom = \"text\",\n        size = 3,\n        color = \"black\"\n    ) +\n    labs(\n        x = \"Länge der Nachricht\"\n    )"
  },
  {
    "objectID": "slides/slides-10.html#validierung-validierung-validierung",
    "href": "slides/slides-10.html#validierung-validierung-validierung",
    "title": "🔨 Sentiment Analysis",
    "section": "Validierung, Validierung, Validierung",
    "text": "Validierung, Validierung, Validierung\nÜberprüfung besonders “positiver” Nachrichten\n\nchats_sentiment %&gt;% \n    filter(compound &gt;= 0.95) %&gt;% \n    arrange(desc(compound)) %&gt;% \n    select(message_content, compound) %&gt;% \n    head(n = 3) %&gt;% \n    gt() %&gt;% gtExtras::gt_theme_538()\n\n\n\n\n\n\n\nmessage_content\ncompound\n\n\n\n\nmizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3\n0.997\n\n\nhasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY\n0.996\n\n\nhasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY\n0.996"
  },
  {
    "objectID": "slides/slides-10.html#validierung-validierung-validierung-1",
    "href": "slides/slides-10.html#validierung-validierung-validierung-1",
    "title": "🔨 Sentiment Analysis",
    "section": "Validierung, Validierung, Validierung",
    "text": "Validierung, Validierung, Validierung\nÜberprüfung besonders “negativer” Nachrichten\n\nchats_sentiment %&gt;% \n    filter(compound &lt;= -0.95) %&gt;% \n    arrange(compound) %&gt;% \n    select(message_content, compound) %&gt;% \n    head(n = 3) %&gt;% \n    gt() %&gt;% gtExtras::gt_theme_538()\n\n\n\n\n\n\n\nmessage_content\ncompound\n\n\n\n\npepeMeltdown OH SHIT MY OIL FUCK FUCK FUCK pepeMeltdown OH SHIT MY OIL FUCK FUCK FUCK pepeMeltdown OH SHIT MY OIL FUCK FUCK FUCK pepeMeltdown OH SHIT MY OIL FUCK FUCK FUCK\n-0.997\n\n\nbut why kill more birdsbut why kill more birdsbut why kill more birdsbut why kill more birdsbut why kill more birdsbut why kill more birdsbut why kill more birds\n-0.996\n\n\ncry bully ass losers KEKL cry bully ass losers KEKL cry bully ass losers KEKL cry bully ass losers KEKL cry bully ass losers KEKL\n-0.996"
  },
  {
    "objectID": "slides/slides-10.html#validierung-validierung-validierung-2",
    "href": "slides/slides-10.html#validierung-validierung-validierung-2",
    "title": "🔨 Sentiment Analysis",
    "section": "Validierung, Validierung, Validierung",
    "text": "Validierung, Validierung, Validierung\nWersendert besonders negative Nachrichten?\n\nchats_sentiment %&gt;% \n    filter(compound &gt;= 0.95) %&gt;% \n    sjmisc::frq(\n        user_name, \n        min.frq = 5,\n        sort.frq = \"desc\")\n\nuser_name &lt;character&gt; \n# total N=289 valid N=289 mean=85.70 sd=50.72\n\nValue                |   N | Raw % | Valid % | Cum. %\n-----------------------------------------------------\nnotilandefinitelynot |  17 |  5.88 |    5.88 |   5.88\ndirty_barn_owl       |  16 |  5.54 |    5.54 |  11.42\naliisontw1tch        |   7 |  2.42 |    2.42 |  13.84\nomnivalor            |   7 |  2.42 |    2.42 |  16.26\nx7yz42               |   6 |  2.08 |    2.08 |  18.34\nchakek1993414        |   5 |  1.73 |    1.73 |  20.07\ndoortoratworld       |   5 |  1.73 |    1.73 |  21.80\nmuon_2ms             |   5 |  1.73 |    1.73 |  23.53\nn &lt; 5                | 221 | 76.47 |   76.47 | 100.00\n&lt;NA&gt;                 |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;"
  },
  {
    "objectID": "slides/slides-10.html#was-nehmen-wir-mit",
    "href": "slides/slides-10.html#was-nehmen-wir-mit",
    "title": "🔨 Sentiment Analysis",
    "section": "Was nehmen wir mit?",
    "text": "Was nehmen wir mit?\nKurze Zusammenfassung der Inhalte zur Sentimentanalyse\n\nVerschiedene Möglichkeiten (Modelle, Dictionaries, etc), Sentimentanalyse in R durchzuführen\nDie verschiedene Möglichkeiten haben unterschiedliche Vor- und Nachteile\nAllgemein gilt:\n\nDie Wahl des Modells hängt von der spezifischen Forschungsfrage und den verfügbaren Daten ab\nValidieren, Validieren, Validieren (& Optimieren!)\n\n\nAber: Wie sinnvoll und aussagekräftig sind (unsupervised) Sentimentanalysen in der Praxis?"
  },
  {
    "objectID": "slides/slides-10.html#goodbye-theory-hello-practice",
    "href": "slides/slides-10.html#goodbye-theory-hello-practice",
    "title": "🔨 Sentiment Analysis",
    "section": "Goodbye theory, hello practice!",
    "text": "Goodbye theory, hello practice!\nEin Blick auf die kommenden Sitzungen\n\nAbschluss der inhaltlichen Sitzungen ➜ “Projektphase”\nZiel: Durchführung einer “Mini-Studie”\n\nEntwicklung einer Forschungsfrage (auf Basis der Inhalte der Vorträge) und\nAnwendung mindestens einer der behandelten Methoden\nauf bereitgestellte Datensätze"
  },
  {
    "objectID": "slides/slides-10.html#fokus-auf-gruppenarbeit",
    "href": "slides/slides-10.html#fokus-auf-gruppenarbeit",
    "title": "🔨 Sentiment Analysis",
    "section": "Fokus auf Gruppenarbeit",
    "text": "Fokus auf Gruppenarbeit\nZum Ablauf der nächsten zwei Sitzungen\n\nFokus der nächsten Sitzungen liegt auf eigenständige Gruppenarbeit\n\nGrobe Struktur: Kurze Input-Session am Anfang (Fragerunde, Orgaupdates), danach Fokus auf Arbeit in den Gruppen\n\nNutzt die Möglichkeit für den Austausch oder Nachfragen\n\nTauscht euch untereinander aus, sprecht mit den Expert:innen der jeweiligen Sitzung!\nIch stehe währendessen als Ansprechpartner zur Verfügung\n\nDenkt an die anstehenden Assignments (Präsentationsentwurf & Peer Review)!"
  },
  {
    "objectID": "slides/slides-10.html#and-now-you",
    "href": "slides/slides-10.html#and-now-you",
    "title": "🔨 Sentiment Analysis",
    "section": "🧪 And now … you!",
    "text": "🧪 And now … you!\nFür den Rest der Sitzung: Grupppenarbeit am Projektpräsentationsentwurf\n\n\n\n\n\n\nWichtige Hinweise\n\n\n\nNächste Woche (15.01.) ist die Deadline für den Entwurf der “Projektpräsentation” ( = Grundlage für das Peer Review)\nAblauf wird nächste Woche noch detailliert besprochen!\n\n\n\n\n\n\n\n\n\n\nArbeitsauftrag\n\n\nIn euren Gruppen …\n\nbeginnt die Arbeit an der Projektpräsentation (siehe QR-Code nächste Folie)\nsetzt den Schwerpunkt zunächst auf die Forschungsfrage, und überlegt danach, wie ihr diese mit Hilfe der vorgestellten Methoden beantworten könnt"
  },
  {
    "objectID": "slides/slides-10.html#get-started",
    "href": "slides/slides-10.html#get-started",
    "title": "🔨 Sentiment Analysis",
    "section": "Get started!",
    "text": "Get started!\nBitte nutzt die jeweilige Folienvorlage für die Dokumentation euerer Ergebnisse\n\n\n\n\n     Gruppe 1 \n\n\n\n\n\n     Gruppe 2 \n\n\n\n\n\n     Gruppe 3 \n\n\n\n\n\n\n     Gruppe 4 \n\n\n\n\n\n     Gruppe 5 \n\n\n\n\n\n     Gruppe 6"
  },
  {
    "objectID": "slides/slides-10.html#references",
    "href": "slides/slides-10.html#references",
    "title": "🔨 Sentiment Analysis",
    "section": "References",
    "text": "References\n\n\nBenoit, K., Watanabe, K., Wang, H., Nulty, P., Obeng, A., Müller, S., & Matsuo, A. (2018). Quanteda: An r package for the quantitative analysis of textual data. Journal of Open Source Software, 3(30), 774. https://doi.org/10.21105/joss.00774\n\n\nGruber, J. B., & Weber, M. (2024). Rollama: Communicate with ’ollama’. https://jbgruber.github.io/rollama/\n\n\nHutto, C., & Gilbert, E. (2014). VADER: A parsimonious rule-based model for sentiment analysis of social media text. Proceedings of the International AAAI Conference on Web and Social Media, 8(1), 216–225. https://doi.org/10.1609/icwsm.v8i1.14550\n\n\nNielsen, F. Å. (2011). A new ANEW: Evaluation of a word list for sentiment analysis in microblogs. https://doi.org/10.48550/ARXIV.1103.2903\n\n\nRoehrick, K. (n.d.). vader: Valence Aware Dictionary and sEntiment Reasoner (VADER). https://doi.org/10.32614/CRAN.package.vader\n\n\nWickham, H., & Cheng, J. (2024). Ellmer: Chat with large language models. https://ellmer.tidyverse.org\n\n\nYoung, L., & Soroka, S. (2012). Affective News: The Automated Coding of Sentiment in Political Texts. Political Communication, 29(2), 205–231. https://doi.org/10.1080/10584609.2012.671234"
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Christoph Adrian (he/him)     is a Research Assistant at the Chair of Communication Science at Friedrich-Alexander-Universität (FAU) Erlangen-Nürnberg.\nHis work focuses on computational methods, especially Text as Data Approaches and workin with Digital behavioral data, with an emphasis on computing, reproducible research, student-centered learning, and open-source education.\n\n\n\nOffice hours\nLocation\n\n\n\n\nWednesday 15:30 - 16:30\nFG 2.031",
    "crumbs": [
      "Course information",
      "Teaching team"
    ]
  },
  {
    "objectID": "course-team.html#instructor",
    "href": "course-team.html#instructor",
    "title": "Teaching team",
    "section": "",
    "text": "Christoph Adrian (he/him)     is a Research Assistant at the Chair of Communication Science at Friedrich-Alexander-Universität (FAU) Erlangen-Nürnberg.\nHis work focuses on computational methods, especially Text as Data Approaches and workin with Digital behavioral data, with an emphasis on computing, reproducible research, student-centered learning, and open-source education.\n\n\n\nOffice hours\nLocation\n\n\n\n\nWednesday 15:30 - 16:30\nFG 2.031",
    "crumbs": [
      "Course information",
      "Teaching team"
    ]
  },
  {
    "objectID": "data_collection/01_03-data_collection-twitch_streamer_stats.html",
    "href": "data_collection/01_03-data_collection-twitch_streamer_stats.html",
    "title": "Mining: Twitch streamer statistics",
    "section": "",
    "text": "# Create a tibble based on the provided data\nlibrary(tidyverse)\n\ndata_hasanabi &lt;- tribble(\n  ~Month, ~Avg_Viewers, ~Avg_Viewers_Gain, ~Avg_Viewers_Percent_Gain, ~Peak_Viewers, ~Hours_Streamed, ~Hours_Streamed_Gain, ~Hours_Streamed_Percent_Gain, ~Followers, ~Followers_Gain, ~Followers_Percent_Gain, ~Followers_Per_Hour,\n  \"Dec 2024\", 26265, -17031, -39.3, 37094, 18.4, -221, -92.3, 2793538, 604, NA, 32.8,\n  \"Nov 2024\", 43296, 12510, 40.6, 312431, 239, 16.8, 7.6, 2792934, 59296, 2.2, 248,\n  \"Oct 2024\", 30786, 2921, 10.5, 110889, 223, -12.9, -5.5, 2733638, 17998, 0.7, 80.9,\n  \"Sep 2024\", 27865, -8174, -22.7, 192633, 236, 3.9, 1.7, 2715640, 26341, 1, 112,\n  \"Aug 2024\", 36039, -57, -0.2, 86598, 232, 12.1, 5.5, 2689299, 23519, 0.9, 102,\n  \"Jul 2024\", 36096, 14830, 69.7, 135265, 220, 3.7, 1.7, 2665780, 37035, 1.4, 169,\n  \"Jun 2024\", 21266, -1291, -5.7, 126069, 216, -9.5, -4.2, 2628745, 4805, 0.2, 22.3,\n  \"May 2024\", 22557, 1939, 9.4, 37656, 225, -24.4, -9.8, 2623940, 61255, 2.4, 272,\n  \"Apr 2024\", 20618, 1980, 10.6, 64992, 250, 31, 14.2, 2562685, 4545, 0.2, 18.2,\n  \"Mar 2024\", 18638, -374, -2, 46536, 219, -8.5, -3.7, 2558140, -4783, -0.2, -21.9,\n  \"Feb 2024\", 19012, -714, -3.6, 34541, 227, 8.3, 3.8, 2562923, 690, NA, 3,\n  \"Jan 2024\", 19726, 1504, 8.3, 37875, 219, 31.5, 16.8, 2562233, 1035, NA, 4.7\n  ) %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(streamer = \"hasanabi\")\n\n\n# Create a tibble based on the provided data\ndata_zackrawrr &lt;- tribble(\n  ~Month, ~Avg_Viewers, ~Avg_Viewers_Gain, ~Avg_Viewers_Percent_Gain, ~Peak_Viewers, ~Hours_Streamed, ~Hours_Streamed_Gain, ~Hours_Streamed_Percent_Gain, ~Followers, ~Followers_Gain, ~Followers_Percent_Gain, ~Followers_Per_Hour,\n  \"Dec 2024\", 31240, -17585, -36, 41061, 13.5, -155, -92, 1914946, 280, NA, 20.8,\n  \"Nov 2024\", 48825, 11527, 30.9, 170847, 168, 57.9, 52.5, 1914666, 21419, 1.1, 127,\n  \"Oct 2024\", 37298, 5369, 16.8, 80561, 110, -93.5, -45.9, 1893247, 9610, 0.5, 87.2,\n  \"Sep 2024\", 31929, -1415, -4.2, 71902, 204, 8.8, 4.5, 1883637, 21098, 1.1, 104,\n  \"Aug 2024\", 33344, 1054, 3.3, 74397, 195, -5.1, -2.5, 1862539, 22585, 1.2, 116,\n  \"Jul 2024\", 32290, 4919, 18, 93783, 200, 14.4, 7.8, 1839954, 25887, 1.4, 130,\n  \"Jun 2024\", 27371, 10911, 66.3, 62836, 186, -55.8, -23.1, 1814067, 25388, 1.4, 137,\n  \"May 2024\", 16460, -1888, -10.3, 48105, 241, 39.2, 19.4, 1788679, 15092, 0.9, 62.5,\n  \"Apr 2024\", 18348, -2500, -12, 35976, 202, 12.7, 6.7, 1773587, 18650, 1.1, 92.3,\n  \"Mar 2024\", 20848, 1142, 5.8, 40612, 190, 24.4, 14.7, 1754937, 32400, 1.9, 171,\n  \"Feb 2024\", 19706, -5189, -20.8, 41302, 165, 0.8, 0.5, 1722537, 36598, 2.2, 222,\n  \"Jan 2024\", 24895, -6579, -20.9, 45829, 164, -29.2, -15.1, 1685939, 42503, 2.6, 259\n  ) %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(streamer = \"zackrawrr\")\n\n\n# Create a tibble based on the provided data\ndata_tmr &lt;- tribble(\n  ~Month, ~Avg_Viewers, ~Avg_Viewers_Gain, ~Avg_Viewers_Percent_Gain, ~Peak_Viewers, ~Hours_Streamed, ~Hours_Streamed_Gain, ~Hours_Streamed_Percent_Gain, ~Followers, ~Followers_Gain, ~Followers_Percent_Gain, ~Followers_Per_Hour,\n  \"Dec 2024\", 1005, -632, -38.6, 1971, 7.1, -55.2, -88.6, 83325, 200, 0.2, 28.1,\n  \"Nov 2024\", 1637, 588, 56.1, 4995, 62.3, -8.2, -11.6, 83125, 5639, 7.3, 90.5,\n  \"Oct 2024\", 1049, 102, 10.8, 2383, 70.5, -0.2, -0.3, 77486, 2098, 2.8, 29.8,\n  \"Sep 2024\", 947, -131, -12.2, 2144, 70.7, -1.6, -2.2, 75388, 1075, 1.4, 15.2,\n  \"Aug 2024\", 1078, 197, 22.4, 2189, 72.3, -0.7, -0.9, 74313, 2215, 3.1, 30.6,\n  \"Jul 2024\", 881, 242, 37.9, 1971, 73, 10, 15.9, 72098, 2058, 2.9, 28.2,\n  \"Jun 2024\", 639, 7, 1.1, 1230, 62.9, -3.7, -5.5, 70040, 691, 1, 11,\n  \"May 2024\", 632, 83, 15.1, 1100, 66.6, -1.5, -2.2, 69349, 715, 1, 10.7,\n  \"Apr 2024\", 549, -52, -8.7, 905, 68.1, 4.2, 6.6, 68634, 501, 0.7, 7.4,\n  \"Mar 2024\", 601, 71, 13.4, 1206, 63.9, -1.3, -2, 68133, 427, 0.6, 6.7,\n  \"Feb 2024\", 530, -22, -4, 1907, 65.2, 0.2, 0.3, 67706, 156, 0.2, 2.4,\n  \"Jan 2024\", 552, 33, 6.4, 971, 65, 9.6, 17.3, 67550, 232, 0.3, 3.6\n  ) %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(streamer = \"the_majority_report\")\n\n\ndata &lt;- bind_rows(data_hasanabi, data_zackrawrr, data_tmr) %&gt;%\n  mutate(month = lubridate::my(month))\n\n\nqs::qsave(data, file = \"local_data/twitch_streamer_stats.qs\")"
  },
  {
    "objectID": "data_collection/01_11-data_processing-chats.html",
    "href": "data_collection/01_11-data_processing-chats.html",
    "title": "Processing: Chats",
    "section": "",
    "text": "Information\n\n\n\n\nProcesses chat data from presidential and vice-presidential debates.\nImports raw data and combines it for analysis.\nRecodes variables and adds new information.\nRenames and reorders variables for clarity.\nSaves the processed data for further use.",
    "crumbs": [
      "Data collection",
      "Processing: Chats"
    ]
  },
  {
    "objectID": "data_collection/01_11-data_processing-chats.html#preparation",
    "href": "data_collection/01_11-data_processing-chats.html#preparation",
    "title": "Processing: Chats",
    "section": "Preparation",
    "text": "Preparation\n\n# Load packages\nsource(file = here::here(\n  \"data_collection/00_02-setup-session.R\"\n))\n\n\nchat &lt;- list(\n    raw = list(\n        presidential = qs::qread(here(\"local_data/chat_raw-vods_presidential_debate.qs\")),\n        vice_presidential = qs::qread(here(\"local_data/chat_raw-vods_vice_presidential_debate.qs\"))\n    )\n)",
    "crumbs": [
      "Data collection",
      "Processing: Chats"
    ]
  },
  {
    "objectID": "data_collection/01_11-data_processing-chats.html#process-data",
    "href": "data_collection/01_11-data_processing-chats.html#process-data",
    "title": "Processing: Chats",
    "section": "Process data",
    "text": "Process data\n\n\n\n\n\n\nChangelog\n\n\n\n\nCombined data from presidential and vice-presidential debates.\nDropped “empty” meta variables: user_is_*, user_type, user_gender.\nAdded debate variable to indicate the source of the data.\nRecoded user_id to character type.\nRecoded stream_id to streamer names.\nAdded platform variable based on the URL.\nAdded message_length, message_timecode, message_time, and message_during_debate variables.\nAdded user information variables: user_has_badge, user_is_premium, user_is_subscriber, user_is_turbo, user_is_moderator, user_is_partner, user_is_subgifter, user_is_broadcaster, user_is_vip, user_is_twitchdj, user_is_founder, user_is_staff, user_is_game_dev, user_is_ambassador, user_no_audio, user_no_video.\nConverted user information variables to numeric type.\nRenamed variables: stream_id to streamer, username to user_name, display_name to user_display_name, badges to user_badges, timestamp to message_timestamp, emotes to message_emotes.\nReordered variables: moved platform and debate after url, and message_emotes after message_content.\n\n\n\n\nchat$correct &lt;- bind_rows(\n    # Combine data from presidential and vicepresidental debate\n    chat$raw$presidential %&gt;%\n        select(!c(starts_with(\"user_is_\"), user_type, user_gender)) %&gt;% # drop \"empty\" meta variables\n        mutate(debate = \"presidential\"), # add source \n    chat$raw$vice_presidential %&gt;% \n        select(!c(starts_with(\"user_is_\"), user_type, user_gender)) %&gt;% # drop \"empty\" meta variables\n        mutate(debate = \"vice presidential\")\n    ) %&gt;% \n    # Manual wrangling\n    mutate(\n        # Correction / recoding variables\n        across(user_id, ~as.character(.)), \n        across(stream_id, ~case_when(\n            str_detect(.x, \"stream_1\") ~ \"hasanabi\",\n            str_detect(.x, \"stream_2\") ~ \"zackrawrr\",\n            str_detect(.x, \"stream_3\") ~ \"the_majority_report\",\n            TRUE ~ \"unknown\")\n        ), \n        # Add information\n        platform = case_when(\n            str_detect(url, \"twitch\") ~ \"twitch\",\n            str_detect(url, \"youtube\") ~ \"youtube\",\n            TRUE ~ \"unknown\"),\n        message_length = nchar(message_content), \n        message_timecode = seconds_to_period(timestamp),\n        message_time = format(as.POSIXct(timestamp, origin = \"1970-01-01\", tz = \"UTC\"), \"%H:%M:%S\"),\n        message_during_debate = case_when(\n            # Presidential debate\n            stream_id == \"hasanabi\" & debate == \"presidential\" &\n            timestamp &gt;= hms::as_hms(\"07:00:11\") & timestamp &lt;= hms::as_hms(\"08:45:21\") |\n            stream_id == \"zackrawrr\" & debate == \"presidential\" &\n            timestamp &gt;= hms::as_hms(\"08:02:12\") & timestamp &lt;= hms::as_hms(\"09:46:15\") |\n            stream_id == \"the_majority_report\" & debate == \"presidential\" &\n            timestamp &gt;= hms::as_hms(\"00:12:53\") & timestamp &lt;= hms::as_hms(\"01:57:49\") |\n            # # Vice-Presidential debate\n            stream_id == \"hasanabi\" & debate == \"vice presidential\" &\n            timestamp &gt;= hms::as_hms(\"06:57:00\") & timestamp &lt;= hms::as_hms(\"08:43:17\") |\n            stream_id == \"zackrawrr\" & debate == \"vice presidential\" &\n            timestamp &gt;= hms::as_hms(\"07:19:26\") & timestamp &lt;= hms::as_hms(\"09:05:41\") |\n            stream_id == \"the_majority_report\" & debate == \"vice presidential\" &\n            timestamp &gt;= hms::as_hms(\"00:09:52\") & timestamp &lt;= hms::as_hms(\"01:57:07\")        \n             ~ 1,\n            TRUE ~ 0\n        ), \n        # Add user information\n        user_has_badge = map_lgl(badges, ~ length(.x) &gt; 0),\n        user_is_premium = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"premium\")), \n        user_is_subscriber = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"subscriber\")), \n        user_is_turbo = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"turbo\")),\n        user_is_moderator = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"moderator\")),\n        user_is_partner = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"partner\")),\n        user_is_subgifter = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"subgifter\")),\n        user_is_broadcaster = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"broadcaster\")),\n        user_is_vip = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"vip\")),\n        user_is_twitchdj = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"twitch_dj\")),\n        user_is_founder = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"founder\")),\n        user_is_staff = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"staff\")),\n        user_is_game_dev = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"game_developer\")),\n        user_is_ambassador = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"ambassador\")),\n        user_no_audio = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"no_audio\")),\n        user_no_video = map_lgl(badges, ~ any(map_chr(.x, ~ .x$name %||% NA_character_) == \"no_video\")),\n        across(starts_with(\"user_is_\"), ~as.numeric(.)),\n        across(starts_with(\"user_no_\"), ~as.numeric(.)),\n        across(starts_with(\"user_has_\"), ~as.numeric(.))\n    ) %&gt;% \n    # Rename and reorder variables\n    rename(\n        streamer = stream_id,\n        user_name = username,\n        user_display_name = display_name,\n        user_badges = badges,\n        message_timestamp = timestamp,\n        message_emotes = emotes,\n    ) %&gt;% \n    relocate(platform, debate, .after = url) %&gt;% \n    relocate(message_emotes, .after = message_content)\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nstreamer\nName of the streamer.\n\n\nurl\nURL of the video.\n\n\nplatform\nPlatform where the video is hosted (e.g., Twitch, YouTube).\n\n\ndebate\nType of debate (e.g., presidential, vice presidential).\n\n\nuser_name\nUsername of the user.\n\n\nuser_id\nUnique identifier for the user.\n\n\nuser_display_name\nDisplay name of the user.\n\n\nuser_badges\nList of badges associated with the user.\n\n\nmessage_timestamp\nTimestamp of the message in seconds.\n\n\nmessage_id\nUnique identifier for the message.\n\n\nmessage_type\nType of message (e.g., text_message).\n\n\nmessage_content\nContent of the message.\n\n\nmessage_emotes\nList of emotes used in the message.\n\n\nmessage_length\nLength of the message content.\n\n\nmessage_timecode\nTimecode of the message in Period format.\n\n\nmessage_time\nTime of the message in HH:MM:SS format.\n\n\nmessage_during_debate\nIndicator if the message was sent during the debate (1 for yes, 0 for no).\n\n\nuser_has_badge\nIndicator if the user has any badge (1 for yes, 0 for no).\n\n\nuser_is_premium\nIndicator if the user is a premium member (1 for yes, 0 for no).\n\n\nuser_is_subscriber\nIndicator if the user is a subscriber (1 for yes, 0 for no).\n\n\nuser_is_turbo\nIndicator if the user is a Turbo member (1 for yes, 0 for no).\n\n\nuser_is_moderator\nIndicator if the user is a moderator (1 for yes, 0 for no).\n\n\nuser_is_partner\nIndicator if the user is a partner (1 for yes, 0 for no).\n\n\nuser_is_subgifter\nIndicator if the user is a subgifter (1 for yes, 0 for no).\n\n\nuser_is_broadcaster\nIndicator if the user is the broadcaster (1 for yes, 0 for no).\n\n\nuser_is_vip\nIndicator if the user is a VIP (1 for yes, 0 for no).\n\n\nuser_is_twitchdj\nIndicator if the user is a Twitch DJ (1 for yes, 0 for no).\n\n\nuser_is_founder\nIndicator if the user is a founder (1 for yes, 0 for no).\n\n\nuser_is_staff\nIndicator if the user is a staff member (1 for yes, 0 for no).\n\n\nuser_is_game_dev\nIndicator if the user is a game developer (1 for yes, 0 for no).\n\n\nuser_is_ambassador\nIndicator if the user is an ambassador (1 for yes, 0 for no).\n\n\nuser_no_audio\nIndicator if the user has no audio (1 for yes, 0 for no).\n\n\nuser_no_video\nIndicator if the user has no video (1 for yes, 0 for no).",
    "crumbs": [
      "Data collection",
      "Processing: Chats"
    ]
  },
  {
    "objectID": "data_collection/01_11-data_processing-chats.html#save-output",
    "href": "data_collection/01_11-data_processing-chats.html#save-output",
    "title": "Processing: Chats",
    "section": "Save output",
    "text": "Save output\n\nqs::qsave(\n    chat,\n    file = here(\"local_data/chat-debates_full.qs\")\n)\n\nqs::qsave(\n    chat$correct, \n    file = here(\"local_data/chat-debates.qs\")\n)",
    "crumbs": [
      "Data collection",
      "Processing: Chats"
    ]
  },
  {
    "objectID": "data_collection/01_12-data_processing-transcripts.html",
    "href": "data_collection/01_12-data_processing-transcripts.html",
    "title": "Processing: Transcripts",
    "section": "",
    "text": "Information\n\n\n\n\nProcesses transcripts of live-streamed debates.\nImports, cleans, and transforms the data.\nSaves the processed data for further analysis.",
    "crumbs": [
      "Data collection",
      "Processing: Transcripts"
    ]
  },
  {
    "objectID": "data_collection/01_12-data_processing-transcripts.html#preparation",
    "href": "data_collection/01_12-data_processing-transcripts.html#preparation",
    "title": "Processing: Transcripts",
    "section": "Preparation",
    "text": "Preparation\n\n# Load packages\nsource(file = here::here(\n  \"data_collection/00_02-setup-session.R\"\n))\n\n\ntranscripts &lt;- list(\n    raw = fs::dir_ls(\n        path = here(\"local_data/transcripts\"), \n        glob = \"*.txt\"\n        ) %&gt;%\n        # Read the files into the list\n        map(~ read_file(.)) %&gt;%\n        # Set the list names to the base file names (without the path)\n        set_names(~ str_extract(basename(.), \"^(.*)(?=\\\\.txt)\"))\n)",
    "crumbs": [
      "Data collection",
      "Processing: Transcripts"
    ]
  },
  {
    "objectID": "data_collection/01_12-data_processing-transcripts.html#process-data",
    "href": "data_collection/01_12-data_processing-transcripts.html#process-data",
    "title": "Processing: Transcripts",
    "section": "Process data",
    "text": "Process data\n\n\n\n\n\n\nChangelog\n\n\n\n\nAdded extraction of speaker and timestamp from each line of the transcript.\nRemoved brackets from the timestamp.\nExtracted dialogue text and calculated its length.\nFiltered out lines without speaker or dialogue.\nConverted timestamp to hms object and calculated duration between dialogues.\nMerged all processed files into a single data frame with source identifiers.\nAdded columns for debate type, streamer, and numeric streamer identifier.\nCreated unique speaker identifiers combining debate type, streamer, and speaker.\nAdded sequence_during_debate indicator based on timestamp ranges for each source.\nGenerated unique id_sequence for each speaking sequence.\nRemoved unnecessary columns (line, prefix) and reordered columns.\n\n\n\n\ntranscripts$correct &lt;- transcripts$raw %&gt;% \n    # Import and process each file\n    map(~ read_file(.) %&gt;%\n            str_split(\"\\n\") %&gt;%\n            .[[1]] %&gt;%\n            tibble(line = .) %&gt;%\n            mutate(\n                speaker = str_extract(line, \"S\\\\d+\"),  # Extract speaker \n                timestamp = str_extract(line, \"\\\\[\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\]\"),  # Extract timestamps\n                across(timestamp, ~str_remove_all(., \"[\\\\[\\\\]]\")),  # Remove the brackets from timestamp\n                dialogue = str_remove(line, \"S\\\\d+ \\\\[\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\]: \"),  # Extract dialogue text\n                dialogue_length = nchar(dialogue),\n            ) %&gt;% \n            filter( # Filter out lines without speaker or dialogue\n                !is.na(speaker) &\n                !is.na(dialogue)\n                ) %&gt;% \n            mutate(\n                timestamp = hms::as_hms(timestamp),  # Convert timestamp to hms object\n                duration = as.numeric(difftime(lead(timestamp), timestamp, , units = \"secs\"))  # Calculate duration\n            )\n    ) %&gt;%\n    bind_rows(.id = \"source\") %&gt;% \n    mutate(\n        debate = case_when(\n            str_detect(source, \"vice_presidential\") ~ \"vice_presidential\",\n            TRUE ~ \"presidential\"), \n        streamer = case_when(\n            str_detect(source, \"abc\") | str_detect(source, \"cbs\") ~ \"tv_station\",\n            str_detect(source, \"hasanabi\") ~ \"hasanabi\",\n            str_detect(source, \"zackrawrr\") ~ \"zackrawrr\",\n            str_detect(source, \"the_majority_report\") ~ \"the_majority_report\",\n            TRUE ~ \"unknown\"\n        ),\n        id_streamer = case_when(\n            streamer == \"tv_station\" ~ 1,\n            streamer == \"hasanabi\" ~ 2,\n            streamer == \"the_majority_report\" ~ 3,\n            streamer == \"zackrawrr\" ~ 4,\n            TRUE ~ 0\n        ),\n        prefix = paste0(\n            ifelse(debate == \"presidential\", \"p\", \"vp\"),\n            id_streamer, \"_\"\n        ),\n        id_speaker = paste0(prefix, tolower(speaker)),\n        sequence_during_debate = case_when(\n            # Presidential debate\n            source == \"presidential_debate-abc\" &\n            timestamp &gt;= hms::as_hms(\"00:00:00\") & timestamp &lt;= hms::as_hms(\"01:45:07\") |\n            source == \"presidential_debate-hasanabi\" & \n            timestamp &gt;= hms::as_hms(\"07:00:11\") & timestamp &lt;= hms::as_hms(\"08:45:21\") |\n            source == \"presidential_debate-zackrawrr\" & \n            timestamp &gt;= hms::as_hms(\"08:02:12\") & timestamp &lt;= hms::as_hms(\"09:46:15\") |\n            source == \"presidential_debate-the_majority_report\" & \n            timestamp &gt;= hms::as_hms(\"00:12:53\") & timestamp &lt;= hms::as_hms(\"01:57:49\") |\n            # Vice-Presidential debate\n            source == \"vice_presidential_debate-cbs\" &\n            timestamp &gt;= hms::as_hms(\"00:00:00\") & timestamp &lt;= hms::as_hms(\"01:47:48\") |\n            source == \"vice_presidential_debate-hasanabi\" & \n            timestamp &gt;= hms::as_hms(\"06:57:00\") & timestamp &lt;= hms::as_hms(\"08:43:17\") |\n            source == \"presidential_debate-zackrawrr\" & \n            timestamp &gt;= hms::as_hms(\"07:19:26\") & timestamp &lt;= hms::as_hms(\"09:05:41\") |\n            source == \"vice_presidential_debate-the_majority_report\" & \n            timestamp &gt;= hms::as_hms(\"00:09:52\") & timestamp &lt;= hms::as_hms(\"01:57:07\")        \n             ~ 1,\n            TRUE ~ 0\n        )\n    ) %&gt;% \n    group_by(prefix, id_streamer) %&gt;%\n    mutate(id_sequence = paste0(prefix, \"s\", sprintf(\"%04d\", row_number()))) %&gt;%\n    ungroup() %&gt;% \n    relocate(id_sequence) %&gt;% \n    select(-line, -prefix) \n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nid_sequence\nUnique identifier for each speaking sequence\n\n\nsource\nSource of the transcript (e.g., presidential_debate-abc)\n\n\nspeaker\nIdentifier for the speaker (e.g., S27)\n\n\ntimestamp\nTimestamp of the dialogue in HH:MM:SS format\n\n\ndialogue\nText of the dialogue\n\n\ndialogue_length\nLength of the dialogue text in characters\n\n\nduration\nDuration of the dialogue in seconds\n\n\ndebate\nType of debate (e.g., presidential, vice_presidential)\n\n\nstreamer\nSource of the stream (e.g., tv_station, hasanabi)\n\n\nid_streamer\nNumeric identifier for the streamer\n\n\nid_speaker\nUnique identifier for the speaker, combining debate type, streamer, and speaker\n\n\nsequence_during_debate\nIndicator if the sequence occurred during the debate (1 = yes, 0 = no)",
    "crumbs": [
      "Data collection",
      "Processing: Transcripts"
    ]
  },
  {
    "objectID": "data_collection/01_12-data_processing-transcripts.html#save-output",
    "href": "data_collection/01_12-data_processing-transcripts.html#save-output",
    "title": "Processing: Transcripts",
    "section": "Save output",
    "text": "Save output\n\nqs::qsave(\n    transcripts,\n    file = here(\"local_data/transcripts-debates_full.qs\")\n)\n\nqs::qsave(\n    transcripts$correct,\n    file = here(\"local_data/transcripts-debates.qs\")\n)",
    "crumbs": [
      "Data collection",
      "Processing: Transcripts"
    ]
  },
  {
    "objectID": "data_collection/01_21-corpora_chats-creation.html",
    "href": "data_collection/01_21-corpora_chats-creation.html",
    "title": "Corpus: Chats",
    "section": "",
    "text": "Information\n\n\n\nBased on the chat data, this script: - Creates a corpus, tokens, and a document-feature matrix with the quanteda package [v4.1.0, @benoit2018]. - Utilizes udpipe [v0.8.11, @wijffels2023] and spacyr [v1.3.0, @benoit2023] packages for additional linguistic processing, adding lemmatization, part-of-speech tagging, and named entity recognition.",
    "crumbs": [
      "Data collection",
      "Corpus: Chats"
    ]
  },
  {
    "objectID": "data_collection/01_21-corpora_chats-creation.html#preparation",
    "href": "data_collection/01_21-corpora_chats-creation.html#preparation",
    "title": "Corpus: Chats",
    "section": "Preparation",
    "text": "Preparation\n\n# Load packages\nsource(file = here::here(\n  \"data_collection/00_02-setup-session.R\"\n))\n\n\nchat &lt;- qs::qread(here(\"local_data/chat-debates_full.qs\"))\nchat_corpora &lt;- qs::qread(here(\"local_data/chat-corpora_full.qs\"))",
    "crumbs": [
      "Data collection",
      "Corpus: Chats"
    ]
  },
  {
    "objectID": "data_collection/01_21-corpora_chats-creation.html#process-data",
    "href": "data_collection/01_21-corpora_chats-creation.html#process-data",
    "title": "Corpus: Chats",
    "section": "Process data",
    "text": "Process data\n\nchat_corpora &lt;- list()\n\n# Create corpus\nchat_corpora$corp &lt;- chats$correct %&gt;% \n    quanteda::corpus(\n        docid_field = \"message_id\", \n        text_field = \"message_content\"\n  )\n\n# Create tokens\nchat_corpora$toks &lt;- chat_corpora$corp %&gt;% \n    quanteda::tokens() \n\n# Create Document Feature Matrix (DFM)\nchat_corpora$dfm &lt;- chat_corpora$toks %&gt;% \n    quanteda::dfm()\n\n\n# Execute on first run, to download the model \n# udmodel &lt;- udpipe::udpipe_download_model(\n#     language = \"english\",\n#     model_dir = here(\"models\"))\n\n# Load udpipe model\nudmodel_english &lt;- udpipe::udpipe_load_model(file = here(\"models/english-ewt-ud-2.5-191206.udpipe\"))\n\nchat_corpora$udpipe &lt;- chat$correct %&gt;% \n  rename(\n    doc_id = message_id,\n    text = message_content\n  ) %&gt;% \n  udpipe::udpipe(udmodel_english)\n\n\n# Define environment\nreticulate::use_virtualenv(\"r-spacyr\")\n\n# Initialize\nspacyr::spacy_download_langmodel(\"en_core_web_sm\")\nspacyr::spacy_initialize(\"en_core_web_sm\")\n\nchat_corpora$spacyr &lt;- chat_corpora$corp %&gt;% \n    spacyr::spacy_parse(.,\n        tag = TRUE,\n        pos = TRUE,\n        lemma = TRUE,\n        dependency = TRUE,\n        multithread = TRUE\n    )",
    "crumbs": [
      "Data collection",
      "Corpus: Chats"
    ]
  },
  {
    "objectID": "data_collection/01_21-corpora_chats-creation.html#save-data",
    "href": "data_collection/01_21-corpora_chats-creation.html#save-data",
    "title": "Corpus: Chats",
    "section": "Save data",
    "text": "Save data\n\n# Save complete data\nqs::qsave(\n    chat_corpora,\n    file = here(\"local_data/chat-corpora_full.qs\")\n)\n\n# Save udpipe corpus\nqs::qsave(\n    chat_corpora$udpipe, \n    file = here(\"local_data/chat-corpus_udpipe.qs\")\n)\n\n# Save spacyr corpus\nqs::qsave(\n    chat_corpora$spacyr, \n    file = here(\"local_data/chat-corpus_spacyr.qs\")\n)",
    "crumbs": [
      "Data collection",
      "Corpus: Chats"
    ]
  },
  {
    "objectID": "data_collection/02_01-qc-chat-emotes.html",
    "href": "data_collection/02_01-qc-chat-emotes.html",
    "title": "Quality Control: Chat Emotes",
    "section": "",
    "text": "Information\n\n\n\nThis document outlines the process of quality control for chat emotes in a dataset. The key steps include:\n\nExtracting and processing emote information from chat messages.\nGenerating frequency tables for emotes and the number of emotes per message.\nIdentifying and handling cases where emote names are missing.\nCreating and saving a dictionary of unique emotes for further analysis.",
    "crumbs": [
      "Data collection",
      "Quality Control: Chat Emotes"
    ]
  },
  {
    "objectID": "data_collection/02_01-qc-chat-emotes.html#preparation",
    "href": "data_collection/02_01-qc-chat-emotes.html#preparation",
    "title": "Quality Control: Chat Emotes",
    "section": "Preparation",
    "text": "Preparation\n\n# Load packages\nsource(here::here(\"data_collection/00_02-setup-session.R\"))\n\n\n# Load data\nchat &lt;- qs::qread(here(\"local_data/chat-debates_full.qs\"))",
    "crumbs": [
      "Data collection",
      "Quality Control: Chat Emotes"
    ]
  },
  {
    "objectID": "data_collection/02_01-qc-chat-emotes.html#create-list-of-emotes",
    "href": "data_collection/02_01-qc-chat-emotes.html#create-list-of-emotes",
    "title": "Quality Control: Chat Emotes",
    "section": "Create list of emotes",
    "text": "Create list of emotes\n\n\n\n\n\n\nNote\n\n\n\nFor example images of the emojis visit\n\nhttps://twitchemotes.com/ or\nhttps://www.twitchmetrics.net/emotes\n\n\n\n\nemotes &lt;- chat$correct %&gt;%\n    select(message_id, message_emotes) %&gt;%\n    unnest(message_emotes) %&gt;%\n    mutate(\n        emote_id = sapply(message_emotes, function(emote) emote$id),\n        emote_name = sapply(message_emotes, function(emote) emote$name),\n        emote_locations = sapply(message_emotes, function(emote) paste(emote$locations, collapse = \", \"))\n    ) %&gt;%\n    select(message_id, emote_id, emote_name, emote_locations)\n\n\nemotes %&gt;% \n    frq(emote_name, sort.frq = \"desc\", min.frq = 25) %&gt;% \n    data.frame() %&gt;% \n    select(val, frq, raw.prc, cum.prc) %&gt;% \n    rownames_to_column(\"rank\") %&gt;%\n    rename(\"Emote\" = val, \"n\" = frq) %&gt;%\n    gt() %&gt;% \n    gtExtras::gt_theme_538() %&gt;% \n    tab_options(table.width = pct(80))\n\n\n\n\n\n\n\nrank\nEmote\nn\nraw.prc\ncum.prc\n\n\n\n\n1\nLUL\n14364\n20.38\n20.4\n\n\n2\nhasL\n5640\n8.00\n28.4\n\n\n3\nbleedPurple\n5171\n7.34\n35.7\n\n\n4\nKappa\n4154\n5.89\n41.6\n\n\n5\n&lt;3\n1789\n2.54\n44.1\n\n\n6\nNotLikeThis\n1322\n1.88\n46.0\n\n\n7\nhasChud\n1154\n1.64\n47.7\n\n\n8\n:)\n1050\n1.49\n49.1\n\n\n9\nelbyBlom\n1001\n1.42\n50.6\n\n\n10\nhasSlam\n965\n1.37\n51.9\n\n\n11\nhasHi\n840\n1.19\n53.1\n\n\n12\nWutFace\n681\n0.97\n54.1\n\n\n13\n:(\n675\n0.96\n55.1\n\n\n14\nhasMods\n593\n0.84\n55.9\n\n\n15\nhasO\n576\n0.82\n56.7\n\n\n16\nhasBoot\n540\n0.77\n57.5\n\n\n17\n:D\n538\n0.76\n58.2\n\n\n18\nhasRaid\n456\n0.65\n58.9\n\n\n19\nhasSadge\n443\n0.63\n59.5\n\n\n20\nhasBaited\n384\n0.54\n60.1\n\n\n21\nhasKkona\n374\n0.53\n60.6\n\n\n22\nhasHmm\n368\n0.52\n61.1\n\n\n23\nPopNemo\n331\n0.47\n61.6\n\n\n24\n4Head\n305\n0.43\n62.0\n\n\n25\nhasCapital\n292\n0.41\n62.4\n\n\n26\nhas0head\n286\n0.41\n62.8\n\n\n27\nhasFlex\n283\n0.40\n63.2\n\n\n28\nSeemsGood\n271\n0.38\n63.6\n\n\n29\nDinoDance\n267\n0.38\n64.0\n\n\n30\nResidentSleeper\n261\n0.37\n64.4\n\n\n31\nTheIlluminati\n255\n0.36\n64.7\n\n\n32\nR)\n245\n0.35\n65.1\n\n\n33\nSUBtember\n234\n0.33\n65.4\n\n\n34\nTwitchConHYPE\n232\n0.33\n65.8\n\n\n35\nDoritosChip\n230\n0.33\n66.1\n\n\n36\nhasWeird\n226\n0.32\n66.4\n\n\n37\nJebaited\n217\n0.31\n66.7\n\n\n38\nhasPray\n214\n0.30\n67.0\n\n\n39\nhasPOGGIES\n213\n0.30\n67.3\n\n\n40\nhasLeft\n211\n0.30\n67.6\n\n\n41\nhasRight\n204\n0.29\n67.9\n\n\n42\nPogChamp\n204\n0.29\n68.2\n\n\n43\nhasSmol\n201\n0.29\n68.5\n\n\n44\n\n200\n0.28\n68.8\n\n\n45\nBatChest\n196\n0.28\n69.0\n\n\n46\n:face_with_tears_of_joy:\n192\n0.27\n69.3\n\n\n47\n:rolling_on_the_floor_laughing:\n178\n0.25\n69.6\n\n\n48\nMrDestructoid\n167\n0.24\n69.8\n\n\n49\nhasKapp\n152\n0.22\n70.0\n\n\n50\nBibleThump\n149\n0.21\n70.2\n\n\n51\n:coconut:\n142\n0.20\n70.4\n\n\n52\nhasPause\n137\n0.19\n70.6\n\n\n53\nDansGame\n136\n0.19\n70.8\n\n\n54\nhasSilly\n135\n0.19\n71.0\n\n\n55\nhasWut\n134\n0.19\n71.2\n\n\n56\n:O\n129\n0.18\n71.4\n\n\n57\nhasREE\n125\n0.18\n71.5\n\n\n58\nhasWhat\n124\n0.18\n71.7\n\n\n59\ndsaFP\n123\n0.17\n71.9\n\n\n60\nhasChair\n119\n0.17\n72.1\n\n\n61\nFailFish\n117\n0.17\n72.2\n\n\n62\nvioSASS\n117\n0.17\n72.4\n\n\n63\nhasNerd\n112\n0.16\n72.6\n\n\n64\nhasPains\n110\n0.16\n72.7\n\n\n65\nVoteYea\n110\n0.16\n72.9\n\n\n66\nTransgenderPride\n107\n0.15\n73.0\n\n\n67\natpRtsd\n106\n0.15\n73.2\n\n\n68\nBabyRage\n106\n0.15\n73.3\n\n\n69\nL\n106\n0.15\n73.5\n\n\n70\n:P\n104\n0.15\n73.6\n\n\n71\nPoroSad\n104\n0.15\n73.8\n\n\n72\nSMOrc\n102\n0.14\n73.9\n\n\n73\ndsaL\n100\n0.14\n74.1\n\n\n74\n:/\n96\n0.14\n74.2\n\n\n75\ndsaNODDERS\n94\n0.13\n74.3\n\n\n76\nrathboPALESTINEHEART\n94\n0.13\n74.5\n\n\n77\nhasRage\n91\n0.13\n74.6\n\n\n78\nhasPOGGERS\n88\n0.12\n74.7\n\n\n79\nHeyGuys\n84\n0.12\n74.8\n\n\n80\nCoolStoryBob\n83\n0.12\n75.0\n\n\n81\nhasRant\n83\n0.12\n75.1\n\n\n82\nhas5\n81\n0.11\n75.2\n\n\n83\ndsaKEKW\n79\n0.11\n75.3\n\n\n84\nCurseLit\n77\n0.11\n75.4\n\n\n85\nBigSad\n73\n0.10\n75.5\n\n\n86\nhasGun\n73\n0.10\n75.6\n\n\n87\nWhySoSerious\n73\n0.10\n75.7\n\n\n88\n;)\n70\n0.10\n75.8\n\n\n89\nPopCorn\n69\n0.10\n75.9\n\n\n90\nhasSpooked\n66\n0.09\n76.0\n\n\n91\nSquid2\n65\n0.09\n76.1\n\n\n92\natpCap\n63\n0.09\n76.2\n\n\n93\nSquid1\n63\n0.09\n76.3\n\n\n94\n:grinning_squinting_face:\n62\n0.09\n76.4\n\n\n95\nSquid3\n62\n0.09\n76.5\n\n\n96\nSquid4\n62\n0.09\n76.5\n\n\n97\nhasPog\n61\n0.09\n76.6\n\n\n98\nhasComfy\n60\n0.09\n76.7\n\n\n99\nhasBuff\n59\n0.08\n76.8\n\n\n100\nhasUnless\n59\n0.08\n76.9\n\n\n101\nGoldPLZ\n57\n0.08\n77.0\n\n\n102\nhasZzz\n54\n0.08\n77.0\n\n\n103\nKomodoHype\n54\n0.08\n77.1\n\n\n104\nGoatEmotey\n53\n0.08\n77.2\n\n\n105\nhasPrime\n53\n0.08\n77.3\n\n\n106\nhasStop\n53\n0.08\n77.3\n\n\n107\nShush\n51\n0.07\n77.4\n\n\n108\nCoolCat\n50\n0.07\n77.5\n\n\n109\n🇵🇸\n49\n0.07\n77.5\n\n\n110\ndsaKnee\n49\n0.07\n77.6\n\n\n111\nhasWicked\n49\n0.07\n77.7\n\n\n112\nPowerUpR\n49\n0.07\n77.8\n\n\n113\nCaitlynS\n48\n0.07\n77.8\n\n\n114\nhasHug\n48\n0.07\n77.9\n\n\n115\nHypeLUL\n48\n0.07\n78.0\n\n\n116\nKappaPride\n48\n0.07\n78.0\n\n\n117\nhasHAAA\n47\n0.07\n78.1\n\n\n118\nironmouseLOVE\n47\n0.07\n78.2\n\n\n119\nsL\n47\n0.07\n78.2\n\n\n120\nstrug4Free\n47\n0.07\n78.3\n\n\n121\nFBCatch\n46\n0.07\n78.4\n\n\n122\nOSFrog\n46\n0.07\n78.4\n\n\n123\nPowerUpL\n46\n0.07\n78.5\n\n\n124\nGlitchCat\n45\n0.06\n78.6\n\n\n125\nhasMad\n45\n0.06\n78.6\n\n\n126\nhasSmash\n45\n0.06\n78.7\n\n\n127\nhasTruth\n45\n0.06\n78.8\n\n\n128\nMaxLOL\n45\n0.06\n78.8\n\n\n129\nScaredyCat\n44\n0.06\n78.9\n\n\n130\n:face-fuchsia-tongue-out:\n42\n0.06\n78.9\n\n\n131\nhasD\n42\n0.06\n79.0\n\n\n132\nOhMyDog\n42\n0.06\n79.1\n\n\n133\nDatSheffy\n41\n0.06\n79.1\n\n\n134\nFBtouchdown\n41\n0.06\n79.2\n\n\n135\nPJSugar\n41\n0.06\n79.2\n\n\n136\nTTours\n41\n0.06\n79.3\n\n\n137\n:face-green-smiling:\n40\n0.06\n79.3\n\n\n138\nrathboFREE\n39\n0.06\n79.4\n\n\n139\ndsaCOPIUM\n38\n0.05\n79.5\n\n\n140\nhasBOOMER\n38\n0.05\n79.5\n\n\n141\n:fire:\n37\n0.05\n79.6\n\n\n142\n:p\n37\n0.05\n79.6\n\n\n143\nBloodTrail\n37\n0.05\n79.7\n\n\n144\nEleGiggle\n37\n0.05\n79.7\n\n\n145\nFBBlock\n37\n0.05\n79.8\n\n\n146\nKeepo\n37\n0.05\n79.8\n\n\n147\nrathboEST\n37\n0.05\n79.9\n\n\n148\nrathboPAL\n37\n0.05\n79.9\n\n\n149\ngremloeFP\n36\n0.05\n80.0\n\n\n150\nhasEZ\n36\n0.05\n80.0\n\n\n151\nrathboINE\n36\n0.05\n80.1\n\n\n152\n:palm_tree:\n35\n0.05\n80.1\n\n\n153\nrathboFREEPALESTINE\n35\n0.05\n80.2\n\n\n154\nyugopnXISALUTE\n35\n0.05\n80.2\n\n\n155\nstrug4F12\n34\n0.05\n80.3\n\n\n156\nDarkMode\n32\n0.05\n80.3\n\n\n157\ndsaFacepalm\n32\n0.05\n80.4\n\n\n158\nmermai40Heart\n32\n0.05\n80.4\n\n\n159\nvioSLUDGE\n32\n0.05\n80.5\n\n\n160\nyugopnRAID\n32\n0.05\n80.5\n\n\n161\nBopBop\n31\n0.04\n80.5\n\n\n162\ndsaBrooks\n31\n0.04\n80.6\n\n\n163\nhasSammie\n31\n0.04\n80.6\n\n\n164\nPokPikachu\n31\n0.04\n80.7\n\n\n165\nPotFriend\n31\n0.04\n80.7\n\n\n166\nforsenBased\n30\n0.04\n80.8\n\n\n167\n:o\n29\n0.04\n80.8\n\n\n168\nhasKomrade\n29\n0.04\n80.8\n\n\n169\nGayPride\n28\n0.04\n80.9\n\n\n170\nironmouseSABERDANCE\n28\n0.04\n80.9\n\n\n171\nPixelBob\n28\n0.04\n81.0\n\n\n172\nVoHiYo\n28\n0.04\n81.0\n\n\n173\n🇺🇦\n27\n0.04\n81.0\n\n\n174\nCarlSmile\n27\n0.04\n81.1\n\n\n175\nckwanFartdance\n27\n0.04\n81.1\n\n\n176\nMiniK\n27\n0.04\n81.2\n\n\n177\nvioFP\n27\n0.04\n81.2\n\n\n178\nVoteNay\n27\n0.04\n81.2\n\n\n179\nyugopnPal\n27\n0.04\n81.3\n\n\n180\n:orange_circle:\n26\n0.04\n81.3\n\n\n181\nforsenLaughingAtYou\n26\n0.04\n81.3\n\n\n182\nFortOne\n26\n0.04\n81.4\n\n\n183\nhasFatty\n26\n0.04\n81.4\n\n\n184\nhasGachi\n26\n0.04\n81.5\n\n\n185\nKappaHD\n26\n0.04\n81.5\n\n\n186\nKEKHeim\n26\n0.04\n81.5\n\n\n187\nrhyzROT\n26\n0.04\n81.6\n\n\n188\n:loudly_crying_face:\n25\n0.04\n81.6\n\n\n189\ndsaDance\n25\n0.04\n81.7\n\n\n190\ndsaHmm\n25\n0.04\n81.7\n\n\n191\nironmouseWiggly\n25\n0.04\n81.7\n\n\n192\nPogBones\n25\n0.04\n81.8\n\n\n193\nn &lt; 25\n12861\n18.25\n100.0\n\n\n194\nNA\n0\n0.00\nNA\n\n\n\n\n\n\n\n\nemotes %&gt;% \n    group_by(message_id) %&gt;% \n    summarise( n = n()) %&gt;% \n    frq(n) %&gt;% \n    data.frame() %&gt;% \n    select(val, frq, raw.prc, cum.prc) %&gt;%\n    rename(\"Number of Emotes\" = val, \"n\" = frq) %&gt;%\n    gt() %&gt;% \n    gtExtras::gt_theme_538()    \n\n\n\n\n\n\n\nNumber of Emotes\nn\nraw.prc\ncum.prc\n\n\n\n\n1\n62534\n94.93\n94.9\n\n\n2\n2465\n3.74\n98.7\n\n\n3\n612\n0.93\n99.6\n\n\n4\n178\n0.27\n99.9\n\n\n5\n55\n0.08\n100.0\n\n\n6\n13\n0.02\n100.0\n\n\n7\n9\n0.01\n100.0\n\n\n8\n4\n0.01\n100.0\n\n\n10\n1\n0.00\n100.0\n\n\n13\n1\n0.00\n100.0\n\n\nNA\n0\n0.00\nNA",
    "crumbs": [
      "Data collection",
      "Quality Control: Chat Emotes"
    ]
  },
  {
    "objectID": "data_collection/02_01-qc-chat-emotes.html#check-empty-emotes",
    "href": "data_collection/02_01-qc-chat-emotes.html#check-empty-emotes",
    "title": "Quality Control: Chat Emotes",
    "section": "Check: “Empty emotes”",
    "text": "Check: “Empty emotes”\n\nemotes %&gt;% \n    group_by(emote_name, emote_id) %&gt;% \n    summarise(n = n()) %&gt;% \n    arrange(desc(n)) %&gt;% \n    filter(emote_name == \"\") %&gt;%\n    gt() %&gt;% \n    gtExtras::gt_theme_538()\n\n`summarise()` has grouped output by 'emote_name'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\nemote_id\nn\n\n\n\n\n\n\n\n425618\n53\n\n\n302140936\n29\n\n\n555555558\n21\n\n\n555555584\n21\n\n\n1\n13\n\n\n555555560\n11\n\n\n555555580\n8\n\n\n306629700\n5\n\n\n300238152\n4\n\n\n302587115\n3\n\n\n303446392\n3\n\n\nemotesv2_4f058d58458544a4971de55672468204\n3\n\n\n300238154\n2\n\n\n300756431\n2\n\n\n555555577\n2\n\n\n555555589\n2\n\n\n300238151\n1\n\n\n300238155\n1\n\n\n301079765\n1\n\n\n303433990\n1\n\n\n303706436\n1\n\n\n304192517\n1\n\n\n354\n1\n\n\n489\n1\n\n\n508650\n1\n\n\n555555557\n1\n\n\n555555563\n1\n\n\n555555585\n1\n\n\n6\n1\n\n\n626795\n1\n\n\n62835\n1\n\n\nemotesv2_1c432fe325994220960ed5720682ca63\n1\n\n\nemotesv2_819621bcb8f44566a1bd8ea63d06c58f\n1\n\n\nemotesv2_89106685bdb643c2943994b027e25556\n1\n\n\n\n\n\n\n\n\nemote_name_recode &lt;- emotes %&gt;%\n    group_by(emote_id, emote_name) %&gt;%\n    summarise(n = n(), .groups = 'drop') %&gt;%\n    arrange(desc(n)) %&gt;%\n    group_by(emote_id) %&gt;%\n    filter(n == max(n)) %&gt;%\n    slice(1) %&gt;%\n    select(emote_id, emote_name)\n\nemotes_recoded &lt;- emotes %&gt;%\n    left_join(emote_name_recode, by = \"emote_id\", suffix = c(\"\", \"_recode\")) %&gt;%\n    mutate(emote_name = ifelse(!is.na(emote_name_recode), emote_name_recode, emote_name)) %&gt;%\n    select(-emote_name_recode)",
    "crumbs": [
      "Data collection",
      "Quality Control: Chat Emotes"
    ]
  },
  {
    "objectID": "data_collection/02_01-qc-chat-emotes.html#create-emote-dictionary",
    "href": "data_collection/02_01-qc-chat-emotes.html#create-emote-dictionary",
    "title": "Quality Control: Chat Emotes",
    "section": "Create emote dictionary",
    "text": "Create emote dictionary\n\n# Extract unique emotes\nemotes_recoded_names &lt;- emotes_recoded %&gt;% \n    distinct(emote_name, .keep_all = TRUE) %&gt;% \n    arrange(emote_name) %&gt;% \n    filter(emote_name != \"\") %&gt;%\n    pull(emote_name)\n\n# Ensure the list is named\nnamed_emotes_recoded_names &lt;- setNames(\n  as.list(emotes_recoded_names), emotes_recoded_names)\n\n# Create a dictionary\ndict_emotes &lt;- dictionary(named_emotes_recoded_names)\n\n# Save the dictionary to a file\nsaveRDS(dict_emotes, file = here(project_dir, \"local_data/dictionary_chat_emotes.RDS\"))",
    "crumbs": [
      "Data collection",
      "Quality Control: Chat Emotes"
    ]
  },
  {
    "objectID": "course-presentations.html",
    "href": "course-presentations.html",
    "title": "Course presentations",
    "section": "",
    "text": "Zimmer, F., Scheibe, K., & Stock, W. G. (2018). A Model for Information Behavior Research on Social Live Streaming Services (SLSSs) (G. Meiselwitz, Ed.; pp. 429–448). Springer International Publishing. https://doi.org/10.1007/978-3-319-91485-5_33\nHou, F., Guan, Z., Li, B., & Chong, A. Y. L. (2020). Factors influencing people’s continuous watching intention and consumption intention in live streaming. Internet Research, 30(1), 141–163. https://doi.org/10.1108/INTR-04-2018-0177\nCabeza-Ramírez, L. J., Fuentes-García, F. J., & Muñoz-Fernandez, G. A. (2021). Exploring the Emerging Domain of Research on Video Game Live Streaming in Web of Science: State of the Art, Changes and Trends. International Journal of Environmental Research and Public Health, 18(6), 2917. https://doi.org/10.3390/ijerph18062917\nNguyen, T. T., & Veer, E. (2024). Why people watch user-generated videos? A systematic review and meta-analysis. International Journal of Human-Computer Studies, 181, 103144. https://doi.org/10.1016/j.ijhcs.2023.103144\n\n\nXu, Y., Kapitan, S., & Phillips, M. (2023). The commercial impact of live streaming: A systematic literature review and future research agenda. International Journal of Consumer Studies, 47(6), 2495–2527. https://doi.org/10.1111/ijcs.12960\n\n\n\n\nGros, D., Hackenholt, A., Zawadzki, P., & Wanner, B. (2018). Interactions of Twitch Users and Their Usage Behavior (G. Meiselwitz, Ed.; pp. 201–213). Springer International Publishing. https://doi.org/10.1007/978-3-319-91485-5_15\nWolff, G. H., & Shen, C. (2022). Audience size, moderator activity, gender, and content diversity: Exploring user participation and financial commitment on Twitch.tv. New Media & Society, 146144482110699. https://doi.org/10.1177/14614448211069996\nGamir-Ríos, J., Cano-Orón, L., & García-Casas, D. (2024). Twitch’s Second Phase of Development: Analyzing Streamer Profiles and Content Trends That Boost Its Evolution into a Mass Media. Games and Culture, 15554120241257030. https://doi.org/10.1177/15554120241257030\nCastro-Agirre, I., & Martínez-Fernández, G. (2024). From gamer niche to mainstream media: Twitch’s most popular media figures and content. Communication & Society, 179–196. https://doi.org/10.15581/003.37.2.179-196"
  },
  {
    "objectID": "course-presentations.html#introduction-to-social-live-streaming-services",
    "href": "course-presentations.html#introduction-to-social-live-streaming-services",
    "title": "Course presentations",
    "section": "",
    "text": "Zimmer, F., Scheibe, K., & Stock, W. G. (2018). A Model for Information Behavior Research on Social Live Streaming Services (SLSSs) (G. Meiselwitz, Ed.; pp. 429–448). Springer International Publishing. https://doi.org/10.1007/978-3-319-91485-5_33\nHou, F., Guan, Z., Li, B., & Chong, A. Y. L. (2020). Factors influencing people’s continuous watching intention and consumption intention in live streaming. Internet Research, 30(1), 141–163. https://doi.org/10.1108/INTR-04-2018-0177\nCabeza-Ramírez, L. J., Fuentes-García, F. J., & Muñoz-Fernandez, G. A. (2021). Exploring the Emerging Domain of Research on Video Game Live Streaming in Web of Science: State of the Art, Changes and Trends. International Journal of Environmental Research and Public Health, 18(6), 2917. https://doi.org/10.3390/ijerph18062917\nNguyen, T. T., & Veer, E. (2024). Why people watch user-generated videos? A systematic review and meta-analysis. International Journal of Human-Computer Studies, 181, 103144. https://doi.org/10.1016/j.ijhcs.2023.103144\n\n\nXu, Y., Kapitan, S., & Phillips, M. (2023). The commercial impact of live streaming: A systematic literature review and future research agenda. International Journal of Consumer Studies, 47(6), 2495–2527. https://doi.org/10.1111/ijcs.12960\n\n\n\n\nGros, D., Hackenholt, A., Zawadzki, P., & Wanner, B. (2018). Interactions of Twitch Users and Their Usage Behavior (G. Meiselwitz, Ed.; pp. 201–213). Springer International Publishing. https://doi.org/10.1007/978-3-319-91485-5_15\nWolff, G. H., & Shen, C. (2022). Audience size, moderator activity, gender, and content diversity: Exploring user participation and financial commitment on Twitch.tv. New Media & Society, 146144482110699. https://doi.org/10.1177/14614448211069996\nGamir-Ríos, J., Cano-Orón, L., & García-Casas, D. (2024). Twitch’s Second Phase of Development: Analyzing Streamer Profiles and Content Trends That Boost Its Evolution into a Mass Media. Games and Culture, 15554120241257030. https://doi.org/10.1177/15554120241257030\nCastro-Agirre, I., & Martínez-Fernández, G. (2024). From gamer niche to mainstream media: Twitch’s most popular media figures and content. Communication & Society, 179–196. https://doi.org/10.15581/003.37.2.179-196"
  },
  {
    "objectID": "course-presentations.html#motivations-for-using-slss",
    "href": "course-presentations.html#motivations-for-using-slss",
    "title": "Course presentations",
    "section": "Motivations for using SLSS",
    "text": "Motivations for using SLSS\n\nTopic 3: By streamers\nZimmer, F., & Scheibe, K. (2019). Hawaii international conference on system sciences. https://doi.org/10.24251/HICSS.2019.306\nScheibe, K., Zimmer, F., Fietkiewicz, K., & Stock, W. (2022). Interpersonal Relations and Social Actions on Live Streaming Services. A Systematic Review on Cyber-social Relations. http://hdl.handle.net/10125/79744\nYoung, A., & Wiedenfeld, G. (2022). A Motivation Analysis of Video Game Microstreamers: “Finding My People and Myself” on YouTube and Twitch. Journal of Broadcasting & Electronic Media, 66(2), 381–399. https://doi.org/10.1080/08838151.2022.2086549\nXi, D., Xu, W., Tang, L., & Han, B. (2024). The impact of streamer emotions on viewer gifting behavior: Evidence from entertainment live streaming. Internet Research, 34(3), 748–783. https://doi.org/10.1108/INTR-05-2022-0350\nTomlinson, C. (2024). Community Grievances, personal responsibility, and DIY protection: Frustrations and solution-seeking among marginalized Twitch streamers. Convergence: The International Journal of Research into New Media Technologies, 30(1), 358–374. https://doi.org/10.1177/13548565231184060\n\nOptional:\nChinchilla, P., & Kim, J. (2024). “Let’s Chill and Chat”: Exploring the Effects of Streamers’ Self-Disclosure on Parasocial Interaction via Social Presence. International Journal of HumanComputer Interaction, 1–11. https://doi.org/10.1080/10447318.2024.2390263\n\n\n\nTopic 4: By users\nHilvert-Bruce, Z., Neill, J. T., Sjöblom, M., & Hamari, J. (2018). Social motivations of live-streaming viewer engagement on twitch. Computers in Human Behavior, 84, 58–67. https://doi.org/10.1016/j.chb.2018.02.013\nGoh, Z. H., Tandoc, E. C., & Ng, B. (2021). “Live” Together with You: Livestream Views Mitigate the Effects of Loneliness on Well-being. Journal of Broadcasting & Electronic Media, 65(4), 505–524. https://doi.org/10.1080/08838151.2021.1994970\nLeith, A. P., & Gheen, E. (2022). Twitch in the time of quarantine: The role of engagement in needs fulfillment. Psychology of Popular Media, 11(3), 275–280. https://doi.org/10.1037/ppm0000372\nLessel, P., Altmeyer, M., Sahner, J., & Krüger, A. (2022). Streamer’s hell - investigating audience influence in live-streams beyond the game. Proc. ACM Hum.-Comput. Interact., 6(CHI PLAY), 252:1252:27. https://doi.org/10.1145/3549515\nZsila, Á., Shabahang, R., Aruguete, M. S., Bőthe, B., Gregor-Tóth, P., & Orosz, G. (2024). Exploring the association between Twitch use and well-being. Psychology of Popular Media, 13(4), 620–632. https://doi.org/10.1037/ppm0000486\nKneisel, A., & Sternadori, M. (2023). Effects of parasocial affinity and gender on live streaming fans’ motivations. Convergence, 29(2), 322–341. https://doi.org/10.1177/13548565221114461"
  },
  {
    "objectID": "course-presentations.html#effects-of-slss",
    "href": "course-presentations.html#effects-of-slss",
    "title": "Course presentations",
    "section": "Effects of SLSS",
    "text": "Effects of SLSS\n\nUse specific\nBründl, S., Matt, C., Hess, T., & Engert, S. (2023). How synchronous participation affects the willingness to subscribe to social live streaming services: The role of co-interactive behavior on twitch. European Journal of Information Systems, 32(5), 800–817. https://doi.org/10.1080/0960085X.2022.2062468\nNavarro, A., & Tapiador, F. J. (2023). Twitch as a privileged locus to analyze young people’s attitudes in the climate change debate: a quantitative analysis. Humanities and Social Sciences Communications, 10(1), 1–13. https://doi.org/10.1057/s41599-023-02377-4\n\n\nContent specific\nDutt, S., & Graham, S. (2023). Video, talk and text: How do parties communicate coherently across modalities in live videostreams? Discourse, Context and Media, 55. https://doi.org/10.1016/j.dcm.2023.100726\nMao, E. (2022). How live stream content types impact viewers’ support behaviors? Mediational analysis on psychological and social gratifications. Frontiers in Psychology, 13. https://doi.org/10.3389/fpsyg.2022.951055\nRiddick, S., & Shivener, R. (2022). Affective spamming on twitch: Rhetorics of an emote-only audience in a presidential inauguration livestream. Computers and Composition, 64. https://doi.org/10.1016/j.compcom.2022.102711\n\nEmotions\nLacko, D., Dufková, E., & Machackova, H. (2023). Does aggressive commentary by streamers during violent video game affect state aggression in adolescents? New Media & Society, 14614448231182620. https://doi.org/10.1177/14614448231182620\nLacko, D., Macháčková, H., & Smahel, D. (n.d.). Does violence in video games impact aggression and empathy? A longitudinal study of czech adolescents to differentiate within- and between-person effects. https://doi.org/10.31234/osf.io/dns8t"
  },
  {
    "objectID": "data_collection/01_01-data_collection-presidential_debate.html",
    "href": "data_collection/01_01-data_collection-presidential_debate.html",
    "title": "Mining: Presidential Debate",
    "section": "",
    "text": "Information\n\n\n\nThis document outlines the process of collecting live chat data from Twitch and YouTube VODs of the Presidential debate held on 10.09.2024. The steps taken include:\n\nSetting up the R and Python environments, including installing necessary packages.\nAuthenticating with Twitch using the twitchr package.\nPreparing a list of VOD URLs to be processed.\nDownloading the VODs using the twitch-dl tool.\nCollecting live chat messages from the VODs using the chat_downloader Python package.\nConverting the collected chat data into a pandas DataFrame and then into an R tibble.\nExporting the collected chat data to a local file for further analysis.",
    "crumbs": [
      "Data collection",
      "Mining: Presidential Debate"
    ]
  },
  {
    "objectID": "data_collection/01_01-data_collection-presidential_debate.html#preparation",
    "href": "data_collection/01_01-data_collection-presidential_debate.html#preparation",
    "title": "Mining: Presidential Debate",
    "section": "Preparation",
    "text": "Preparation\n\n# Setup R environment\npacman::p_load(\n    here, fs, \n    twitchr,\n    tidyverse,\n    reticulate\n)\n\n\n# Setup Python environment\nimport datetime\nimport email, smtplib, ssl\nimport pandas as pd\nimport rpy2.robjects as robjects\nimport twitchdl\n\nfrom chat_downloader import ChatDownloader\nfrom chat_downloader.sites import TwitchChatDownloader \n\n\n# twitch authorization\ntwitchr::twitch_auth()",
    "crumbs": [
      "Data collection",
      "Mining: Presidential Debate"
    ]
  },
  {
    "objectID": "data_collection/01_01-data_collection-presidential_debate.html#prepare-list-of-vods",
    "href": "data_collection/01_01-data_collection-presidential_debate.html#prepare-list-of-vods",
    "title": "Mining: Presidential Debate",
    "section": "Prepare list of VODs",
    "text": "Prepare list of VODs\n\ndebate_vods_urls &lt;- c(\n    # Twitch\n    \"https://www.twitch.tv/videos/2247664726\", #hasanabi\n    \"https://www.twitch.tv/videos/2247617457\", #zackrawrr\n    # # YouTube\n    \"https://www.youtube.com/watch?v=lzobJil9Sgc\" # Majority Report Live\n)",
    "crumbs": [
      "Data collection",
      "Mining: Presidential Debate"
    ]
  },
  {
    "objectID": "data_collection/01_01-data_collection-presidential_debate.html#prepare-the-download-links",
    "href": "data_collection/01_01-data_collection-presidential_debate.html#prepare-the-download-links",
    "title": "Mining: Presidential Debate",
    "section": "Prepare the download links",
    "text": "Prepare the download links\n\n# HasanAbi\ntwitch-dl download https://www.twitch.tv/videos/2247664726 --quality 720p30\n# zackrawrr\ntwitch-dl download https://www.twitch.tv/videos/2247617457 --quality 720p60",
    "crumbs": [
      "Data collection",
      "Mining: Presidential Debate"
    ]
  },
  {
    "objectID": "data_collection/01_01-data_collection-presidential_debate.html#collect-live-chat",
    "href": "data_collection/01_01-data_collection-presidential_debate.html#collect-live-chat",
    "title": "Mining: Presidential Debate",
    "section": "Collect live chat",
    "text": "Collect live chat\n\n# Assuming url_py is already a Python list of URLs from R\nurl_py = list(robjects.globalenv['debate_vods_urls'])\n\n# Initialize the ChatDownloader\nchat_downloader = ChatDownloader()\n\n# Initialize an empty list to store message data\nmessage_list = []\n\n# Function to generate a unique stream ID (can be URL or index-based)\ndef generate_stream_id(url, index):\n    return f\"stream_{index+1}\"\n\n# Debugging: Print the list of URLs\nprint(\"URLs to process:\", url_py)\n\n# Loop through each URL and download the chat\nfor idx, url in enumerate(url_py):\n    try:\n        print(f\"Processing URL: {url}\")\n        \n        # Fetch chat\n        chat = chat_downloader.get_chat(url)\n        if not chat:\n            print(f\"No chat data found for {url}\")\n            continue  # Skip to the next URL if no chat found\n\n        stream_id = generate_stream_id(url, idx)  # Generate a unique stream ID\n        \n        print(f\"Downloading chat for {url}\")\n        \n        for message in chat:\n            # Log message info for debugging\n            \n            # Extract message details\n            message_content = message.get('message', '')\n            message_id = message.get('message_id', None)\n            message_type = message.get('message_type', 'None')\n            timestamp = message.get('time_in_seconds', None)\n\n            # Extract author details (ensure the author field exists)\n            author_info = message.get('author', {})  # Unpack dictionary with author info\n            author_id = author_info.get('id', 'NA')  # Extract author ID\n            author_name = author_info.get('name', 'NA')  # Extract author name\n            author_type = author_info.get('type', 'NA')  # Extract type of author\n            author_gender = author_info.get('gender', 'NA')  # Extract gender of the author\n            author_bot = author_info.get('is_bot', 'NA')  # True if the user is a bot, False otherwise.\n            author_poster = author_info.get('is_original_poster', 'NA')  # True if the user is the original poster, False otherwise.\n            author_verified = author_info.get('is_verified', 'NA')  # True if the user is verified, False otherwise.\n            display_name = author_info.get('display_name', author_name)  # Extract display name\n            badges = author_info.get('badges', [])  # Keep badges as a list\n            emotes = message.get('emotes', [])  # Keep emotes as a list\n\n            # Create a dictionary representing one row of the tibble\n            message_with_info = {\n                'stream_id': stream_id,  # Add the stream ID\n                'url': url,  # Add the stream URL\n                'username': author_name,  \n                'user_id': author_id,  \n                'display_name': display_name,  \n                'user_type': author_type, \n                'user_gender': author_gender,\n                'user_is_bot': author_bot,\n                'user_is_original_poster': author_poster,\n                'user_is_verified': author_verified,\n                'badges': badges,  # Add the badges as a list\n                'emotes': emotes,  # Add the emotes as a list\n                'timestamp': timestamp,  # Add the message timestamp\n                'message_id': message_id,  # Add the message ID\n                'message_type': message_type,  # Add the message type\n                'message_content': message_content  # Add the actual message text\n            }\n\n            # Append the dictionary to the list\n            message_list.append(message_with_info)\n                \n    except Exception as e:\n        print(f\"Error processing {url}: {e}\")\n\n# Print the final list of messages collected\nprint(\"Collection finished\")\n\n\n# Convert the list of dictionaries to a pandas DataFrame\nmessage_df = pd.DataFrame(message_list)\n\n\n# Access the message_df from Python\ndf &lt;- py$message_df %&gt;% \n    as_tibble()\n\n# Check the structure of the tibble\ndf %&gt;% glimpse\n\n\n# Name of subdirectory for easier path managment\nproject_dir &lt;- here::here(\"2024-nlp_of_live_stream_chat\")\n\nqs::qsave(df, file = here(project_dir, \"local_data/chat_raw-vods_presidential_debate.qs\"))",
    "crumbs": [
      "Data collection",
      "Mining: Presidential Debate"
    ]
  },
  {
    "objectID": "data_collection/01_02-data_collection-vice_presidential_debate.html",
    "href": "data_collection/01_02-data_collection-vice_presidential_debate.html",
    "title": "Mining: Vice-Presidential Debate",
    "section": "",
    "text": "Information\n\n\n\nThis document outlines the process of collecting live chat data from Twitch VODs of the Vice-Presidential debate held on 02.10.2024. The steps taken include:\n\nSetting up the R and Python environments, including installing necessary packages.\nAuthenticating with Twitch using the twitchr package.\nPreparing a list of VOD URLs to be processed.\nDownloading the VODs using the twitch-dl tool.\nCollecting live chat messages from the VODs using the chat_downloader Python package.\nConverting the collected chat data into a pandas DataFrame and then into an R tibble.\nExporting the collected chat data to a local file for further analysis.",
    "crumbs": [
      "Data collection",
      "Mining: Vice-Presidential Debate"
    ]
  },
  {
    "objectID": "data_collection/01_02-data_collection-vice_presidential_debate.html#preparation",
    "href": "data_collection/01_02-data_collection-vice_presidential_debate.html#preparation",
    "title": "Mining: Vice-Presidential Debate",
    "section": "Preparation",
    "text": "Preparation\n\n# Setup R environment\npacman::p_load(\n    here, fs, \n    twitchr,\n    tidyverse,\n    reticulate\n)\n\n\n# Setup Python environment\nimport datetime\nimport email, smtplib, ssl\nimport pandas as pd\nimport rpy2.robjects as robjects\nimport twitchdl\n\nfrom chat_downloader import ChatDownloader\nfrom chat_downloader.sites import TwitchChatDownloader \n\n\n# twitch authorization\ntwitchr::twitch_auth()",
    "crumbs": [
      "Data collection",
      "Mining: Vice-Presidential Debate"
    ]
  },
  {
    "objectID": "data_collection/01_02-data_collection-vice_presidential_debate.html#prepare-list-of-vods",
    "href": "data_collection/01_02-data_collection-vice_presidential_debate.html#prepare-list-of-vods",
    "title": "Mining: Vice-Presidential Debate",
    "section": "Prepare list of VODs",
    "text": "Prepare list of VODs\n\ndebate_vods_urls &lt;- c(\n    # Twitch\n    \"https://www.twitch.tv/videos/2265091277\", #hasanabi\n    \"https://www.twitch.tv/videos/2265091311\", #zackrawrr\n    \"https://www.twitch.tv/videos/2265413840\" # Majority Report Live\n)\n\ns ## Prepare the download links\n\n# HasanAbi\ntwitch-dl download https://www.twitch.tv/videos/2265091277 --quality 720p30\n# zackrawrr\ntwitch-dl download https://www.twitch.tv/videos/2265091311 --quality 720p60\n# The Majority Report\ntwitch-dl download https://www.twitch.tv/videos/2265413840 --quality 720p30",
    "crumbs": [
      "Data collection",
      "Mining: Vice-Presidential Debate"
    ]
  },
  {
    "objectID": "data_collection/01_02-data_collection-vice_presidential_debate.html#collect-live-chat",
    "href": "data_collection/01_02-data_collection-vice_presidential_debate.html#collect-live-chat",
    "title": "Mining: Vice-Presidential Debate",
    "section": "Collect live chat",
    "text": "Collect live chat\n\n# Assuming url_py is already a Python list of URLs from R\nurl_py = list(robjects.globalenv['debate_vods_urls'])\n\n# Initialize the ChatDownloader\nchat_downloader = ChatDownloader()\n\n# Initialize an empty list to store message data\nmessage_list = []\n\n# Function to generate a unique stream ID (can be URL or index-based)\ndef generate_stream_id(url, index):\n    return f\"stream_{index+1}\"\n\n# Debugging: Print the list of URLs\nprint(\"URLs to process:\", url_py)\n\n# Loop through each URL and download the chat\nfor idx, url in enumerate(url_py):\n    try:\n        print(f\"Processing URL: {url}\")\n        \n        # Fetch chat\n        chat = chat_downloader.get_chat(url)\n        if not chat:\n            print(f\"No chat data found for {url}\")\n            continue  # Skip to the next URL if no chat found\n\n        stream_id = generate_stream_id(url, idx)  # Generate a unique stream ID\n        \n        print(f\"Downloading chat for {url}\")\n        \n        for message in chat:\n            # Log message info for debugging\n            \n            # Extract message details\n            message_content = message.get('message', '')\n            message_id = message.get('message_id', None)\n            message_type = message.get('message_type', 'None')\n            timestamp = message.get('time_in_seconds', None)\n\n            # Extract author details (ensure the author field exists)\n            author_info = message.get('author', {})  # Unpack dictionary with author info\n            author_id = author_info.get('id', 'NA')  # Extract author ID\n            author_name = author_info.get('name', 'NA')  # Extract author name\n            author_type = author_info.get('type', 'NA')  # Extract type of author\n            author_gender = author_info.get('gender', 'NA')  # Extract gender of the author\n            author_bot = author_info.get('is_bot', 'NA')  # True if the user is a bot, False otherwise.\n            author_poster = author_info.get('is_original_poster', 'NA')  # True if the user is the original poster, False otherwise.\n            author_verified = author_info.get('is_verified', 'NA')  # True if the user is verified, False otherwise.\n            author_moderator = author_info.get('is_moderator', 'NA')  # True if the user is a moderator, False otherwise.\n            author_subscriber = author_info.get('is_subscriber', 'NA')  # True if the user is a subscriber, False otherwise.\n            display_name = author_info.get('display_name', author_name)  # Extract display name\n            badges = author_info.get('badges', [])  # Keep badges as a list\n            emotes = message.get('emotes', [])  # Keep emotes as a list\n\n            # Create a dictionary representing one row of the tibble\n            message_with_info = {\n                'stream_id': stream_id,  # Add the stream ID\n                'url': url,  # Add the stream URL\n                'username': author_name,  \n                'user_id': author_id,  \n                'display_name': display_name,  \n                'user_type': author_type, \n                'user_gender': author_gender,\n                'user_is_bot': author_bot,\n                'user_is_original_poster': author_poster,\n                'user_is_verified': author_verified,\n                'user_is_moderator': author_moderator,\n                'user_is_subscriber': author_subscriber,\n                'badges': badges,  # Add the badges as a list\n                'emotes': emotes,  # Add the emotes as a list\n                'timestamp': timestamp,  # Add the message timestamp\n                'message_id': message_id,  # Add the message ID\n                'message_type': message_type,  # Add the message type\n                'message_content': message_content  # Add the actual message text\n            }\n\n            # Append the dictionary to the list\n            message_list.append(message_with_info)\n                \n    except Exception as e:\n        print(f\"Error processing {url}: {e}\")\n\n# Print the final list of messages collected\nprint(\"Collection finished\")\n\n\n# Convert the list of dictionaries to a pandas DataFrame\nmessage_df = pd.DataFrame(message_list)\n\n\n# Access the message_df from Python\ndf &lt;- py$message_df %&gt;% \n    as_tibble()\n\n# Check the structure of the tibble\ndf %&gt;% glimpse\n\n\n# Name of subdirectory for easier path managment\nproject_dir &lt;- here::here(\"2024-nlp_of_live_stream_chat\")\n\nqs::qsave(df, file = here(project_dir, \"local_data/chat_raw-vods_vice_presidential_debate.qs\"))",
    "crumbs": [
      "Data collection",
      "Mining: Vice-Presidential Debate"
    ]
  },
  {
    "objectID": "data_collection/01_22-corpora_transcripts-creation.html",
    "href": "data_collection/01_22-corpora_transcripts-creation.html",
    "title": "Corpus: Transcripts",
    "section": "",
    "text": "Information\n\n\n\nBased on the transcript data, this script:\n\nCreates a corpus, tokens, and a document-feature matrix with the quanteda package (v4.1.0, Benoit et al. 2018).\nUtilizes udpipe (v0.8.11, Wijffels 2023) and spacyr (v1.3.0, Benoit and Matsuo 2023) packages for additional linguistic processing, adding lemmatization, part-of-speech tagging, and named entity recognition.",
    "crumbs": [
      "Data collection",
      "Corpus: Transcripts"
    ]
  },
  {
    "objectID": "data_collection/01_22-corpora_transcripts-creation.html#preparation",
    "href": "data_collection/01_22-corpora_transcripts-creation.html#preparation",
    "title": "Corpus: Transcripts",
    "section": "Preparation",
    "text": "Preparation\n\n# Load packages\nsource(file = here::here(\n  \"data_collection/00_02-setup-session.R\"\n))\n\n\ntranscripts &lt;- qs::qread(here(\"local_data/transcripts-debates_full.qs\"))",
    "crumbs": [
      "Data collection",
      "Corpus: Transcripts"
    ]
  },
  {
    "objectID": "data_collection/01_22-corpora_transcripts-creation.html#process-data",
    "href": "data_collection/01_22-corpora_transcripts-creation.html#process-data",
    "title": "Corpus: Transcripts",
    "section": "Process data",
    "text": "Process data\n\ntranscripts_corpora &lt;- list()\n\n# Create corpus\ntranscripts_corpora$corp &lt;- transcripts$correct %&gt;% \n    quanteda::corpus(\n        docid_field = \"id_sequence\", \n        text_field = \"dialogue\"\n  )\n\n# Create tokens\ntranscripts_corpora$toks &lt;- transcripts_corpora$corp %&gt;% \n    quanteda::tokens(\n        remove_punct = TRUE, \n        remove_symbols = TRUE,\n        remove_numbers = TRUE,\n        remove_url = TRUE, \n        split_hyphens = FALSE,\n        split_tags = FALSE\n        ) %&gt;% \n    quanteda::tokens_remove(\n        pattern = quanteda::stopwords(\"en\")\n    )\n\n# Create Document Feature Matrix (DFM)\ntranscripts_corpora$dfm &lt;- transcripts_corpora$toks %&gt;% \n    quanteda::dfm()\n\n\n# Execute on first run, to download the model \n# udmodel &lt;- udpipe::udpipe_download_model(\n#     language = \"english\",\n#     model_dir = here(\"models\"))\n\n# Load udpipe model\nudmodel_english &lt;- udpipe::udpipe_load_model(file = here(\"models/english-ewt-ud-2.5-191206.udpipe\"))\n\ntranscripts_corpora$udpipe &lt;- transcripts$correct %&gt;% \n  rename(\n    doc_id = id_sequence,\n    text = dialogue\n  ) %&gt;% \n  udpipe::udpipe(udmodel_english)\n\n\n# Define environment\nreticulate::use_virtualenv(\"r-spacyr\")\n\n# Initialize\n# spacyr::spacy_download_langmodel(\"en_core_web_sm\", force = TRUE)\nspacyr::spacy_initialize(\"en_core_web_sm\")\n\n# Parse text\ntranscripts_corpora$spacyr &lt;- transcripts_corpora$corp %&gt;% \n    spacyr::spacy_parse(.,\n        tag = TRUE,\n        pos = TRUE,\n        lemma = TRUE,\n        entity = TRUE,\n        dependency = TRUE,\n        nounphrase = TRUE,\n        multithread = TRUE,\n        additional_attributes = c(\n          \"is_punct\"\n        )\n    )",
    "crumbs": [
      "Data collection",
      "Corpus: Transcripts"
    ]
  },
  {
    "objectID": "data_collection/01_22-corpora_transcripts-creation.html#save-data",
    "href": "data_collection/01_22-corpora_transcripts-creation.html#save-data",
    "title": "Corpus: Transcripts",
    "section": "Save data",
    "text": "Save data\n\n# Save complete data\nqs::qsave(\n    transcripts_corpora,\n    file = here(\"local_data/transcripts-corpora_full.qs\")\n)\n\n# Save udpipe corpus\nqs::qsave(\n    transcripts_corpora$udpipe, \n    file = here(\"local_data/transcripts-corpus_udpipe.qs\")\n)\n\n# Save spacyr corpus\nqs::qsave(\n    transcripts_corpora$spacyr, \n    file = here(\"local_data/transcripts-corpus_spacyr.qs\")\n)",
    "crumbs": [
      "Data collection",
      "Corpus: Transcripts"
    ]
  },
  {
    "objectID": "data_collection/02_02-qc-chat-badges.html",
    "href": "data_collection/02_02-qc-chat-badges.html",
    "title": "Quality Control: Chat Badges",
    "section": "",
    "text": "Information\n\n\n\n\nThe document sets up the session by sourcing a setup script.\nIt imports chat data from a local file.\nIt creates a list of user badges from the chat data.\nIt provides a link to example images of the badges.\nIt analyzes the frequency and distribution of badges per message.",
    "crumbs": [
      "Data collection",
      "Quality Control: Chat Badges"
    ]
  },
  {
    "objectID": "data_collection/02_02-qc-chat-badges.html#preparation",
    "href": "data_collection/02_02-qc-chat-badges.html#preparation",
    "title": "Quality Control: Chat Badges",
    "section": "Preparation",
    "text": "Preparation\n\nsource(here::here(\"data_collection/00_02-setup-session.R\"))\n\n\n# Load data\nchat &lt;- qs::qread(here(\"local_data/chat-debates_full.qs\"))",
    "crumbs": [
      "Data collection",
      "Quality Control: Chat Badges"
    ]
  },
  {
    "objectID": "data_collection/02_02-qc-chat-badges.html#create-list-of-badges",
    "href": "data_collection/02_02-qc-chat-badges.html#create-list-of-badges",
    "title": "Quality Control: Chat Badges",
    "section": "Create list of badges",
    "text": "Create list of badges\n\nbadges &lt;- chat$correct %&gt;%\n    select(message_id, user_badges) %&gt;%\n    unnest(user_badges) %&gt;%\n    mutate(\n        badge_name = sapply(user_badges, function(badge) badge$name),\n    ) %&gt;%\n    unnest(badge_name)\n\n\n\n\n\n\n\nNote\n\n\n\nFor example images of the emojis visit https://twitchinsights.net/badges\n\n\n\nbadges %&gt;% \n    frq(badge_name)\n\nbadge_name &lt;character&gt; \n# total N=677804 valid N=677804 mean=34.00 sd=8.99\n\nValue                                        |      N | Raw % | Valid % | Cum. %\n--------------------------------------------------------------------------------\nambassador                                   |      9 |  0.00 |    0.00 |   0.00\nanomaly_warzone_earth_1                      |     19 |  0.00 |    0.00 |   0.00\nbattlerite_1                                 |    259 |  0.04 |    0.04 |   0.04\nbits                                         |  11124 |  1.64 |    1.64 |   1.68\nbits_charity                                 |   5246 |  0.77 |    0.77 |   2.46\nbits_leader                                  |     41 |  0.01 |    0.01 |   2.46\nbroadcaster                                  |    724 |  0.11 |    0.11 |   2.57\nchatter_cs_go_2022                           |   1443 |  0.21 |    0.21 |   2.78\ncreator_cs_go_2022                           |      1 |  0.00 |    0.00 |   2.78\ncuphead_1                                    |     10 |  0.00 |    0.00 |   2.78\ndestiny_2_final_shape_raid_race              |   7713 |  1.14 |    1.14 |   3.92\ndestiny_2_the_final_shape_streamer           |    376 |  0.06 |    0.06 |   3.98\ndreamcon_2024                                |   1026 |  0.15 |    0.15 |   4.13\neso_1                                        |     25 |  0.00 |    0.00 |   4.13\nfirewatch_1                                  |      5 |  0.00 |    0.00 |   4.13\nfounder                                      |    356 |  0.05 |    0.05 |   4.19\ngame_developer                               |     96 |  0.01 |    0.01 |   4.20\nglhf_pledge                                  |   8441 |  1.25 |    1.25 |   5.45\nglitchcon2020                                |  19012 |  2.80 |    2.80 |   8.25\ngold_pixel_heart                             |   5194 |  0.77 |    0.77 |   9.02\nH1Z1_1                                       |    107 |  0.02 |    0.02 |   9.03\nhype_train                                   |   4200 |  0.62 |    0.62 |   9.65\nla_velada_iv                                 |   2628 |  0.39 |    0.39 |  10.04\nminecraft_15th_anniversary_celebration       |   2804 |  0.41 |    0.41 |  10.45\nmoderator                                    |  19200 |  2.83 |    2.83 |  13.29\nno_audio                                     |  18780 |  2.77 |    2.77 |  16.06\nno_video                                     |  13104 |  1.93 |    1.93 |  17.99\noverwatch_league_insider_1                   |    376 |  0.06 |    0.06 |  18.05\noverwatch_league_insider_2018B               |    226 |  0.03 |    0.03 |  18.08\noverwatch_league_insider_2019A               |   1451 |  0.21 |    0.21 |  18.29\npartner                                      |  15617 |  2.30 |    2.30 |  20.60\npredictions                                  |  11701 |  1.73 |    1.73 |  22.32\npremium                                      | 193824 | 28.60 |   28.60 |  50.92\nraging_wolf_helm                             |  88083 | 13.00 |   13.00 |  63.92\nrplace_2023                                  |   7954 |  1.17 |    1.17 |  65.09\nstaff                                        |    158 |  0.02 |    0.02 |  65.11\nstreamer_awards_2024                         |   2321 |  0.34 |    0.34 |  65.45\nsub_gift_leader                              |    262 |  0.04 |    0.04 |  65.49\nsub_gifter                                   |  15331 |  2.26 |    2.26 |  67.76\nsubscriber                                   | 106973 | 15.78 |   15.78 |  83.54\nsubtember_2024                               |   7010 |  1.03 |    1.03 |  84.57\nsuperultracombo_2023                         |  15703 |  2.32 |    2.32 |  86.89\nthe_game_awards_2023                         |    242 |  0.04 |    0.04 |  86.92\nthe_golden_predictor_of_the_game_awards_2023 |      4 |  0.00 |    0.00 |  86.92\nturbo                                        |  27778 |  4.10 |    4.10 |  91.02\ntwitch_dj                                    |    365 |  0.05 |    0.05 |  91.08\ntwitch_recap_2023                            |  53608 |  7.91 |    7.91 |  98.99\ntwitchcon_2024___rotterdam                   |    289 |  0.04 |    0.04 |  99.03\ntwitchcon_2024___san_diego                   |   1802 |  0.27 |    0.27 |  99.29\ntwitchcon2017                                |    304 |  0.04 |    0.04 |  99.34\ntwitchcon2018                                |    353 |  0.05 |    0.05 |  99.39\ntwitchconAmsterdam2020                       |    122 |  0.02 |    0.02 |  99.41\ntwitchconEU2019                              |    129 |  0.02 |    0.02 |  99.43\ntwitchconEU2022                              |    197 |  0.03 |    0.03 |  99.46\ntwitchconEU2023                              |     62 |  0.01 |    0.01 |  99.47\ntwitchconNA2019                              |    282 |  0.04 |    0.04 |  99.51\ntwitchconNA2022                              |    997 |  0.15 |    0.15 |  99.66\ntwitchconNA2023                              |    363 |  0.05 |    0.05 |  99.71\nvip                                          |    723 |  0.11 |    0.11 |  99.82\nzevent_2024                                  |   1251 |  0.18 |    0.18 | 100.00\n&lt;NA&gt;                                         |      0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;",
    "crumbs": [
      "Data collection",
      "Quality Control: Chat Badges"
    ]
  },
  {
    "objectID": "data_collection/02_02-qc-chat-badges.html#distribution-of-badges-per-message",
    "href": "data_collection/02_02-qc-chat-badges.html#distribution-of-badges-per-message",
    "title": "Quality Control: Chat Badges",
    "section": "Distribution of badges per message",
    "text": "Distribution of badges per message\n\nbadges %&gt;% \n    group_by(message_id) %&gt;% \n    summarise( n = n()) %&gt;% \n    describe_distribution()\n\nVariable | Mean |   SD | IQR |        Range | Skewness | Kurtosis |      n | n_Missing\n--------------------------------------------------------------------------------------\nn        | 1.16 | 0.38 |   0 | [1.00, 3.00] |     2.05 |     2.94 | 582419 |         0",
    "crumbs": [
      "Data collection",
      "Quality Control: Chat Badges"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Digital Behavioral Data",
    "section": "",
    "text": "Note\n\n\n\nThis page provides an outline of the topics, contents, and assignments for the semester. Please note that the contents of the course sessions ( background information,  slides,  exercises and  tutorials) will be updated continuously throughout the semester, with all changes being documented here.",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "index.html#copyright",
    "href": "index.html#copyright",
    "title": "Digital Behavioral Data",
    "section": "Copyright",
    "text": "Copyright\nThis content is licensed under a GPL-3.0 License.",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "slides/slides-07.html#seminarplan",
    "href": "slides/slides-07.html#seminarplan",
    "title": "🔨 Text as data in R",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\nSession\nDatum\nTopic\nPresenter\n\n\n\n\n\n📂 Block 1\nIntroduction\n\n\n\n1\n23.10.2024\nKick-Off\nChristoph Adrian\n\n\n2\n30.10.2024\nDBD: Overview & Introduction\nChristoph Adrian\n\n\n3\n06.11.2024\n🔨 Introduction to working with R\nChristoph Adrian\n\n\n\n📂 Block 2\nTheoretical Background: Twitch & TV Election Debates\n\n\n\n4\n13.11.2024\n📚 Twitch-Nutzung im Fokus\nStudent groups\n\n\n5\n20.11.2024\n📚 (Wirkungs-)Effekte von Twitch & TV-Debatten\nStudent groups\n\n\n6\n27.11.2024\n📚 Politische Debatten & Social Media\nStudent groups\n\n\n\n📂 Block 3\nMethod: Natural Language Processing\n\n\n\n7\n04.12.2024\n🔨 Text as data I: Introduction\nChristoph Adrian\n\n\n8\n11.12.2024\n🔨 Text as data II: Advanced Methods\nChristoph Adrian\n\n\n9\n18.12.2024\n🔨 Advanced Method I: Topic Modeling\nChristoph Adrian\n\n\n\nNo lecture\n🎄Christmas Break\n\n\n\n10\n08.01.2025\n🔨 Advanced Method II: Machine Learning\nChristoph Adrian\n\n\n\n📂 Block 4\nProject Work\n\n\n\n11\n15.01.2025\n🔨 Project work\nStudent groups\n\n\n12\n22.01.2025\n🔨 Project work\nStudent groups\n\n\n13\n29.01.2025\n📊 Project Presentation I\nStudent groups (TBD)\n\n\n14\n05.02.2025\n📊 Project Presentation & 🏁 Evaluation\nStudentds (TBD) & Christoph Adrian"
  },
  {
    "objectID": "slides/slides-07.html#wahldebatte-am-digitalen-lagerfeuer",
    "href": "slides/slides-07.html#wahldebatte-am-digitalen-lagerfeuer",
    "title": "🔨 Text as data in R",
    "section": "Wahldebatte am digitalen Lagerfeuer",
    "text": "Wahldebatte am digitalen Lagerfeuer\nWas wir (bisher) aus der Literatur gelernt haben\n\nWahldebatten sind ein spezifischer Teil des politischen Diskurses und haben Einfluss auf Emotionen, Einstellungen und Handlungen von Menschen\nSoziale Medien haben die Kommunikation und Interaktion zwischen Politikern und Bürgern verändert\n Twitch ist eine Social Networking Site (SNS) mit besonderen Eigenschaften (z.B. “Live”-Aspekt & die Bedeutung der community) & zunehmend Ort für politische Diskurse"
  },
  {
    "objectID": "slides/slides-07.html#who-are-we-looking-at",
    "href": "slides/slides-07.html#who-are-we-looking-at",
    "title": "🔨 Text as data in R",
    "section": "Who are we looking at?",
    "text": "Who are we looking at?\nÜberblick über verschiedenen Statistiken der betrachteten Streamer\n\n\nExpand for full code\nstreamer_stats &lt;- qs::qread(here(\"local_data/twitch_streamer_stats.qs\"))\n\nstreamer_stats %&gt;% \n  pivot_longer(cols = c(avg_viewers, followers, hours_streamed), names_to = \"statistic\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = month, y = value, fill = streamer)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_grid(statistic ~ ., scales = \"free_y\", labeller = as_labeller(c(\n    avg_viewers = \"Average Viewers\",\n    followers = \"Followers\",\n    hours_streamed = \"Hours Streamed\"))) +\n  theme_minimal() +\n  labs(\n    x = \"Month\",\n    y = \"\",\n    title = \"Streamer Statistics Over Time\", \n    fill = \"Streamer\") +\n  scale_y_continuous(labels = scales::comma) +\n  ggsci::scale_fill_cosmic()"
  },
  {
    "objectID": "slides/slides-07.html#wie-wurden-die-daten-erhoben",
    "href": "slides/slides-07.html#wie-wurden-die-daten-erhoben",
    "title": "🔨 Text as data in R",
    "section": "Wie wurden die Daten erhoben?",
    "text": "Wie wurden die Daten erhoben?\nÜberblick über den Prozess der Datenerhebung\n\nErhebung des Live-Stream-Chats mit  Paket chat_downloader\nDownload der Twitch & TV VoDs mit dem  Paket twitch-dl\nTranskription der Streams & der Debatte mit AI-based Transkriptionstool NoScribe\n\nHerausforderungen\n\nEingeschränkte  Twitch API ➜ nicht alle (ursprünglich verfügbaren) Informationen abrufbar\nLimitierte Verfügbarkeit der VoDs auf  Twitch ➜ Re-Upload auf unserem  YouTube-Kanal (ungelistet)\nQualität der Transkription ist gut, Identifkation der sprechenden Person(en) ausbaufähig"
  },
  {
    "objectID": "slides/slides-07.html#welche-daten-stehen-zur-verfügung",
    "href": "slides/slides-07.html#welche-daten-stehen-zur-verfügung",
    "title": "🔨 Text as data in R",
    "section": "Welche Daten stehen zur Verfügung?",
    "text": "Welche Daten stehen zur Verfügung?\nÜberblick über die Daten\n\n\nLinks zu den (Uploads der) VoDs auf StudOn im Ordner Kursmaterialien/VODs\nDatensatz chats.qs (& Dokumentation) mit den Chatnachrichten aller Live-Streams ( hasanabi,  zackrawrr und | TheMajorityReport)\nDatensatz transcripts.qs (& Dokumentation) mit den Transkripten der TV-Debatten (Presidential auf ABC, Vice Presidential auf CBS) & aller Live-Streams ( hasanabi,  zackrawrr und | TheMajorityReport)\nDictionary dictionary_chat_emotes.RDS mit den Emojis und Emotes, die in den hier analyisierten  Twitch &  YouTube-Chats verwendet wurden"
  },
  {
    "objectID": "slides/slides-07.html#chats-als-rohform-von-dbd",
    "href": "slides/slides-07.html#chats-als-rohform-von-dbd",
    "title": "🔨 Text as data in R",
    "section": "Chats als Rohform von DBD",
    "text": "Chats als Rohform von DBD\nKurzer Überblick über den chats-Datensatz\n\nchats %&gt;% glimpse \n\nRows: 913,375\nColumns: 33\n$ streamer              &lt;chr&gt; \"hasanabi\", \"hasanabi\", \"hasanabi\", \"hasanabi\", …\n$ url                   &lt;chr&gt; \"https://www.twitch.tv/videos/2247664726\", \"http…\n$ platform              &lt;chr&gt; \"twitch\", \"twitch\", \"twitch\", \"twitch\", \"twitch\"…\n$ debate                &lt;chr&gt; \"presidential\", \"presidential\", \"presidential\", …\n$ user_name             &lt;chr&gt; \"bendaspur\", \"spackle_pirate\", \"texaschollima\", …\n$ user_id               &lt;chr&gt; \"54058406\", \"182041182\", \"185502300\", \"159018462…\n$ user_display_name     &lt;chr&gt; \"BenDaSpur\", \"spackle_pirate\", \"TexasChollima\", …\n$ user_badges           &lt;list&gt; [], [], [], [[\"twitch_recap_2023\", 1, \"Twitch R…\n$ message_timestamp     &lt;dbl&gt; 19, 19, 20, 20, 21, 21, 22, 22, 24, 25, 25, 25, …\n$ message_id            &lt;chr&gt; \"dc03b89a-722d-4eaa-a895-736533a68aca\", \"6be50e1…\n$ message_type          &lt;chr&gt; \"text_message\", \"text_message\", \"text_message\", …\n$ message_content       &lt;chr&gt; \"60fps LETSGO 60fps LETSGO 60fps LETSGO 60fps LE…\n$ message_emotes        &lt;list&gt; [], [], [], [], [], [], [], [], [], [], [], [[\"…\n$ message_length        &lt;int&gt; 51, 17, 20, 27, 35, 14, 20, 5, 10, 9, 106, 97, 3…\n$ message_timecode      &lt;Period&gt; 19S, 19S, 20S, 20S, 21S, 21S, 22S, 22S, 24S, …\n$ message_time          &lt;chr&gt; \"00:00:19\", \"00:00:19\", \"00:00:20\", \"00:00:20\", …\n$ message_during_debate &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_has_badge        &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, …\n$ user_is_premium       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, …\n$ user_is_subscriber    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ user_is_turbo         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_moderator     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …\n$ user_is_partner       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …\n$ user_is_subgifter     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_broadcaster   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_vip           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_twitchdj      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_founder       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_staff         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_game_dev      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_ambassador    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_no_audio         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_no_video         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …"
  },
  {
    "objectID": "slides/slides-07.html#möglichkeite-zum-data-linking-mit-transkripten",
    "href": "slides/slides-07.html#möglichkeite-zum-data-linking-mit-transkripten",
    "title": "🔨 Text as data in R",
    "section": "Möglichkeite zum Data Linking mit Transkripten",
    "text": "Möglichkeite zum Data Linking mit Transkripten\nKurzer Überblick über den transcripts-Datensatz\n\ntranscripts %&gt;% glimpse \n\nRows: 5,861\nColumns: 12\n$ id_sequence            &lt;chr&gt; \"p1_s0001\", \"p1_s0002\", \"p1_s0003\", \"p1_s0004\",…\n$ source                 &lt;chr&gt; \"presidential_debate-abc\", \"presidential_debate…\n$ speaker                &lt;chr&gt; \"S27\", \"S35\", \"S27\", \"S55\", \"S61\", \"S55\", \"S43\"…\n$ timestamp              &lt;time&gt; 00:00:00, 00:00:11, 00:00:20, 00:00:34, 00:00:…\n$ dialogue               &lt;chr&gt; \"Tonight, the high-stakes showdown here in Phil…\n$ dialogue_length        &lt;int&gt; 229, 148, 245, 91, 31, 13, 37, 102, 316, 409, 6…\n$ duration               &lt;dbl&gt; 11, 9, 14, 6, 4, 1, 4, 10, 17, 21, 28, 8, 13, 4…\n$ debate                 &lt;chr&gt; \"presidential\", \"presidential\", \"presidential\",…\n$ streamer               &lt;chr&gt; \"tv_station\", \"tv_station\", \"tv_station\", \"tv_s…\n$ id_streamer            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ id_speaker             &lt;chr&gt; \"p1_s27\", \"p1_s35\", \"p1_s27\", \"p1_s55\", \"p1_s61…\n$ sequence_during_debate &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…"
  },
  {
    "objectID": "slides/slides-07.html#where-the-magic-happends",
    "href": "slides/slides-07.html#where-the-magic-happends",
    "title": "🔨 Text as data in R",
    "section": "Where the “magic” happends",
    "text": "Where the “magic” happends\nAutomatisierte Inhaltsanalyse: Definition und Ablauf\n\nAutomatisierte Inhaltsanalyse beschreibt die automatisierte (z. B. via Programmierskript) Analyse von Inhalten (z. B. Text, Bilder). Dabei unterstützen Forschende/manuelle Codierer:innen, etwa durch die Validierung von Ergebnissen. (Hase, 2023)\n\nAber:\n\nAutomatisierte Methoden “augment humans, not replace them” (Grimmer & Stewart, 2013, p. S.270)\n„English before everything“ (Baden et al., 2022, p. S.9)\n(Systematische) Fehler: “All quantitative models of language are wrong — but some are useful” (Grimmer & Stewart, 2013, p. S.269)"
  },
  {
    "objectID": "slides/slides-07.html#workflow-demystified",
    "href": "slides/slides-07.html#workflow-demystified",
    "title": "🔨 Text as data in R",
    "section": "Workflow, demystified",
    "text": "Workflow, demystified\nTypische Schritte der automatisierten Inhaltsanalyse"
  },
  {
    "objectID": "slides/slides-07.html#building-a-shared-vocabulary",
    "href": "slides/slides-07.html#building-a-shared-vocabulary",
    "title": "🔨 Text as data in R",
    "section": "Building a shared vocabulary",
    "text": "Building a shared vocabulary\nGrundbegriffe und Terminologien I\n\n\nToken: A token is a string with a known meaning, and a token may be a word, number or just characters like punctuation. “Hello”, “123”, and “-” are some examples of tokens.\nSentence: A sentence is a group of tokens that is complete in meaning. “The weather looks good” is an example of a sentence, and the tokens of the sentence are [“The”, “weather”, “looks”, “good].\nParagraph: A paragraph is a collection of sentences or phrases, and a sentence can alternatively be viewed as a token of a paragraph.\nDocuments: A document might be a sentence, a paragraph, or a set of paragraphs. A text message sent to an individual is an example of a document.\nCorpus: A corpus is typically an extensive collection of documents as a Bag-of-words. A corpus comprises each word’s id and frequency count in each record. An example of a corpus is a collection of emails or text messages sent to a particular person."
  },
  {
    "objectID": "slides/slides-07.html#building-a-shared-vocabulary-1",
    "href": "slides/slides-07.html#building-a-shared-vocabulary-1",
    "title": "🔨 Text as data in R",
    "section": "Building a shared vocabulary",
    "text": "Building a shared vocabulary\nGrundbegriffe und Terminologien II\n\nLemma: Die Grundform eines Wortes. Zum Beispiel ist “run”” das Lemma von “running”“.\nStoppwörter: Wörter, die in der Regel keine inhaltliche Bedeutung haben und daher aus dem Text entfernt werden, z.B. “and”, “or” & “the”.\nParts of speech (POS): Linguistische Marker, die die allgemeine Kategorie einer sprachlichen Eigenschaft eines Wortes anzeigen, z.B. Nomen, Verb, Adjektiv usw.\nNamed entities: Ein reales Objekt, wie Personen, Orte, Organisationen, Produkte usw., das mit einem Eigennamen bezeichnet werden kann, z.B. “Donald Trump“ oder „Vereinigte Staaten”.\nMulti-word expressions: Wortfolgen, die ein einzelnes Konzept bezeichnen (und im Deutschen wären), z.B. “Mehrwertsteuer” (im Englischen: “value added tax”)."
  },
  {
    "objectID": "slides/slides-07.html#was-ist-preprocessing-und-warum-ist-es-wichtig",
    "href": "slides/slides-07.html#was-ist-preprocessing-und-warum-ist-es-wichtig",
    "title": "🔨 Text as data in R",
    "section": "Was ist Preprocessing und warum ist es wichtig?",
    "text": "Was ist Preprocessing und warum ist es wichtig?\nTypische Schritte des Preprocessings\n\nReduziert die Komplexität von Textdaten, ohne deren substanzielle Bedeutung zu minimieren\nUmfasst die Bereinigung (Reduzierung von systematischen Fehlern, z.B. Encoding) & Normalisierung (Texte über Dokumente, Sprache, Plattformen, etc. vergleichbar machen)\nTypische Bestandteile der Normalisierung sind Tokenisierung, Kleinschreibung, Entfernung von Stoppwörtern, Satz- &Sonderzeichen, Lemmatisierung/Stemming und “Pruning” (häufige/seltene Features entfernen)"
  },
  {
    "objectID": "slides/slides-07.html#abwägungen-beim-preprocessing",
    "href": "slides/slides-07.html#abwägungen-beim-preprocessing",
    "title": "🔨 Text as data in R",
    "section": "Abwägungen beim Preprocessing",
    "text": "Abwägungen beim Preprocessing\nHerausforderungen und Konsequenzen der Entscheidungen beim Preprocessing\n\nOft verändert Kleinschreibung die Bedeutung von Features nicht, aber es gibt Ausnahmen (“Bild” vs. BILD-Zeitung)\nOft sind Sonderzeichen (z.B. Satzzeichen) nicht von substanzieller Bedeutung, aber es gibt Ausnahmen (z.B. #metoo, G7, Emojis)\nStoppwörter sind stark kontextabhängig! ➜ oft ist es sinnvoll, eigene “organische” Stoppwortlisten zu erstellen\nLemmatisierung (“running” “ran” ➜ “run”) häufig “besser”, Stemming (“running” “ran” ➜ “run” “ran”) häufig schneller\nReihenfolge des Preprocessings kann Ergebnisse beeinflussen (z.B. Entfernung von Stoppwörtern vor oder nach Lemmatisierung)"
  },
  {
    "objectID": "slides/slides-07.html#wenn-aus-wörtern-zahlen-werden",
    "href": "slides/slides-07.html#wenn-aus-wörtern-zahlen-werden",
    "title": "🔨 Text as data in R",
    "section": "Wenn aus Wörtern Zahlen werden",
    "text": "Wenn aus Wörtern Zahlen werden\nDie einfachste Form der Textrepräsentation: bag-of-words\n\nDamit Computer Text verstehen bzw. verarbeiten können, muss der Text in ein numerisches Format umgewandelt werden\nEine einfache und weit verbreitete Methode zur Textrepräsentation in der natürlichen Sprachverarbeitung (NLP) ist das bag-of-words Modell\n\nrepräsentiert einen Text (z.B. einen Satz oder ein Dokument) als eine Sammlung von Wörtern, ohne Berücksichtigung der Reihenfolge oder Grammatik\nAnnahme: Reihenfolge und Kontext von Wörtern haben keinen Einfluss auf Ihre Bedeutung"
  },
  {
    "objectID": "slides/slides-07.html#ugly-but-efficient",
    "href": "slides/slides-07.html#ugly-but-efficient",
    "title": "🔨 Text as data in R",
    "section": "Ugly, but efficient",
    "text": "Ugly, but efficient\nWarum die bag-of-words Annahme problematisch ist …\n\nPolysemie: Fliege (Tier & Kleidungsstück), “Maus” (Tier & Computerzubehör)\nVerneinung: “Nicht schlecht!”\nNamed entities: “Olaf Scholz”, “Vereinigte Staaten”\nWörter mit ähnlichen Bedeutungen: “Gemüse” & “Grünzeug”\n\n… und doch so häufig verwendet wird\n\nschnell, resourcenschonend und “robust”\nleichte Anpassung bzw. Erweiterung steigern Aussagekraft"
  },
  {
    "objectID": "slides/slides-07.html#beyond-bag-of-words",
    "href": "slides/slides-07.html#beyond-bag-of-words",
    "title": "🔨 Text as data in R",
    "section": "Beyond “bag-of-words”",
    "text": "Beyond “bag-of-words”\nText-as-Data Repräsentationen, die Reihenfolge und Kontext berücksichtigen\n\nNgram-basierte Repräsentation (z. B. Collocations & Keywords-in-Context)\nSyntax-basierte Repräsentation (z. B. Part-of-Speech Tagging & Dependency Parsing)\nVektor-basierte Repräsentation in semantischen, n-dimensionalen Räumen (z. B. Word Embeddings)"
  },
  {
    "objectID": "slides/slides-07.html#flexible-for-power-users-simple-for-beginners",
    "href": "slides/slides-07.html#flexible-for-power-users-simple-for-beginners",
    "title": "🔨 Text as data in R",
    "section": "Flexible for power users, simple for beginners",
    "text": "Flexible for power users, simple for beginners\nHintergrund zu Paket & Projekt quanteda (Benoit et al., 2018)\n\nquanteda ist ein umfassendes R-Paket für die Textverarbeitung und Textanalyse\nSehr aktives Open-Source-Projekt mit umfangreicher Dokumentation und Community-Support\nBritische gemeinnützige Organisation, die sich der Förderung von Open-Source-Software für die Textanalyse widmet\nAlternative: tidytext(Silge & Robinson, 2017)"
  },
  {
    "objectID": "slides/slides-07.html#grundlage-ist-immer-das-korpus",
    "href": "slides/slides-07.html#grundlage-ist-immer-das-korpus",
    "title": "🔨 Text as data in R",
    "section": "Grundlage ist immer das Korpus",
    "text": "Grundlage ist immer das Korpus\nArbeiten mit quanteda: corpus\n\n\n# Create corpus\ncorp_transcripts &lt;- transcripts %&gt;% \n  quanteda::corpus(\n    docid_field = \"id_sequence\", \n    text_field = \"dialogue\"\n  )\n\n# Output\ncorp_transcripts\n\n\nCorpus consisting of 5,861 documents and 10 docvars.\np1_s0001 :\n\"Tonight, the high-stakes showdown here in Philadelphia betwe...\"\n\np1_s0002 :\n\"A historic race for president upended just weeks ago, Presid...\"\n\np1_s0003 :\n\"The candidates separated by the smallest of margins, essenti...\"\n\np1_s0004 :\n\"This is an ABC News special. The most consequential moment o...\"\n\np1_s0005 :\n\"Together, we'll chart a... (..)\"\n\np1_s0006 :\n\"Donald Trump.\"\n\n[ reached max_ndoc ... 5,855 more documents ]"
  },
  {
    "objectID": "slides/slides-07.html#einfache-tokenisierung",
    "href": "slides/slides-07.html#einfache-tokenisierung",
    "title": "🔨 Text as data in R",
    "section": "Einfache Tokenisierung …",
    "text": "Einfache Tokenisierung …\nEinfluss der Preporcessing-Schritte am Beispiel (I)\n\n# Tokenize corpus\ntoks_simple &lt;- corp_transcripts %&gt;% \n  quanteda::tokens() \n\n# Output\nhead(toks_simple[[1]], 100)\n\n [1] \"Tonight\"      \",\"            \"the\"          \"high-stakes\"  \"showdown\"    \n [6] \"here\"         \"in\"           \"Philadelphia\" \"between\"      \"Vice\"        \n[11] \"President\"    \"Kamala\"       \"Harris\"       \"and\"          \"former\"      \n[16] \"President\"    \"Donald\"       \"Trump\"        \".\"            \"Their\"       \n[21] \"first\"        \"face-to-face\" \"meeting\"      \"in\"           \"this\"        \n[26] \"presidential\" \"election\"     \",\"            \"their\"        \"first\"       \n[31] \"face-to-face\" \"meeting\"      \"ever\"         \".\""
  },
  {
    "objectID": "slides/slides-07.html#mit-entfernung-von-satz--und-sonderzeichen",
    "href": "slides/slides-07.html#mit-entfernung-von-satz--und-sonderzeichen",
    "title": "🔨 Text as data in R",
    "section": "… mit Entfernung von Satz- und Sonderzeichen …",
    "text": "… mit Entfernung von Satz- und Sonderzeichen …\nEinfluss der Preporcessing-Schritte am Beispiel (II)\n\ntoks_nopunct &lt;- corp_transcripts %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  )\n\nhead(toks_nopunct[[1]], 100)\n\n [1] \"Tonight\"      \"the\"          \"high-stakes\"  \"showdown\"     \"here\"        \n [6] \"in\"           \"Philadelphia\" \"between\"      \"Vice\"         \"President\"   \n[11] \"Kamala\"       \"Harris\"       \"and\"          \"former\"       \"President\"   \n[16] \"Donald\"       \"Trump\"        \"Their\"        \"first\"        \"face-to-face\"\n[21] \"meeting\"      \"in\"           \"this\"         \"presidential\" \"election\"    \n[26] \"their\"        \"first\"        \"face-to-face\" \"meeting\"      \"ever\""
  },
  {
    "objectID": "slides/slides-07.html#und-ohne-stopwörter",
    "href": "slides/slides-07.html#und-ohne-stopwörter",
    "title": "🔨 Text as data in R",
    "section": "… und ohne Stopwörter",
    "text": "… und ohne Stopwörter\nEinfluss der Preporcessing-Schritte am Beispiel (III)\n\ntoks_nostopw &lt;- corp_transcripts %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  ) %&gt;% \n  quanteda::tokens_remove(\n    pattern = quanteda::stopwords(\"en\")\n  )\n\nhead(toks_nostopw[[1]], 100)\n\n [1] \"Tonight\"      \"high-stakes\"  \"showdown\"     \"Philadelphia\" \"Vice\"        \n [6] \"President\"    \"Kamala\"       \"Harris\"       \"former\"       \"President\"   \n[11] \"Donald\"       \"Trump\"        \"first\"        \"face-to-face\" \"meeting\"     \n[16] \"presidential\" \"election\"     \"first\"        \"face-to-face\" \"meeting\"     \n[21] \"ever\""
  },
  {
    "objectID": "slides/slides-07.html#direkter-vergleich",
    "href": "slides/slides-07.html#direkter-vergleich",
    "title": "🔨 Text as data in R",
    "section": "Direkter Vergleich",
    "text": "Direkter Vergleich\nEinfluss der Preporcessing-Schritte am Beispiel (IV)\n\nhead(toks_simple[[1]], 100)\n\n [1] \"Tonight\"      \",\"            \"the\"          \"high-stakes\"  \"showdown\"    \n [6] \"here\"         \"in\"           \"Philadelphia\" \"between\"      \"Vice\"        \n[11] \"President\"    \"Kamala\"       \"Harris\"       \"and\"          \"former\"      \n[16] \"President\"    \"Donald\"       \"Trump\"        \".\"            \"Their\"       \n[21] \"first\"        \"face-to-face\" \"meeting\"      \"in\"           \"this\"        \n[26] \"presidential\" \"election\"     \",\"            \"their\"        \"first\"       \n[31] \"face-to-face\" \"meeting\"      \"ever\"         \".\"           \n\nhead(toks_nopunct[[1]], 100)\n\n [1] \"Tonight\"      \"the\"          \"high-stakes\"  \"showdown\"     \"here\"        \n [6] \"in\"           \"Philadelphia\" \"between\"      \"Vice\"         \"President\"   \n[11] \"Kamala\"       \"Harris\"       \"and\"          \"former\"       \"President\"   \n[16] \"Donald\"       \"Trump\"        \"Their\"        \"first\"        \"face-to-face\"\n[21] \"meeting\"      \"in\"           \"this\"         \"presidential\" \"election\"    \n[26] \"their\"        \"first\"        \"face-to-face\" \"meeting\"      \"ever\"        \n\nhead(toks_nostopw[[1]], 100)\n\n [1] \"Tonight\"      \"high-stakes\"  \"showdown\"     \"Philadelphia\" \"Vice\"        \n [6] \"President\"    \"Kamala\"       \"Harris\"       \"former\"       \"President\"   \n[11] \"Donald\"       \"Trump\"        \"first\"        \"face-to-face\" \"meeting\"     \n[16] \"presidential\" \"election\"     \"first\"        \"face-to-face\" \"meeting\"     \n[21] \"ever\""
  },
  {
    "objectID": "slides/slides-07.html#ngrams-für-mehr-kontext",
    "href": "slides/slides-07.html#ngrams-für-mehr-kontext",
    "title": "🔨 Text as data in R",
    "section": "Ngrams für mehr Kontext",
    "text": "Ngrams für mehr Kontext\nTokenisierung von Bi & Skipgrams\n\n# Bigrams\ntoks_nostopw %&gt;% \n  tokens_ngrams(n = 2) %&gt;% \n  .[[1]]\n\n [1] \"Tonight_high-stakes\"   \"high-stakes_showdown\"  \"showdown_Philadelphia\"\n [4] \"Philadelphia_Vice\"     \"Vice_President\"        \"President_Kamala\"     \n [7] \"Kamala_Harris\"         \"Harris_former\"         \"former_President\"     \n[10] \"President_Donald\"      \"Donald_Trump\"          \"Trump_first\"          \n[13] \"first_face-to-face\"    \"face-to-face_meeting\"  \"meeting_presidential\" \n[16] \"presidential_election\" \"election_first\"        \"first_face-to-face\"   \n[19] \"face-to-face_meeting\"  \"meeting_ever\"         \n\n\n\n# Skipgrams\ntoks_nostopw %&gt;% \n  tokens_ngrams(n = 2, skip = 0:1) %&gt;% \n  .[[1]]\n\n [1] \"Tonight_high-stakes\"       \"Tonight_showdown\"         \n [3] \"high-stakes_showdown\"      \"high-stakes_Philadelphia\" \n [5] \"showdown_Philadelphia\"     \"showdown_Vice\"            \n [7] \"Philadelphia_Vice\"         \"Philadelphia_President\"   \n [9] \"Vice_President\"            \"Vice_Kamala\"              \n[11] \"President_Kamala\"          \"President_Harris\"         \n[13] \"Kamala_Harris\"             \"Kamala_former\"            \n[15] \"Harris_former\"             \"Harris_President\"         \n[17] \"former_President\"          \"former_Donald\"            \n[19] \"President_Donald\"          \"President_Trump\"          \n[21] \"Donald_Trump\"              \"Donald_first\"             \n[23] \"Trump_first\"               \"Trump_face-to-face\"       \n[25] \"first_face-to-face\"        \"first_meeting\"            \n[27] \"face-to-face_meeting\"      \"face-to-face_presidential\"\n[29] \"meeting_presidential\"      \"meeting_election\"         \n[31] \"presidential_election\"     \"presidential_first\"       \n[33] \"election_first\"            \"election_face-to-face\"    \n[35] \"first_face-to-face\"        \"first_meeting\"            \n[37] \"face-to-face_meeting\"      \"face-to-face_ever\"        \n[39] \"meeting_ever\""
  },
  {
    "objectID": "slides/slides-07.html#welche-features-treten-häufig-nacheinander-auf",
    "href": "slides/slides-07.html#welche-features-treten-häufig-nacheinander-auf",
    "title": "🔨 Text as data in R",
    "section": "Welche Features treten häufig nacheinander auf?",
    "text": "Welche Features treten häufig nacheinander auf?\nKollokationen für Identifkation prominenter Bigramme\n\n\ntoks_nostopw %&gt;% \n  quanteda.textstats::textstat_collocations(\n    size = 2, \n    min_count = 5\n  ) %&gt;% \n  head(25)\n\n\n        collocation count count_nested length    lambda        z\n1         know know  1337            0      2  3.890787 98.31370\n2        saying bad   558            0      2  6.503305 81.19215\n3        bad saying   553            0      2  6.481795 80.98625\n4         going say   666            0      2  4.555737 80.92246\n5         say going   661            0      2  4.562963 80.82524\n6      donald trump   755            0      2  7.422847 75.19267\n7     kamala harris   494            0      2  7.873933 68.88003\n8    vice president   429            0      2  7.258104 56.72753\n9             oh oh   229            0      2  4.679549 54.95066\n10        right now   269            0      2  3.996328 53.27167\n11    senator vance   129            0      2  6.583164 49.48173\n12       little bit   132            0      2  8.268099 45.80914\n13 president harris   186            0      2  4.027681 45.69845\n14           oh god   154            0      2  6.175930 44.94423\n15        years ago   102            0      2  6.436807 43.06424\n16         tim walz    90            0      2  7.588398 43.05596\n17  president trump   203            0      2  3.468589 42.70406\n18       four years   100            0      2  6.381600 42.53221\n19      health care   136            0      2  7.400206 41.83123\n20      white house    85            0      2  8.071701 41.52827\n21   donald trump's   132            0      2  6.408658 41.05938\n22 former president   141            0      2  5.790480 41.04486\n23  curious curious   373            0      2 11.836611 40.77202\n24    governor walz    77            0      2  6.512020 39.65499\n25      two minutes    84            0      2  6.490910 39.39074"
  },
  {
    "objectID": "slides/slides-07.html#von-tokens-zur-dfm",
    "href": "slides/slides-07.html#von-tokens-zur-dfm",
    "title": "🔨 Text as data in R",
    "section": "Von Tokens zur DFM",
    "text": "Von Tokens zur DFM\nErklärung der Dokument-Feature-Matrix (DFM)\n\n\nSehr häufig genutzten Stukturen der “klassischen” Textverarbeitung mit folgende Merkmale:\n\njede Zeile ein Dokument (wie z.B. eine Chatnachricht oder eine Sprecher:innensequenz),\njede Spalte repräsentiert einen Begriff, und\njeder Wert enthält (typischerweise) die Anzahl der Häufigkeit dieses Begriffs in diesem Dokument.\n\n\n\n\n\n\n\n\n(Zheng & Casari, 2018)"
  },
  {
    "objectID": "slides/slides-07.html#überprüfung-häufigster-token",
    "href": "slides/slides-07.html#überprüfung-häufigster-token",
    "title": "🔨 Text as data in R",
    "section": "Überprüfung häufigster Token",
    "text": "Überprüfung häufigster Token\nAnwendung der DFM\n\n\n# Check top 25 features\ntoks_nostopw %&gt;%\n  quanteda::dfm() %&gt;% \n  quanteda.textstats::textstat_frequency(\n    n = 25) \n\n\n     feature frequency rank docfreq group\n1       like      6350    1    1617   all\n2       know      3872    2    1325   all\n3      think      2932    3    1202   all\n4     people      2710    4    1158   all\n5       yeah      2551    5    1107   all\n6      going      2364    6     956   all\n7       just      2145    7    1322   all\n8        say      1632    8     715   all\n9      right      1556    9     930   all\n10     trump      1492   10     859   all\n11       one      1461   11     958   all\n12 president      1372   12     787   all\n13       get      1320   13     850   all\n14      said      1262   14     822   all\n15       now      1222   15     883   all\n16      want      1207   16     755   all\n17       can      1137   17     723   all\n18    really      1137   17     620   all\n19        uh      1134   19     421   all\n20   fucking      1074   20     522   all\n21       lot      1049   21     632   all\n22    saying      1042   22     376   all\n23        oh      1003   23     546   all\n24      well       974   24     740   all\n25       bad       963   25     251   all"
  },
  {
    "objectID": "slides/slides-07.html#corpus-tokens-dfm",
    "href": "slides/slides-07.html#corpus-tokens-dfm",
    "title": "🔨 Text as data in R",
    "section": "Corpus ➞ ( Tokens ➞ DFM ) ⟳",
    "text": "Corpus ➞ ( Tokens ➞ DFM ) ⟳\nBeispiel für den Loop des (Pre-)Processing\n\n\n# Customize stopwords\ncustom_stopwords &lt;- c(\"uh\", \"oh\")\n\n# Remove custom stopwords\ntoks_no_custom_stopw &lt;- toks_nostopw %&gt;% \n  quanteda::tokens_remove(\n    pattern = custom_stopwords\n  )\n\n# Check top 25 features\ntoks_no_custom_stopw %&gt;%\n  quanteda::dfm() %&gt;% \n  quanteda.textstats::textstat_frequency(\n    n = 25) \n\n\n     feature frequency rank docfreq group\n1       like      6350    1    1617   all\n2       know      3872    2    1325   all\n3      think      2932    3    1202   all\n4     people      2710    4    1158   all\n5       yeah      2551    5    1107   all\n6      going      2364    6     956   all\n7       just      2145    7    1322   all\n8        say      1632    8     715   all\n9      right      1556    9     930   all\n10     trump      1492   10     859   all\n11       one      1461   11     958   all\n12 president      1372   12     787   all\n13       get      1320   13     850   all\n14      said      1262   14     822   all\n15       now      1222   15     883   all\n16      want      1207   16     755   all\n17       can      1137   17     723   all\n18    really      1137   17     620   all\n19   fucking      1074   19     522   all\n20       lot      1049   20     632   all\n21    saying      1042   21     376   all\n22      well       974   22     740   all\n23       bad       963   23     251   all\n24      mean       935   24     557   all\n25       way       905   25     572   all"
  },
  {
    "objectID": "slides/slides-07.html#welche-user-werden-am-häufigsten-erwähnt",
    "href": "slides/slides-07.html#welche-user-werden-am-häufigsten-erwähnt",
    "title": "🔨 Text as data in R",
    "section": "Welche User werden am häufigsten erwähnt?",
    "text": "Welche User werden am häufigsten erwähnt?\nBeispiele für Analysen auf Basis der DFM: Auswahl bestimmter Muster\n\n\n# Create corpus\ncorp_chats &lt;- chats %&gt;% \n  quanteda::corpus(\n    docid_field = \"message_id\", \n    text_field = \"message_content\"\n  )\n\n# Create DFM\ndfm_chats &lt;- corp_chats %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  ) %&gt;% \n  quanteda::dfm() \n\n# Output\ndfm_chats %&gt;% \n  quanteda::dfm_select(pattern = \"@*\") %&gt;% \n  quanteda.textstats::textstat_frequency(\n    n = 25) \n\n\n            feature frequency rank docfreq group\n1         @hasanabi     29173    1   28371   all\n2        @zackrawrr     11430    2   11381   all\n3       @gizmomacks       246    3     243   all\n4         @toxicsjw       167    4     167   all\n5       @beteljuice       158    5     158   all\n6       @megaphonix       154    6     150   all\n7            @hasan        76    7      76   all\n8  @depressedaether        68    8      68   all\n9     @nicebathroom        68    8      68   all\n10    @littlebear36        64   10      61   all\n11   @david_leonard        61   11      61   all\n12     @hasandpiker        58   12      58   all\n13      @sambarty2k        58   12      58   all\n14         @tiamani        55   14      55   all\n15     @matefeedart        50   15      50   all\n16           @wihby        47   16      47   all\n17      @freejam013        44   17      44   all\n18      @austinshow        42   18      42   all\n19         @mf_jewm        41   19      41   all\n20            @chat        41   19      33   all\n21    @lakemcgroove        37   21      37   all\n22            @mhud        37   21      37   all\n23  @thistwitchname        36   23      36   all\n24     @mangobreezy        35   24      35   all\n25        @mijnboot        35   24      35   all"
  },
  {
    "objectID": "slides/slides-07.html#gezielte-suche-nach-spezifischen-worten",
    "href": "slides/slides-07.html#gezielte-suche-nach-spezifischen-worten",
    "title": "🔨 Text as data in R",
    "section": "Gezielte Suche nach spezifischen Worten",
    "text": "Gezielte Suche nach spezifischen Worten\nHintergrund und Anwendung von Diktionären\n\nListen von Features, die ein bestimmtes Konstrukt (z.B. Emotionalisierung) beschreiben.\nIn der (klassischen) Diktionär-Analysen wird gezählt, wie häufig manifeste Features vorkommen, um darauf zu schliessen, inwiefern ein latentes Konstrukt vorkommt.\nVerschiedene Unterscheidungen:\n\nOff-the-shelf (z.B. LIWC, ANEW) vs. organische (eigene, domänenspezifische) Diktionäre\nBreite (möglichst umfassende Wortliste) vs. spezifische (möglichst spezifische Wortliste) Diktionäre"
  },
  {
    "objectID": "slides/slides-07.html#organisch-aber-datenbasiert",
    "href": "slides/slides-07.html#organisch-aber-datenbasiert",
    "title": "🔨 Text as data in R",
    "section": "Organisch, aber datenbasiert",
    "text": "Organisch, aber datenbasiert\nVorstellung des (erstellen) Emoji-Diktionärs\n\n# Load custom emoji-dictionary\ndict_chat_emotes &lt;- readRDS(here(\"local_data/dictionary_chat_emotes.RDS\"))\n\n# Output\ndict_chat_emotes\n\nDictionary object with 5546 key entries.\n- [0Unroll]:\n  - 0unroll\n- [1]:\n  - 1\n- [2020Celebrate]:\n  - 2020celebrate\n- [2020Forward]:\n  - 2020forward\n- [2020Glitchy]:\n  - 2020glitchy\n- [2020Pajamas]:\n  - 2020pajamas\n[ reached max_nkey ... 5,540 more keys ]"
  },
  {
    "objectID": "slides/slides-07.html#welche-emojis-werden-am-häufigsten-verwendet",
    "href": "slides/slides-07.html#welche-emojis-werden-am-häufigsten-verwendet",
    "title": "🔨 Text as data in R",
    "section": "Welche emojis werden am häufigsten verwendet?",
    "text": "Welche emojis werden am häufigsten verwendet?\nBeispiele für Analysen auf Basis der DFM: Dictionary\n\n\n# Lookup emojis in DFM of chats\ndfm_emotes &lt;- dfm_chats %&gt;% \n  quanteda::dfm_lookup(\n    dictionary = dict_chat_emotes)\n\n# Output frequency of emojis\ndfm_emotes %&gt;% \n  quanteda.textstats::textstat_frequency(\n    n = 25) \n\n\n         feature frequency rank docfreq group\n1            LUL     20194    1   14967   all\n2           hasL     12455    2    5856   all\n3    bleedPurple      5188    3    5174   all\n4          Kappa      4971    4    4240   all\n5        hasSlam      2989    5    1002   all\n6    NotLikeThis      2341    6    1354   all\n7             🇵🇸      1968    7     780   all\n8        hasChud      1792    8    1206   all\n9          hasHi      1401    9     851   all\n10          hasO      1375   10     609   all\n11       hasBoot      1209   11     551   all\n12       hasRaid      1092   12     470   all\n13      elbyBlom      1001   13    1001   all\n14       WutFace       964   14     687   all\n15     hasBaited       901   15     396   all\n16       hasMods       853   16     604   all\n17          Guns       755   17     728   all\n18      hasKkona       727   18     384   all\n19     DinoDance       721   19     269   all\n20       PopNemo       709   20     334   all\n21 TwitchConHYPE       630   21     236   all\n22      hasSadge       601   22     481   all\n23      has0head       599   23     301   all\n24       hasFlex       569   24     294   all\n25             e       563   25     496   all"
  },
  {
    "objectID": "slides/slides-07.html#and-now-you",
    "href": "slides/slides-07.html#and-now-you",
    "title": "🔨 Text as data in R",
    "section": "And now … you!",
    "text": "And now … you!\nGruppenarbeit (ca. 15 Minuten) mit kurzer Ergebnisdiskussion (ca. 15 Minuten)\n\n\n\n\n\n\nArbeitsauftrag\n\n\n\nÜberlegt zusammen mit eurer/m Präsentationspartner:in (ca. 5 Minuten), welche Bereinigungschritte für die jeweiligen Daten (Chats, Transkripte und Korpus) im Kontext euers Projekts notwendig sind.\nDiskutiert eure Ergebnisse mit einer anderen Präsentationsgruppe (ca. 5 Minuten).\nDokumentiert euer Fazit (inklusive der konkreten Schritte) auf einer der Folienvorlagen (siehe nächste Slide).\n\n\n\n\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/slides-07.html#please-discuss",
    "href": "slides/slides-07.html#please-discuss",
    "title": "🔨 Text as data in R",
    "section": "Please discuss!",
    "text": "Please discuss!\nBitte nutzt die jeweilige Folienvorlage für die Dokumentation euerer Ergebnisse\n\n\n\n\n\n     Gruppe A\n\n\n\n\n\n     Gruppe B\n\n\n\n\n\n     Gruppe C\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/slides-07.html#references",
    "href": "slides/slides-07.html#references",
    "title": "🔨 Text as data in R",
    "section": "References",
    "text": "References\n\n\nBaden, C., Pipal, C., Schoonvelde, M., & Velden, M. A. C. G. van der. (2022). Three Gaps in Computational Text Analysis Methods for Social Sciences: A Research Agenda. Communication Methods and Measures, 16(1), 1–18. https://doi.org/10.1080/19312458.2021.2015574\n\n\nBenoit, K., Watanabe, K., Wang, H., Nulty, P., Obeng, A., Müller, S., & Matsuo, A. (2018). Quanteda: An r package for the quantitative analysis of textual data. Journal of Open Source Software, 3(30), 774. https://doi.org/10.21105/joss.00774\n\n\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267–297. https://doi.org/10/f458q9\n\n\nHase, V. (2023). Automated Content Analysis (F. Oehmer-Pedrazzi, S. H. Kessler, E. Humprecht, K. Sommer, & L. Castro, Eds.; pp. 23–36). Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-36179-2_3\n\n\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O’Reilly.\n\n\nZheng, A., & Casari, A. (2018). Feature engineering for machine learning: Principles and techniques for data scientists (First edition). O’Reilly."
  },
  {
    "objectID": "slides/slides-01.html#und-nun-zu-ihnen",
    "href": "slides/slides-01.html#und-nun-zu-ihnen",
    "title": "Kick-Off",
    "section": "Und nun zu Ihnen!",
    "text": "Und nun zu Ihnen!\nVorstellungsrunde\n\nWie heißen Sie?\nWas studieren Sie aktuell?\nWas und wo haben Sie im Bachelor studiert?\nWelches soziale Netzwerk/Medium haben Sie letzte Woche am meisten genutzt und warum?\nBeteiligen Sie sich an Online-Diskussionen und wenn ja, wo und warum?\n\n\n\nHintegrund und Vorwissen\nMediennutzung"
  },
  {
    "objectID": "slides/slides-01.html#mehr-daten-durch-fortschreitende-digitalisierung",
    "href": "slides/slides-01.html#mehr-daten-durch-fortschreitende-digitalisierung",
    "title": "Kick-Off",
    "section": "Mehr Daten durch fortschreitende Digitalisierung",
    "text": "Mehr Daten durch fortschreitende Digitalisierung\nBeispiel: Wachsenden Anzahl eingebauter Smartphone-Sensoren\n\nGraphik aus Struminskaya et al. (2020)"
  },
  {
    "objectID": "slides/slides-01.html#verlagerung-in-den-digitalen-raum",
    "href": "slides/slides-01.html#verlagerung-in-den-digitalen-raum",
    "title": "Kick-Off",
    "section": "Verlagerung in den digitalen Raum",
    "text": "Verlagerung in den digitalen Raum\nTrend der Digitalisierung von Verhaltensweisen und Aktivitäten\n\n\n\n\n\nQuelle: Engel et al. (2021)\n\n\n\n\nEinschränkungen\n\nSelektive Nutzung von bestimmten digitalen Geräten bzw. Funktionen\nKategorisierung ist Momentaufnahme und nicht überschneidungsfrei\n\n\n\n\nEinige inhärent digitale Verhalten (z.B. Web Searches) bei zunehmender Digitalisierung von analogen Verhalten (z.B. Collaborative Work)\nFehlen digitaler Spurendaten in all diesen Quadranten für bestimmte Personen und bestimmte Verhaltensweisen durch selektive Nutzung digitaler Geräte."
  },
  {
    "objectID": "slides/slides-01.html#ein-definitionsversuch-von-dbd",
    "href": "slides/slides-01.html#ein-definitionsversuch-von-dbd",
    "title": "Kick-Off",
    "section": "Ein Definitionsversuch von DBD",
    "text": "Ein Definitionsversuch von DBD\nnach Weller (2021)\n\n\n… fasst eine Vielzahl von möglichen Datenquellen zusammen, die verschiedene Arten von Aktivitäten aufzeichnen\n… können dabei helfen, Meinungen, Verhalten und Merkmale der menschlichen Nutzung digitaler Technologien zu erkennen"
  },
  {
    "objectID": "slides/slides-01.html#lernziele",
    "href": "slides/slides-01.html#lernziele",
    "title": "Kick-Off",
    "section": "Lernziele",
    "text": "Lernziele\nDie Studierenden werden …\n\neinen Überblick über die zentralen Möglichkeiten von DBD und die damit verbundenen Herausforderungen bei der Datenerhebung und -aufbereitung bekommen\nlernen die Stärken und Schwächen verschiedener Methoden zur Erhebung von DBD bewerten\nzentrale Anforderungen an Datenschutz, Forschungsethik und Datenqualität kennen und verstehen lernen\nzentrale sozialwissenschaftliche Methoden zur Analyse von DBD kennenlernen\ndas Wissen über DBD, Statistik und Datenanalyse in eigenen kleinen Projekten zu üben und anzuwenden"
  },
  {
    "objectID": "slides/slides-01.html#themen-blocks",
    "href": "slides/slides-01.html#themen-blocks",
    "title": "Kick-Off",
    "section": "4 (Themen-)Blocks",
    "text": "4 (Themen-)Blocks\nStruktur und Aufbau des Seminars"
  },
  {
    "objectID": "slides/slides-01.html#twitch-meets-politics-meets-nlp",
    "href": "slides/slides-01.html#twitch-meets-politics-meets-nlp",
    "title": "Kick-Off",
    "section": "Twitch meets Politics meets NLP",
    "text": "Twitch meets Politics meets NLP\nAuswertung von Live-Stream(-Chats) zur US-Presidential Debates\n\n\nEntwicklung eigener kleiner Forschungsprojekte mit\n\nDBD in Form von Live-Chats & Transcripten\nNatrual Language Processing (NLP) für die Analyse von\nLive-(Reactions)-Streams von  Twitch-Streamer:innen\nauf die (Vice-)Presidential Debates 2024"
  },
  {
    "objectID": "slides/slides-01.html#warum-twitch",
    "href": "slides/slides-01.html#warum-twitch",
    "title": "Kick-Off",
    "section": "Warum  Twitch?",
    "text": "Warum  Twitch?\nPlattform für Live-Streaming von User-generated content\n\nführende Live-Streaming-Plattform, hauptsächlich für Videospiele, zunehmend aber andere Kategorien (z.B. “Just Chatting”)\nermöglicht Streamern:innen Echtzeit-Interaktionen mit Community via Chat\nbietet Monetarisierungsmöglichkeiten durch Abonnements, Spenden und Werbung\nin Zahlen: 2023 hat Twitch $3 Milliarden Umsatz generiert, 7,1 Millionen aktive Streamer:innen und 2,41 Millionen aktive Zuschauer:innen, die ingesamt 21,4 Milliarden Stunden an Content konsumiert haben\n\nTrend: Plattform für politische Diskussionen und Debatten"
  },
  {
    "objectID": "slides/slides-01.html#nlp-angewendet-auf-chat-logs-und-transkripte",
    "href": "slides/slides-01.html#nlp-angewendet-auf-chat-logs-und-transkripte",
    "title": "Kick-Off",
    "section": "NLP angewendet auf Chat-Logs und Transkripte",
    "text": "NLP angewendet auf Chat-Logs und Transkripte\nInformationen zur Datenanalyse & potentielle Analysestrategien\n\nTranskripte & Chats der Live-Streams von  hasanabi und  zackrawrr und | TheMajorityReport zu der Presidential (Harris vs. Trump) und Vice-Presidential (Vance vs. Walz) Debates 2024\n\nDownload der VODs mit dem  Paket twitch-dl\nDownload & Stream des Live-Chats mit dem  Paket chat_downloader\nTranskription der Streams & der Debatte mit AI-based Transkriptionstool NoScribe\n\nAuswertung mit NLP-Methoden (z.B. Topic Modeling, Sentimentanalyse etc.)"
  },
  {
    "objectID": "slides/slides-01.html#kurzer-exkurs-zur-auswertung-analyse",
    "href": "slides/slides-01.html#kurzer-exkurs-zur-auswertung-analyse",
    "title": "Kick-Off",
    "section": "Kurzer Exkurs zur Auswertung & Analyse",
    "text": "Kurzer Exkurs zur Auswertung & Analyse\nHäufig gestellte Fragen zu notwendigen Methodenvorkenntnissen\n\nWelchen Vorkenntnisse sind für den Kurs vorausgesetzt? Interesse an sozialwissenschaftlichen Perspektiven auf Medien, Kommunikation und digitale Technologien & Grundkenntnisse in der Arbeit mit Statistikprogrammen (z.B. R, Python, Stata, SPSS)\nWerden wir praktisch mit Statistikprogrammen arbeiten? Ja. Dazu werden wir R bzw. RStudio nutzen.\n\nDeswegen: Bitte 💻 mitbringen!\n\nWerden wir die mathematische Grundlagen der vorgestellten Methoden lernen? Ja und Nein. Der Kurs konzentriert sich in erster Linie auf die Anwendung; einige mathematische Parameter der vorgestellten Methoden werden jedoch für die Anwendung benötigt und deswegen kurz erörtert."
  },
  {
    "objectID": "slides/slides-01.html#vorläufiger-seminarplan",
    "href": "slides/slides-01.html#vorläufiger-seminarplan",
    "title": "Kick-Off",
    "section": "(Vorläufiger) Seminarplan",
    "text": "(Vorläufiger) Seminarplan\n\n\n\n\n\n\n\n\nSession\nDatum\nTopic\nPresenter\n\n\n\n\n\n📂 Block 1\nIntroduction\n\n\n\n1\n23.10.2024\nKick-Off\nChristoph Adrian\n\n\n2\n30.10.2024\nDBD: Overview & Introduction\nChristoph Adrian\n\n\n3\n06.11.2024\n🔨 Introduction to working with R\nChristoph Adrian\n\n\n\n📂 Block 2\nTheoretical Background: Twitch & TV Election Debates\n\n\n\n4\n13.11.2024\n📚 Twitch-Nutzung im Fokus\nStudent groups\n\n\n5\n20.11.2024\n📚 (Wirkungs-)Effekte von Twitch & TV-Debatten\nStudent groups\n\n\n6\n27.11.2024\n📚 Politische Debatten & Social Media\nStudent groups\n\n\n\n📂 Block 3\nMethod: Natural Language Processing\n\n\n\n7\n04.12.2024\n🔨 Text as data I: Introduction\nChristoph Adrian\n\n\n8\n11.12.2024\n🔨 Text as data II: Advanced Methods\nChristoph Adrian\n\n\n9\n18.12.2024\n🔨 Advanced Method I: Topic Modeling\nChristoph Adrian\n\n\n\nNo lecture\n🎄Christmas Break\n\n\n\n10\n08.01.2025\n🔨 Advanced Method II: Machine Learning\nChristoph Adrian\n\n\n\n📂 Block 4\nProject Work\n\n\n\n11\n15.01.2025\n🔨 Project work\nStudent groups\n\n\n12\n22.01.2025\n🔨 Project work\nStudent groups\n\n\n13\n29.01.2025\n📊 Project Presentation I\nStudent groups (TBD)\n\n\n14\n05.02.2025\n📊 Project Presentation & 🏁 Evaluation\nStudentds (TBD) & Christoph Adrian"
  },
  {
    "objectID": "slides/slides-01.html#studon-github",
    "href": "slides/slides-01.html#studon-github",
    "title": "Kick-Off",
    "section": "StudOn & Github",
    "text": "StudOn & Github\nMaterialien und Kommunikation im Kurs\n\nInformationen zu Kurs (Semesterplan, Syllabus, Prüfungleistungen etc.) & den einzelnen Sitzungen (Slides, Literatur und ggf. Übungsmaterial) finden sich auf der Github-Kursseite\nPflichtliteratur der Präsentationen & Beispieldatensätze werden auf StudOn bereitgestellt\nFür die Kommunikation gilt\n\nAnkündigungen werden über den StudOn-Verteiler versendet\nfür Fragen, allgemein zum Kurs oder spezifische zu R nutzten Sie bitte das Forum\nKontakt über E-Mail bitte nur bei persönlichen Anliegen, die nicht für die Gruppe relvevant sind.\n\n\n\n\nKurze Vorführung der Webseite\nZulip Frage/Probleme bei Registrierung?\nOptional: Github/OSF"
  },
  {
    "objectID": "slides/slides-01.html#what-is-expected",
    "href": "slides/slides-01.html#what-is-expected",
    "title": "Kick-Off",
    "section": "What is expected",
    "text": "What is expected\nLeistungsanforderungen & Prüfungsleistungen\n\nRegelmäßig aktive Teilnahme an Sitzungen\n\nmax. zwei unentschuldigte Fehltermine (Kulanzregelung), bei Krankheit zählen die Fehltermine mit\n\nBearbeitung von vier Assignments im Rahmen eines Portfolios:\n\n👥 Gruppenpräsentation zu wissenschaftlichen Grundlagen 📚 (30 Pkt.)\n👥 Gruppenpräsentation zum Forschungsprojekt 📊 (15 Pkt.)\n👤 Peer Review (15 Pkt.)\n👥 Projektbericht (40 Pkt.)"
  },
  {
    "objectID": "slides/slides-01.html#die-grundlagenpräsentation",
    "href": "slides/slides-01.html#die-grundlagenpräsentation",
    "title": "Kick-Off",
    "section": "Die “Grundlagenpräsentation”",
    "text": "Die “Grundlagenpräsentation”\nInformationen zur Gruppenpräsentation zu wissenschaftlichen Grundlagen 📚\n\nliefert Überblick über das zentrale Thema der Pflichtlektüre, z.B. zentrale Begriffe, Definitionen, Datengrundlage & - aufbereitung sowie methodische Vorgehensweise (inklusive Analyse)\nvermittelt theoretische Grundlage für Forschungsprojekt\nalle bereitgestellten Pflichtexte müssen in der Präsentation berücksichtigt werden, die Gruppe kann aber eigene Schwerpunkte setzen und eigene Quellen hinzufügen\nzwischen 20 und 30 Minuten, danach Zeit für Fragen und Diskussion\n\nBesonderheit: vorher verpflichtendes Feedbackgespräch"
  },
  {
    "objectID": "slides/slides-01.html#feedback-dann-wenn-es-am-meisten-hilft",
    "href": "slides/slides-01.html#feedback-dann-wenn-es-am-meisten-hilft",
    "title": "Kick-Off",
    "section": "Feedback dann, wenn es am meisten hilft",
    "text": "Feedback dann, wenn es am meisten hilft\nInformationen zum verpflichtenden Feedbackgespräch\n\n\nStudierende erhalten Feedback und Tipps, wie sie ihre Präsentation überarbeiten bzw. optimieren können\n30-minütiges Gespräch in der Sprechstunde (mittwochs, 15:30 bis 16:30 Uhr), eine Woche vor der Präsentation\n\nBuchung des Termins über StudOn\n\nAbgabe des ersten vollständigen Präsentationsentwurfs als PowerPoint- oder PDF-Datei spätestens bis 12:00 Uhr am Vortag des Feedback-Gesprächs"
  },
  {
    "objectID": "slides/slides-01.html#die-projektpräsentation",
    "href": "slides/slides-01.html#die-projektpräsentation",
    "title": "Kick-Off",
    "section": "Die “Projektpräsentation”",
    "text": "Die “Projektpräsentation”\nInformationen zur Gruppenpräsentation zum Forschungsprojekt 📊\n\nPräsentation des (aktuellen Stand des) Projekts, inklusive Vorstellung des theoretischen Hintergrundes, Forschungsfrage Datenaufbereitung, Analyse und ersten Ergebnisse\nzwischen 15 bis 20 Minuten (inklusive mind. 5 Minuten für Nachfragen), maximal 10 Folien\nbietet Möglichkeit für Problembesprechung, Feedback und Diskussion\nzur Vereinfachung der Koordination werden Google Slides Vorlagen zur Verfügung gestellt\n\nBesonderheit: Grundlage für das Peer Review"
  },
  {
    "objectID": "slides/slides-01.html#feedback-geben-lernen",
    "href": "slides/slides-01.html#feedback-geben-lernen",
    "title": "Kick-Off",
    "section": "Feedback geben (lernen)",
    "text": "Feedback geben (lernen)\nInformationen zum Peer Review der Gruppenpräsentationen\n\nPeer-Review-Formularen (Ratingskalen sowie offenen Fragen) für Präsentationen anderer Gruppen ausfüllen\nlernen, andere Projekte zu bewerten und konstruktives Feedback zu geben\nGruppen erhalten zusätzliches Feedback zum eigenen Projekt, dass für die Präsentation bzw. den Projektbericht verwendet werden kann\n\nBesonderheit: Individuelles Assigment!"
  },
  {
    "objectID": "slides/slides-01.html#zusammenführung-aller-assignments",
    "href": "slides/slides-01.html#zusammenführung-aller-assignments",
    "title": "Kick-Off",
    "section": "Zusammenführung aller Assignments",
    "text": "Zusammenführung aller Assignments\nInformationen zum Projektberichtch\n\nProjektbericht ist die schriftliche Ausarbeitung des Forschungsprojekts und führt damit die Arbeit aus den Präsentationen und dem (Feedback der) Peer Reviews zusammen\nmindestens eine der vorgestellten Methoden auf ein selbstgewählte Stichprobe der bereitgestellten Daten anwenden, um ein Thema Ihrer Wahl zu erforschen.\n750 bis 1000 Wörter pro Personn, bei einem Gruppenbericht skaliert die Anzal der Wörter mit einem Faktor von 0,8 pro Person (z.B. sollte eine Zweiergruppe 1200 bis 1600 Wörter schreiben, eine Dreiergruppe 1800 bis 2400 Wörter)\nAbgabe als Quarto-Dokument (& PDF), d.h. der Bericht sowie alle Komponenten (z.B. Tabellen, Grafiken) werden in RStudio erstellt und müssen komplett reproduzierbar sein"
  },
  {
    "objectID": "slides/slides-01.html#short-summary",
    "href": "slides/slides-01.html#short-summary",
    "title": "Kick-Off",
    "section": "Short summary",
    "text": "Short summary\nFahrplan für die Prüfungsleistungen\n\n\n\n\n\n\n\n\n\nZeitpunkte\nLeistung\n\n\n\n\nBlock II (13./20./27.11)\n“Themenpräsentation” 📚\n\n\nbis zum 4.12.\nR-Tutorials abschließen (Grundlage für Methodenblock)\n\n\nbis zum 15.01.\nErster vollständiger Entwurf “Projektpräsentation” 📊\n\n\nvom 16.01. bis zum 21.01.\nAusfüllen der Peer-Review-Formulare\n\n\nvom 22.01 bis 29.01/05.02\nEinarbeitung des Feedback in “Projektpräsentation” 📊\n\n\n29.01./05.02\n“Projektpräsentation” 📊\n\n\nbis zum 02.03.25\nAbgabe des Projektberichts"
  },
  {
    "objectID": "slides/slides-01.html#warum",
    "href": "slides/slides-01.html#warum",
    "title": "Kick-Off",
    "section": "Warum ?",
    "text": "Warum ?\nDer Einsatz von R bzw. RStudio im Kurs\n\nKostenlose Software mit vielen nützlichen und beginner-friendly Tutorials\n or ? Both!\n\nIm Kurs:\n\nBestehende R-Kenntnisse sind förderlich, aber nicht zwigend notwendig, wichtiger sind praktische Erfahrung im syntaxbasierten Arbeiten\nLearn to code by example: Code von Sitzungen & Beispielen wird bereitgestellt (ggf. durch Tutorials)\nPflicht: Tutorials auf Kurshomepage"
  },
  {
    "objectID": "slides/slides-01.html#building-a-common-knowledge-base",
    "href": "slides/slides-01.html#building-a-common-knowledge-base",
    "title": "Kick-Off",
    "section": "Building a common knowledge base",
    "text": "Building a common knowledge base\nInformationen zu den -Tutorials\n\nVermittlung des notwendigen Basiswissens für die Arbeit mit R, RStudio und Quarto mit Hilfe von zwei (Video-)Tutorials:\n\nEinführung in R, RStudio und Quarto von Andy Field für die Vermittlung der Grundlagen & den allgemeinen Umgang mit den Programmen\nPraktisches Arbeiten mit R des CCS Amsterdam legt Schwerpunkt auf das „inhaltliche“ Arbeiten mit R (Vorstellung von Verwendung von wichtigen Funktionen)\n\nWeitere nützliche Quellen finden Sie in dem Bereich Working with R auf der Kursseite"
  },
  {
    "objectID": "slides/slides-01.html#sonderanmeldetermin-für-prüfung",
    "href": "slides/slides-01.html#sonderanmeldetermin-für-prüfung",
    "title": "Kick-Off",
    "section": "Sonderanmeldetermin für Prüfung",
    "text": "Sonderanmeldetermin für Prüfung\nWichtige Informationen zur Prüfungsanmeldung\n\nÄrgerlicherweise gibt es immer wieder Studierende, die sich anmelden und betreuen lassen, aber dann einfach irgendwann (teilweise sehr kurzfristig) “verschwinden”.\nDeshalb: Nutzung des WiSo-Sonderanmeldetermin für Prüfung am 28./29.10.\n\nBitte berücksichtigen Sie unbedingt:\n\n⚠️ Nehmen Sie bitte kein Thema an, wenn absehbar ist, dass Sie nicht teilnehmen werden.\n⚠️ Wir behalten uns vor, bei Rückzug trotz abgeschlossener Themenvergabe, Sie trotzdem für die Prüfung zu melden"
  },
  {
    "objectID": "slides/slides-01.html#please-state-your-preference",
    "href": "slides/slides-01.html#please-state-your-preference",
    "title": "Kick-Off",
    "section": "Please state your preference",
    "text": "Please state your preference\nVergabe der Präsentationsthemen mit SimpleAssign\n\n\n\n\nBitte scannen Sie den QR-Code oder nutzen Sie folgenden Link und geben Sie Ihre Themenpräferenz an:\n\nhttps://simpleassign.net/poll/-O9lSn9X3pAIOdHyNQNu\n\n\n\n\n\n\n\n    \n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/slides-01.html#before-we-meet-again",
    "href": "slides/slides-01.html#before-we-meet-again",
    "title": "Kick-Off",
    "section": "Before we meet again",
    "text": "Before we meet again\nHinweise und offene Fragen\n\nLernen Sie die Kursseite kennen! Und checken Sie die Infos () zur nächten Sitzung.\nVerschaffen Sie sich einen Überblick über die R-Tutorials\n\nEin paar Fragen an Sie:\n\nWhy no English? 🤷\nWarum das große Interesse an Zeitreihenanalyse?\nWelche Erwartung an “Machine Learning”?"
  },
  {
    "objectID": "slides/slides-01.html#literatur",
    "href": "slides/slides-01.html#literatur",
    "title": "Kick-Off",
    "section": "Literatur",
    "text": "Literatur\n\n\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Digital trace data (1st ed., pp. 100–118). Routledge. https://www.taylorfrancis.com/books/9781003024583/chapters/10.4324/9781003024583-8\n\n\nStruminskaya, B., Lugtig, P., Keusch, F., & Höhne, J. K. (2020). Augmenting Surveys With Data From Sensors and Apps: Opportunities and Challenges. Social Science Computer Review, 089443932097995. https://doi.org/10.1177/0894439320979951\n\n\nWeller, K. (2021). A short introduction to computational social science and digital behavioral data. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meettheexperts_Introductioncss.pdf"
  },
  {
    "objectID": "slides/slides-03.html#seminarplan",
    "href": "slides/slides-03.html#seminarplan",
    "title": "🔨 Working with R",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\nSession\nDatum\nTopic\nPresenter\n\n\n\n\n\n📂 Block 1\nIntroduction\n\n\n\n1\n23.10.2024\nKick-Off\nChristoph Adrian\n\n\n2\n30.10.2024\nDBD: Overview & Introduction\nChristoph Adrian\n\n\n3\n06.11.2024\n🔨 Introduction to working with R\nChristoph Adrian\n\n\n\n📂 Block 2\nTheoretical Background: Twitch & TV Election Debates\n\n\n\n4\n13.11.2024\n📚 Twitch-Nutzung im Fokus\nStudent groups\n\n\n5\n20.11.2024\n📚 (Wirkungs-)Effekte von Twitch & TV-Debatten\nStudent groups\n\n\n6\n27.11.2024\n📚 Politische Debatten & Social Media\nStudent groups\n\n\n\n📂 Block 3\nMethod: Natural Language Processing\n\n\n\n7\n04.12.2024\n🔨 Text as data I: Introduction\nChristoph Adrian\n\n\n8\n11.12.2024\n🔨 Text as data II: Advanced Methods\nChristoph Adrian\n\n\n9\n18.12.2024\n🔨 Advanced Method I: Topic Modeling\nChristoph Adrian\n\n\n\nNo lecture\n🎄Christmas Break\n\n\n\n10\n08.01.2025\n🔨 Advanced Method II: Machine Learning\nChristoph Adrian\n\n\n\n📂 Block 4\nProject Work\n\n\n\n11\n15.01.2025\n🔨 Project work\nStudent groups\n\n\n12\n22.01.2025\n🔨 Project work\nStudent groups\n\n\n13\n29.01.2025\n📊 Project Presentation I\nStudent groups (TBD)\n\n\n14\n05.02.2025\n📊 Project Presentation & 🏁 Evaluation\nStudentds (TBD) & Christoph Adrian"
  },
  {
    "objectID": "slides/slides-03.html#kurzes-organisatorische-update",
    "href": "slides/slides-03.html#kurzes-organisatorische-update",
    "title": "🔨 Working with R",
    "section": "Kurzes organisatorische Update",
    "text": "Kurzes organisatorische Update\nInformationen zu Kursdetails und Prüfungsleistungen\n\nInfos zur Zuteilung der Feedbackgespräche\n\nKeine Anmeldung für Feedback notwendig, stattdessen Reihenfolge wie bei Präsentation\nBei Terminproblemen könnt ihr gerne auf mich zukommen\n\n🗣️ 2. Präsentationsgruppe: Bitte denkt\n\nan die Zusendung des Entwurf der Präsentationsfolien bis spätestens nächste Woche Dienstag 12:00!\ndas Feedbackgespräch am Mittwoch im Anschluss an das Seminar."
  },
  {
    "objectID": "slides/slides-03.html#building-best-practice",
    "href": "slides/slides-03.html#building-best-practice",
    "title": "🔨 Working with R",
    "section": "Building best practice",
    "text": "Building best practice\nWillkommen (zurück) zu \n\n\n\nHow most academics learn R:\n\n\n\n\n\n\n\nHow you should learn R:\n\nR nicht systematisch lernen, sondern spezifisch anwenden.\nOrganisation der Arbeit mit RStudio-Projekten\nLesbaren und nachvollziehbaren Code schreiben!\n(Nach-)Fragen!"
  },
  {
    "objectID": "slides/slides-03.html#ein-repository-voller-daten",
    "href": "slides/slides-03.html#ein-repository-voller-daten",
    "title": "🔨 Working with R",
    "section": "Ein Repository voller Daten",
    "text": "Ein Repository voller Daten\nBeispiel für Übung durch Anwendung:  tidytuesday (social data project)\n\n\n\nData is posted to social media every Monday morning.\nExplore the data, watching out for interesting relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "slides/slides-03.html#beispiele-für-tidytuesday",
    "href": "slides/slides-03.html#beispiele-für-tidytuesday",
    "title": "🔨 Working with R",
    "section": "Beispiele für #tidytuesday",
    "text": "Beispiele für #tidytuesday"
  },
  {
    "objectID": "slides/slides-03.html#everything-you-need-in-one-place",
    "href": "slides/slides-03.html#everything-you-need-in-one-place",
    "title": "🔨 Working with R",
    "section": "Everything you need in one place",
    "text": "Everything you need in one place\nOrganisation der Arbeit mit RStudio-Projekten\n\n\n\n\n\n\n\n\nEmpfehlungen:\n\nFür jedes Projekt ein RStudio-Projekt.\nSicherung und Organisation von Daten, Skripte und Ouput an einem Ort, z.B. mit Unterstützung durch R-Pakete wie z.B. prodigenr\nVerwendung von relative, keine absoluten Pfade. Empfehlung: here R-Paket"
  },
  {
    "objectID": "slides/slides-03.html#versionskontrolle-als-kür",
    "href": "slides/slides-03.html#versionskontrolle-als-kür",
    "title": "🔨 Working with R",
    "section": "Versionskontrolle als Kür",
    "text": "Versionskontrolle als Kür\nCrashkurs zu Git(Hub)\n\n MalikaIhle\nVersionkontrolle für Code, gesichert in der Cloud\nVollständige Rückverfolgbarkeit von (gesicherten) Änderungen\nGreat effort, great return."
  },
  {
    "objectID": "slides/slides-03.html#run-chunks-not-whole-scripts",
    "href": "slides/slides-03.html#run-chunks-not-whole-scripts",
    "title": "🔨 Working with R",
    "section": "Run chunks, not (whole) scripts",
    "text": "Run chunks, not (whole) scripts\nOutputorientiertes Coding mit Quarto"
  },
  {
    "objectID": "slides/slides-03.html#quarto-rmarkdown-rscript",
    "href": "slides/slides-03.html#quarto-rmarkdown-rscript",
    "title": "🔨 Working with R",
    "section": "Quarto ≥ RMarkdown ≥ RScript",
    "text": "Quarto ≥ RMarkdown ≥ RScript\nDer Weg vom Code zum Output\n\n\nGrundidee von Quarto : ein Quelldokument kann in eine Vielzahl von Ausgabeformaten umgewandelt werden\nMarkdown-Syntax für Text, verschiedene Programmiersprachen (wie z.B. R und Python) in einem Dokument"
  },
  {
    "objectID": "slides/slides-03.html#develop-your-style",
    "href": "slides/slides-03.html#develop-your-style",
    "title": "🔨 Working with R",
    "section": "Develop your style",
    "text": "Develop your style\nWichtigkeit der Codeformatierung und -dokumentierung\n\n# Strive for \nshort_flights &lt;- flights |&gt; filter(air_time &lt; 60)\n\n# Avoid:\nSHTFTS &lt;- flights |&gt; filter(air_time &lt; 60)\n\n\n# Strive for \nflights |&gt;  \n  filter(!is.na(arr_delay), !is.na(tailnum)) |&gt; \n  count(dest)\n\n# Avoid\nflights|&gt;filter(!is.na(arr_delay), !is.na(tailnum))|&gt;count(dest)\n\n\n\nDie Entwicklung (oder Aneignung) eines Codestils ist wichtig!\nWas sich zunächst willkürlich anfühlt, hilft mit der Zeit sehr\nUnterstützung durch den tidyverse style guide bzw. die Pakete styler oder lintr"
  },
  {
    "objectID": "slides/slides-03.html#empfehlung-tidyverse-is-your-friend",
    "href": "slides/slides-03.html#empfehlung-tidyverse-is-your-friend",
    "title": "🔨 Working with R",
    "section": "Empfehlung: tidyverse is your friend!",
    "text": "Empfehlung: tidyverse is your friend!\nVerschiedenen Paketen für alle Schritte eines Projektes\n\nQuelle: RStudio"
  },
  {
    "objectID": "slides/slides-03.html#the-friend-of-your-friend-easystats",
    "href": "slides/slides-03.html#the-friend-of-your-friend-easystats",
    "title": "🔨 Working with R",
    "section": "The friend of your friend: easystats",
    "text": "The friend of your friend: easystats\nFokus auf die Analyse\n\nQuelle: Lüdecke et al. (2022)"
  },
  {
    "objectID": "slides/slides-03.html#am-anfang-steht-die-theorie",
    "href": "slides/slides-03.html#am-anfang-steht-die-theorie",
    "title": "🔨 Working with R",
    "section": "Am Anfang steht die Theorie",
    "text": "Am Anfang steht die Theorie\nTypischer “data science process” als Kontext der Sitzung\n\nQuelle: Wickham et al. (2023)\n\nAdditional steps (add if necessary):"
  },
  {
    "objectID": "slides/slides-03.html#age-difference-in-years-between-move-love-interests",
    "href": "slides/slides-03.html#age-difference-in-years-between-move-love-interests",
    "title": "🔨 Working with R",
    "section": "Age difference in years between move love interests",
    "text": "Age difference in years between move love interests\nDatengrundlage für die Beispiele: Hollywood Age Gap ( |  )\n\n\n\n\n\n\n\n\n\n\n“An informational site showing the age gap between movie love interests.”\nCommunity-Projekt\n\nGuidlines for participation/submission:\n\nThe two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)\nThe youngest of the two actors is at least 17 years old\nNot animated characters"
  },
  {
    "objectID": "slides/slides-03.html#explore-adapt-repeat",
    "href": "slides/slides-03.html#explore-adapt-repeat",
    "title": "🔨 Working with R",
    "section": "Explore ➞ Adapt ➞ Repeat ⟳",
    "text": "Explore ➞ Adapt ➞ Repeat ⟳\nProzess der Datenaufbereitung\n\n\nnimmt in der Regel den Großteil der Zeit der Datenanalyse in Anspruch\nhäufig bedarf es der mehrfachen Wiederholung dreier Schritte:\n\nder (explorativen) Erkundung,\nder Standartdisierung und\nder (erneuten) Bereinung der Daten"
  },
  {
    "objectID": "slides/slides-03.html#drei-stufen-der-datenqualität",
    "href": "slides/slides-03.html#drei-stufen-der-datenqualität",
    "title": "🔨 Working with R",
    "section": "Drei Stufen der Datenqualität",
    "text": "Drei Stufen der Datenqualität\nTypische Strategien zur Datenbereinigung nach Pearson (2018)\n\n\n\n\n\n\nQuelle: Jonge & Loo (2013)\n\n\n\n\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\nWie viele Fälle sind enthalten? Wie viele Variablen?\nWie lauten die Variablennamen? Sind sie sinnvoll?\nWelchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\nWie viele eindeutige Werte hat jede Variable?\nWelcher Wert tritt am häufigsten auf, und wie oft kommt er vor?\nGibt es fehlende Werte? Wenn ja, wie häufig ist dies der Fall?\n\nUntersuchung deskriptiver Statistiken für jede Variable;\nExplorative Visualisierung;\nVerschiedene Verfahren zur Suche nach Anomalien in den Daten;\nUntersuchung der Beziehungen zwischen Schlüsselvariablen mit Hilfe von Scatterplots/Boxplots/Mosaic-Plots;\nDokumentation des Vorgehens und der Ergebnisse (z.B. mit .rmd-Dokument). Dient als Grundlage für die anschließende Analyse und Erläuterung der Ergebnisse."
  },
  {
    "objectID": "slides/slides-03.html#direkter-download-via-url",
    "href": "slides/slides-03.html#direkter-download-via-url",
    "title": "🔨 Working with R",
    "section": "Direkter Download via URL",
    "text": "Direkter Download via URL\nDatenimport und -preview\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\n🔍 Wie viele Fälle sind enthalten? Wie viele Variablen?\n🔍 Wie lauten die Variablennamen? Sind sie sinnvoll?\nWelchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\nWie viele eindeutige Werte hat jede Variable?\nWelcher Wert tritt am häufigsten auf, und wie oft kommt er vor?\nGibt es fehlende Werte? Wenn ja, wie häufig ist dies der Fall?\n\n\n\n\n\nage_gaps &lt;- read_csv(\"http://hollywoodagegap.com/movies.csv\") \nage_gaps \n\n# A tibble: 1,199 × 12\n   `Movie Name`       `Release Year` Director    `Age Difference` `Actor 1 Name`\n   &lt;chr&gt;                       &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;         \n 1 Harold and Maude             1971 Hal Ashby                 52 Bud Cort      \n 2 Venus                        2006 Roger Mich…               50 Peter O'Toole \n 3 The Quiet American           2002 Phillip No…               49 Michael Caine \n 4 Solitary Man                 2009 Brian Kopp…               45 Michael Dougl…\n 5 The Big Lebowski             1998 Joel Coen                 45 David Huddles…\n 6 Beginners                    2010 Mike Mills                43 Christopher P…\n 7 Poison Ivy                   1992 Katt Shea                 42 Tom Skerritt  \n 8 Dirty Grandpa                2016 Dan Mazer                 41 Robert De Niro\n 9 Whatever Works               2009 Woody Allen               40 Larry David   \n10 Entrapment                   1999 Jon Amiel                 39 Sean Connery  \n# ℹ 1,189 more rows\n# ℹ 7 more variables: `Actor 1 Gender` &lt;chr&gt;, `Actor 1 Birthdate` &lt;date&gt;,\n#   `Actor 1 Age` &lt;dbl&gt;, `Actor 2 Name` &lt;chr&gt;, `Actor 2 Gender` &lt;chr&gt;,\n#   `Actor 2 Birthdate` &lt;chr&gt;, `Actor 2 Age` &lt;dbl&gt;"
  },
  {
    "objectID": "slides/slides-03.html#let-the-cleaning-beginn",
    "href": "slides/slides-03.html#let-the-cleaning-beginn",
    "title": "🔨 Working with R",
    "section": "Let the cleaning beginn",
    "text": "Let the cleaning beginn\nErste Schritte der Datenbereinigung\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\n✅ Wie viele Fälle sind enthalten? Wie viele Variablen?\n✅ Wie lauten die Variablennamen? Sind sie sinnvoll?\n🔍 Welchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\nWie viele eindeutige Werte hat jede Variable?\nWelcher Wert tritt am häufigsten auf, und wie oft kommt er vor?\nGibt es fehlende Werte? Wenn ja, wie häufig ist dies der Fall?\n\n\n\n\n\nage_gaps %&lt;&gt;% janitor::clean_names()\nage_gaps %&gt;% glimpse()\n\nRows: 1,199\nColumns: 12\n$ movie_name        &lt;chr&gt; \"Harold and Maude\", \"Venus\", \"The Quiet American\", \"…\n$ release_year      &lt;dbl&gt; 1971, 2006, 2002, 2009, 1998, 2010, 1992, 2016, 2009…\n$ director          &lt;chr&gt; \"Hal Ashby\", \"Roger Michell\", \"Phillip Noyce\", \"Bria…\n$ age_difference    &lt;dbl&gt; 52, 50, 49, 45, 45, 43, 42, 41, 40, 39, 38, 38, 36, …\n$ actor_1_name      &lt;chr&gt; \"Bud Cort\", \"Peter O'Toole\", \"Michael Caine\", \"Micha…\n$ actor_1_gender    &lt;chr&gt; \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"ma…\n$ actor_1_birthdate &lt;date&gt; 1948-03-29, 1932-08-02, 1933-03-14, 1944-09-25, 193…\n$ actor_1_age       &lt;dbl&gt; 23, 74, 69, 65, 68, 81, 59, 73, 62, 69, 57, 77, 59, …\n$ actor_2_name      &lt;chr&gt; \"Ruth Gordon\", \"Jodie Whittaker\", \"Do Thi Hai Yen\", …\n$ actor_2_gender    &lt;chr&gt; \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"man\", …\n$ actor_2_birthdate &lt;chr&gt; \"1896-10-30\", \"1982-06-03\", \"1982-10-01\", \"1989-01-0…\n$ actor_2_age       &lt;dbl&gt; 75, 24, 20, 20, 23, 38, 17, 32, 22, 30, 19, 39, 23, …"
  },
  {
    "objectID": "slides/slides-03.html#building-the-habits",
    "href": "slides/slides-03.html#building-the-habits",
    "title": "🔨 Working with R",
    "section": "Building the habits!",
    "text": "Building the habits!\nErste Schritte der Datenbereinigung\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\n✅ Wie viele Fälle sind enthalten? Wie viele Variablen?\n✅ Wie lauten die Variablennamen? Sind sie sinnvoll?\n✅ Welchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\nWie viele eindeutige Werte hat jede Variable?\nWelcher Wert tritt am häufigsten auf, und wie oft kommt er vor?\nGibt es fehlende Werte? Wenn ja, wie häufig ist dies der Fall?\n\n\n\n\n\nage_gaps_correct &lt;- age_gaps %&gt;% \n  mutate(\n    across(ends_with(\"_birthdate\"), ~as.Date(.)) # set dates to dates\n  )\n\n\n\nReminder\n\nVeränderungen nicht im selben Datensatz speichern\nVerständliche Benennung & Kommentierung der Daten\nBearbeitungsschritte kommentieren"
  },
  {
    "objectID": "slides/slides-03.html#kontrolle-der-lageparameter",
    "href": "slides/slides-03.html#kontrolle-der-lageparameter",
    "title": "🔨 Working with R",
    "section": "Kontrolle der Lageparameter",
    "text": "Kontrolle der Lageparameter\nErste Schritte der Datenbereinigung\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\n✅ Wie viele Fälle sind enthalten? Wie viele Variablen?\n✅ Wie lauten die Variablennamen? Sind sie sinnvoll?\n✅ Welchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\n🔍 Wie viele eindeutige Werte hat jede Variable?\n🔍 Welcher Wert tritt am häufigsten auf, und wie oft kommt er vor?\n🔍 Gibt es fehlende Werte? Wenn ja, wie häufig ist dies der Fall?\n\n\n\n\n\nage_gaps_correct %&gt;% \n    datawizard::describe_distribution() %&gt;% \n    print_html()\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nMin\nMax\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nrelease_year\n2000.53\n17.07\n15\n1935\n2024\n-1.62\n2.52\n1199\n0\n\n\nage_difference\n10.62\n8.62\n12\n0\n52\n1.19\n1.54\n1199\n0\n\n\nactor_1_age\n40.07\n10.93\n15\n17\n81\n0.54\n0.22\n1199\n0\n\n\nactor_2_age\n31.22\n8.47\n10\n17\n81\n1.39\n3.71\n1199\n0"
  },
  {
    "objectID": "slides/slides-03.html#lets-start-exploring",
    "href": "slides/slides-03.html#lets-start-exploring",
    "title": "🔨 Working with R",
    "section": "Let’s start exploring!",
    "text": "Let’s start exploring!\nWie sind die Altersunterschiede verteilt?\n\nage_gaps_correct %&gt;% \n    ggplot(aes(age_difference)) +\n    geom_bar() +\n    theme_pubr()"
  },
  {
    "objectID": "slides/slides-03.html#a-recent-past",
    "href": "slides/slides-03.html#a-recent-past",
    "title": "🔨 Working with R",
    "section": "A recent past …",
    "text": "A recent past …\nIn welchen Filmen ist der Altersunterschied am höchsten?\n\nage_gaps_correct %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(movie_name, age_difference, release_year) \n\n# A tibble: 1,199 × 3\n   movie_name         age_difference release_year\n   &lt;chr&gt;                       &lt;dbl&gt;        &lt;dbl&gt;\n 1 Harold and Maude               52         1971\n 2 Venus                          50         2006\n 3 The Quiet American             49         2002\n 4 Solitary Man                   45         2009\n 5 The Big Lebowski               45         1998\n 6 Beginners                      43         2010\n 7 Poison Ivy                     42         1992\n 8 Dirty Grandpa                  41         2016\n 9 Whatever Works                 40         2009\n10 Entrapment                     39         1999\n# ℹ 1,189 more rows"
  },
  {
    "objectID": "slides/slides-03.html#or-still-present",
    "href": "slides/slides-03.html#or-still-present",
    "title": "🔨 Working with R",
    "section": "… or still present?",
    "text": "… or still present?\nIn welchen Filmen ist der Altersunterschied am höchsten?\n\nage_gaps_correct %&gt;% \n    filter(release_year &gt;= 2022) %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(\n        movie_name, age_difference, release_year, \n        actor_1_name, actor_2_name)\n\n# A tibble: 19 × 5\n   movie_name              age_difference release_year actor_1_name actor_2_name\n   &lt;chr&gt;                            &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       \n 1 Poor Things                         21         2023 Mark Ruffalo Emma Stone  \n 2 The Bubble                          21         2022 Pedro Pascal Maria Bakal…\n 3 Oppenheimer                         20         2023 Cillian Mur… Florence Pu…\n 4 The Northman                        20         2022 Alexander S… Anya Taylor…\n 5 Spaceman                            19         2024 Adam Sandler Carey Mulli…\n 6 The Lost City                       16         2022 Channing Ta… Sandra Bull…\n 7 We Live in Time                     13         2024 Andrew Garf… Florence Pu…\n 8 The Idea of You                     12         2024 Nicholas Ga… Anne Hathaw…\n 9 Barbie                              10         2023 Ryan Gosling Margot Robb…\n10 Twisters                            10         2024 Glen Powell  Daisy Edgar…\n11 Anyone but You                       9         2023 Glen Powell  Sydney Swee…\n12 Everything Everywhere …              9         2022 Ke Huy Quan  Michelle Ye…\n13 Top Gun: Maverick                    8         2022 Tom Cruise   Jennifer Co…\n14 Oppenheimer                          7         2023 Cillian Mur… Emily Blunt \n15 Your Place or Mine                   7         2023 Ashton Kutc… Zoë Chao    \n16 Your Place or Mine                   5         2023 Jesse Willi… Reese Withe…\n17 Poor Things                          2         2023 Christopher… Emma Stone  \n18 Your Place or Mine                   2         2023 Ashton Kutc… Reese Withe…\n19 You People                           1         2023 Jonah Hill   Lauren Lond…"
  },
  {
    "objectID": "slides/slides-03.html#durchschnitts-unterschied-nach-jahren",
    "href": "slides/slides-03.html#durchschnitts-unterschied-nach-jahren",
    "title": "🔨 Working with R",
    "section": "(Durchschnitts-)Unterschied nach Jahren",
    "text": "(Durchschnitts-)Unterschied nach Jahren\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\nage_gaps_correct %&gt;% \n    group_by(release_year) %&gt;% \n    summarise(age_difference_mean = mean(age_difference)) %&gt;% \n    ggplot(aes(release_year, age_difference_mean)) +\n    geom_col() +\n    theme_pubr()"
  },
  {
    "objectID": "slides/slides-03.html#verteilung-nach-jahren",
    "href": "slides/slides-03.html#verteilung-nach-jahren",
    "title": "🔨 Working with R",
    "section": "Verteilung nach Jahren",
    "text": "Verteilung nach Jahren\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\n\nggpubr::ggboxplot(\n    data = age_gaps_correct, \n    x = \"release_year\", \n    y = \"age_difference\"\n  ) +\n  # Rotate x-axis labels by 90 degrees\n  theme(\n    axis.text.x = element_text(\n        angle = 90,\n        vjust = 0.5,\n         hjust=1))"
  },
  {
    "objectID": "slides/slides-03.html#ein-blick-auf-die-korrelation",
    "href": "slides/slides-03.html#ein-blick-auf-die-korrelation",
    "title": "🔨 Working with R",
    "section": "Ein Blick auf die Korrelation",
    "text": "Ein Blick auf die Korrelation\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\nage_gaps %&gt;%\n  select(release_year, age_difference) %&gt;% \n  correlation::correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1   |     Parameter2 |     r |         95% CI | t(1197) |         p\n----------------------------------------------------------------------------\nrelease_year | age_difference | -0.22 | [-0.27, -0.17] |   -7.83 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 1199"
  },
  {
    "objectID": "slides/slides-03.html#mit-kanonen-auf-spatzen-schießen",
    "href": "slides/slides-03.html#mit-kanonen-auf-spatzen-schießen",
    "title": "🔨 Working with R",
    "section": "Mit Kanonen auf Spatzen schießen",
    "text": "Mit Kanonen auf Spatzen schießen\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\n# Schätzung des Models\nmdl &lt;- lm(age_difference ~ release_year, data = age_gaps_correct)\n\n\n\nmdl %&gt;% parameters::parameters()\n\nParameter    | Coefficient |    SE |           95% CI | t(1197) |      p\n------------------------------------------------------------------------\n(Intercept)  |      233.69 | 28.48 | [177.82, 289.57] |    8.21 | &lt; .001\nrelease year |       -0.11 |  0.01 | [ -0.14,  -0.08] |   -7.83 | &lt; .001\n\n\n\n\nmdl %&gt;% performance::model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n8512.891 | 8512.911 | 8528.159 | 0.049 |     0.048 | 8.403 | 8.410"
  },
  {
    "objectID": "slides/slides-03.html#convenience-wrapper",
    "href": "slides/slides-03.html#convenience-wrapper",
    "title": "🔨 Working with R",
    "section": "Convenience wrapper",
    "text": "Convenience wrapper\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\nmdl %&gt;% report::report()\n\nWe fitted a linear model (estimated using OLS) to predict age_difference with\nrelease_year (formula: age_difference ~ release_year). The model explains a\nstatistically significant and weak proportion of variance (R2 = 0.05, F(1,\n1197) = 61.35, p &lt; .001, adj. R2 = 0.05). The model's intercept, corresponding\nto release_year = 0, is at 233.69 (95% CI [177.82, 289.57], t(1197) = 8.21, p &lt;\n.001). Within this model:\n\n  - The effect of release year is statistically significant and negative (beta =\n-0.11, 95% CI [-0.14, -0.08], t(1197) = -7.83, p &lt; .001; Std. beta = -0.22, 95%\nCI [-0.28, -0.17])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "slides/slides-03.html#try---fail---repeat",
    "href": "slides/slides-03.html#try---fail---repeat",
    "title": "🔨 Working with R",
    "section": "Try - Fail - Repeat",
    "text": "Try - Fail - Repeat\nKurzes Fazit der heutigen Sitzung\n\n\n\n\nWenn R, dann mit RStudio + Quarto!\nAnschauen - nachmachen - ausprobieren\nKeep it tidy\n(Gute) Routinen bilden\n“There is almost always a package for that …”"
  },
  {
    "objectID": "slides/slides-03.html#literatur",
    "href": "slides/slides-03.html#literatur",
    "title": "🔨 Working with R",
    "section": "Literatur",
    "text": "Literatur\n\n\nJonge, E. de, & Loo, M. van der. (2013). An introduction to data cleaning with R.\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Wiernik, B. M., Bacher, E., Thériault, R., & Makowski, D. (2022). Easystats: Framework for easy statistical modeling, visualization, and reporting. CRAN. https://easystats.github.io/easystats/\n\n\nPearson, R. K. (2018). Exploratory data analysis using r. CRC Press/Taylor & Francis Group.\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: import, tidy, transform, visualize, and model data (2nd edition). O’Reilly."
  },
  {
    "objectID": "slides/slides-09.html#seminarplan",
    "href": "slides/slides-09.html#seminarplan",
    "title": "🔨 Topic Modeling",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\nSession\nDatum\nTopic\nPresenter\n\n\n\n\n\n📂 Block 1\nIntroduction\n\n\n\n1\n23.10.2024\nKick-Off\nChristoph Adrian\n\n\n2\n30.10.2024\nDBD: Overview & Introduction\nChristoph Adrian\n\n\n3\n06.11.2024\n🔨 Introduction to working with R\nChristoph Adrian\n\n\n\n📂 Block 2\nTheoretical Background: Twitch & TV Election Debates\n\n\n\n4\n13.11.2024\n📚 Twitch-Nutzung im Fokus\nStudent groups\n\n\n5\n20.11.2024\n📚 (Wirkungs-)Effekte von Twitch & TV-Debatten\nStudent groups\n\n\n6\n27.11.2024\n📚 Politische Debatten & Social Media\nStudent groups\n\n\n\n📂 Block 3\nMethod: Natural Language Processing\n\n\n\n7\n04.12.2024\n🔨 Text as data I: Introduction\nChristoph Adrian\n\n\n8\n11.12.2024\n🔨 Text as data II: Advanced Methods\nChristoph Adrian\n\n\n9\n18.12.2024\n🔨 Advanced Method I: Topic Modeling\nChristoph Adrian\n\n\n\nNo lecture\n🎄Christmas Break\n\n\n\n10\n08.01.2025\n🔨 Advanced Method II: Machine Learning\nChristoph Adrian\n\n\n\n📂 Block 4\nProject Work\n\n\n\n11\n15.01.2025\n🔨 Project work\nStudent groups\n\n\n12\n22.01.2025\n🔨 Project work\nStudent groups\n\n\n13\n29.01.2025\n📊 Project Presentation I\nStudent groups (TBD)\n\n\n14\n05.02.2025\n📊 Project Presentation & 🏁 Evaluation\nStudentds (TBD) & Christoph Adrian"
  },
  {
    "objectID": "slides/slides-09.html#eure-meinung-ist-gefragt",
    "href": "slides/slides-09.html#eure-meinung-ist-gefragt",
    "title": "🔨 Topic Modeling",
    "section": "Eure Meinung ist gefragt!",
    "text": "Eure Meinung ist gefragt!\nBitte nehmt an der kurzen Evaluation teil\n\n\n\n\nBitte nehmen Sie über den QR Code oder folgenden Link an der Evaluation teil:\n\nhttps://eva.fau.de/\nLosung: QNALW\n\n\n\n\n\n\n\n    \n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/slides-09.html#quick-reminder-preview",
    "href": "slides/slides-09.html#quick-reminder-preview",
    "title": "🔨 Topic Modeling",
    "section": "Quick reminder & preview",
    "text": "Quick reminder & preview\nRekapitulation der letzten Sitzung\n\nTopic Modeling ist ein Verfahren des unüberwachten maschinellen Lernens, das sich zur Exploration und Deskription großer Textmengen eignet um\nunbekannte, latente Themen auf Basis von häufig gemeinsam auftretenden (Clustern an) Wörtern in Dokumenten zu identifizieren\n\nHeutiger Fokus: Umsetzung zentraler Schritte\n\nPreprocessing\nModell-Einstellung\nAnalyse & Interpretation\nValdierung"
  },
  {
    "objectID": "slides/slides-09.html#welche-preprocessing-schritte-sind-notwendig",
    "href": "slides/slides-09.html#welche-preprocessing-schritte-sind-notwendig",
    "title": "🔨 Topic Modeling",
    "section": "Welche Preprocessing-Schritte sind notwendig?",
    "text": "Welche Preprocessing-Schritte sind notwendig?\nUmsetzung zentraler Schritte: 1.Preprocessing\n\nVerschiedene Verfahren möglich bzw. empfohlen (z.B. Denny & Spirling, 2018; Maier et al., 2020)\nVerwendung der empfohlenen Schritte nach Maier et al. (2018):\n\n✅ Deduplication;\n✅ Tokenization;\n✅ Transform all characters to lowercase;\n🏗️ Remove punctuation & special characters;\n⚠️ Create/remove custom Ngrams/stopwords;\n✅ Term unification (lemmatization)\n🏗️ Relative Pruning"
  },
  {
    "objectID": "slides/slides-09.html#von-spacyr-zu-tokens",
    "href": "slides/slides-09.html#von-spacyr-zu-tokens",
    "title": "🔨 Topic Modeling",
    "section": "Von spacyr zu Tokens",
    "text": "Von spacyr zu Tokens\nUmsetzung zentraler Schritte: 1.Preprocessing\n\n\n# spacyr-Korpus zu Tokens\nchat_spacyr_toks &lt;- chats_spacyr %&gt;% \n  as.tokens(\n    use_lemma = TRUE\n  ) %&gt;% \n  tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = FALSE,\n    remove_url = FALSE, \n    split_hyphens = FALSE,\n    split_tags = FALSE,\n  ) %&gt;% \n  tokens_remove(\n    pattern = stopwords(\"en\")\n  ) %&gt;% \n  tokens_ngrams(n = 1:3) \n\n# Output\nchat_spacyr_toks\n\n\nTokens consisting of 913,245 documents.\ndc03b89a-722d-4eaa-a895-736533a68aca :\n [1] \"60fps\"        \"LETSGO\"       \"60fps\"        \"letsgo\"       \"60fps\"       \n [6] \"letsgo\"       \"60fps\"        \"letsgo\"       \"60fps_LETSGO\" \"LETSGO_60fps\"\n[11] \"60fps_letsgo\" \"letsgo_60fps\"\n[ ... and 9 more ]\n\n6be50e12-2fd5-436f-b253-b2358b618380 :\n[1] \"captain\"           \"o\"                 \"captain\"          \n[4] \"captain_o\"         \"o_captain\"         \"captain_o_captain\"\n\nf5e41904-7f01-4f03-ad6c-2c0f07d70ed0 :\n[1] \"wokege\"            \"right\"             \"time\"             \n[4] \"wokege_right\"      \"right_time\"        \"wokege_right_time\"\n\n92dc6519-eb54-4c18-abef-27201314b22f :\n[1] \"GECKW\"              \"BITCH\"              \"GECKIN\"            \n[4] \"GECKW\"              \"GECKW_BITCH\"        \"BITCH_GECKIN\"      \n[7] \"GECKIN_GECKW\"       \"GECKW_BITCH_GECKIN\" \"BITCH_GECKIN_GECKW\"\n\n92055088-7067-48c0-aa11-9c6103bdf4c4 :\n [1] \"YOUCANT\"            \"bring\"              \"back\"              \n [4] \"30FPS\"              \"Cinema\"             \"YOUCANT_bring\"     \n [7] \"bring_back\"         \"back_30FPS\"         \"30FPS_Cinema\"      \n[10] \"YOUCANT_bring_back\" \"bring_back_30FPS\"   \"back_30FPS_Cinema\" \n\n03ad4706-aa67-4ddc-a1e4-6f8ca981778e :\n[1] \"time\"        \"Wokege\"      \"time_Wokege\"\n\n[ reached max_ndoc ... 913,239 more documents ]"
  },
  {
    "objectID": "slides/slides-09.html#wenn-die-bereinigung-zu-gut-funktioniert",
    "href": "slides/slides-09.html#wenn-die-bereinigung-zu-gut-funktioniert",
    "title": "🔨 Topic Modeling",
    "section": "Wenn die Bereinigung zu gut funktioniert …",
    "text": "Wenn die Bereinigung zu gut funktioniert …\n1.Preprocessing: Herausforderungen durch leere Nachrichten\n\nAnalysen des stm Topic Models nutzen Bezüge auf die Stammdaten ➜ Fälle von Modell und Stammdaten müssen übereinstimmen\nProbleme:\n\nDurch Tokenisierung & Pruning können “leere” Chatnachrichten entstehen\nDiese leeren Nachrichten werden bei Schätzung nicht berücksichtigt\n\nLösung:\n\n(Mehrfache) Identifikation & Ausschluss von leeren Nachrichten"
  },
  {
    "objectID": "slides/slides-09.html#prüfen-erweitern-filtern",
    "href": "slides/slides-09.html#prüfen-erweitern-filtern",
    "title": "🔨 Topic Modeling",
    "section": "Prüfen ➜ Erweitern ➜ Filtern",
    "text": "Prüfen ➜ Erweitern ➜ Filtern\n1.Preprocessing: Herausforderungen bei der Tokenisierung\n\n\n# Get document names\noriginal_docnames &lt;- chats$message_id\ntoken_docnames &lt;- docnames(chat_spacyr_toks)\n\n# Identify & exclude missing documents\nmissing_docs &lt;- setdiff(\n    original_docnames,\n    token_docnames)\nchats_filtered &lt;- chats %&gt;% \n  filter(!message_id %in% missing_docs)\n\n# Add docvars\ndocvars(chat_spacyr_toks) &lt;- chats_filtered\n\n# Subset tokens based on docvars\nmajority_report_chat_toks &lt;- tokens_subset(\n  chat_spacyr_toks,\n  streamer == \"the_majority_report\")\n\n# Output\nmajority_report_chat_toks\n\n\nTokens consisting of 24,708 documents and 33 docvars.\nChwKGkNJR2poT3pVdVlnREZha1FyUVlkblNrWS1B :\n[1] \"Donnie\"           \"say\"              \"sperm\"            \"Donnie_say\"      \n[5] \"say_sperm\"        \"Donnie_say_sperm\"\n\nChwKGkNLbXd3LXpVdVlnREZWd0wxZ0FkYW9FSWdB :\n [1] \"wait\"             \"drag\"             \"queen\"            \"Susan\"           \n [5] \"chat\"             \"wait_drag\"        \"drag_queen\"       \"queen_Susan\"     \n [9] \"Susan_chat\"       \"wait_drag_queen\"  \"drag_queen_Susan\" \"queen_Susan_chat\"\n\nChwKGkNKR1RsdV9VdVlnREZkNFhyUVlkZ2d3Tk5n :\n [1] \"person\"                          \"turqouise\"                      \n [3] \"waving::planet\"                  \"orange\"                         \n [5] \"purple\"                          \"ring\"                           \n [7] \"person_turqouise\"                \"turqouise_waving::planet\"       \n [9] \"waving::planet_orange\"           \"orange_purple\"                  \n[11] \"purple_ring\"                     \"person_turqouise_waving::planet\"\n[ ... and 3 more ]\n\nChwKGkNOQ3kxUExVdVlnREZVS1k1UWNkQ0t3Mlhn :\n[1] \"re\"           \"need\"         \"link\"         \"re_need\"      \"need_link\"   \n[6] \"re_need_link\"\n\nChwKGkNPcW5fZkxVdVlnREZlSFJsQWtkbThZaUtR :\n[1] \"praise\"     \"god\"        \"praise_god\"\n\nChwKGkNNUHZzdlhVdVlnREZha1FyUVlkblNrWS1B :\n[1] \"STREAM\"          \"start\"           \"15\"              \"STREAM_start\"   \n[5] \"start_15\"        \"STREAM_start_15\"\n\n[ reached max_ndoc ... 24,702 more documents ]"
  },
  {
    "objectID": "slides/slides-09.html#transformation-in-eine-dfm",
    "href": "slides/slides-09.html#transformation-in-eine-dfm",
    "title": "🔨 Topic Modeling",
    "section": "Transformation in eine DFM",
    "text": "Transformation in eine DFM\nUmsetzung zentraler Schritte: 1.Preprocessing\n\n# Convert to DFM\nmajority_report_chat_dfm &lt;- majority_report_chat_toks %&gt;% \n  dfm()\n\n# Output\nmajority_report_chat_dfm %&gt;%\n    print(max_nfeat = 4)\n\nDocument-feature matrix of: 24,708 documents, 84,931 features (&gt;99.99% sparse) and 33 docvars.\n                                          features\ndocs                                       donnie say sperm donnie_say\n  ChwKGkNJR2poT3pVdVlnREZha1FyUVlkblNrWS1B      1   1     1          1\n  ChwKGkNLbXd3LXpVdVlnREZWd0wxZ0FkYW9FSWdB      0   0     0          0\n  ChwKGkNKR1RsdV9VdVlnREZkNFhyUVlkZ2d3Tk5n      0   0     0          0\n  ChwKGkNOQ3kxUExVdVlnREZVS1k1UWNkQ0t3Mlhn      0   0     0          0\n  ChwKGkNPcW5fZkxVdVlnREZlSFJsQWtkbThZaUtR      0   0     0          0\n  ChwKGkNNUHZzdlhVdVlnREZha1FyUVlkblNrWS1B      0   0     0          0\n[ reached max_ndoc ... 24,702 more documents, reached max_nfeat ... 84,927 more features ]"
  },
  {
    "objectID": "slides/slides-09.html#pruning-der-dfm",
    "href": "slides/slides-09.html#pruning-der-dfm",
    "title": "🔨 Topic Modeling",
    "section": "Pruning der DFM",
    "text": "Pruning der DFM\nUmsetzung zentraler Schritte: 1.Preprocessing\n\n# Pruning\nmajority_report_chat_trim &lt;- majority_report_chat_dfm %&gt;% \n    dfm_trim(\n        min_docfreq = 50/nrow(chats),\n        max_docfreq = 0.99, \n        docfreq_type = \"prop\"\n   )\n\n# Output\nmajority_report_chat_trim %&gt;% \n    print(max_nfeat = 4)\n\nDocument-feature matrix of: 24,708 documents, 11,035 features (99.97% sparse) and 33 docvars.\n                                          features\ndocs                                       donnie say sperm wait\n  ChwKGkNJR2poT3pVdVlnREZha1FyUVlkblNrWS1B      1   1     1    0\n  ChwKGkNLbXd3LXpVdVlnREZWd0wxZ0FkYW9FSWdB      0   0     0    1\n  ChwKGkNKR1RsdV9VdVlnREZkNFhyUVlkZ2d3Tk5n      0   0     0    0\n  ChwKGkNOQ3kxUExVdVlnREZVS1k1UWNkQ0t3Mlhn      0   0     0    0\n  ChwKGkNPcW5fZkxVdVlnREZlSFJsQWtkbThZaUtR      0   0     0    0\n  ChwKGkNNUHZzdlhVdVlnREZha1FyUVlkblNrWS1B      0   0     0    0\n[ reached max_ndoc ... 24,702 more documents, reached max_nfeat ... 11,031 more features ]"
  },
  {
    "objectID": "slides/slides-09.html#konvertierung-für-stm-topic-modeling",
    "href": "slides/slides-09.html#konvertierung-für-stm-topic-modeling",
    "title": "🔨 Topic Modeling",
    "section": "Konvertierung für stm Topic Modeling",
    "text": "Konvertierung für stm Topic Modeling\nUmsetzung zentraler Schritte: 1.Preprocessing\n\n# Convert for stm topic modeling\nmajority_report_chat_stm &lt;- majority_report_chat_trim %&gt;% \n   convert(to = \"stm\")\n\nWarning in dfm2stm(x, docvars, omit_empty = TRUE): Dropped 24,708 empty\ndocument(s)\n\n# Output\nmajority_report_chat_stm %&gt;% summary()\n\n          Length Class      Mode     \ndocuments 23060  -none-     list     \nvocab     11035  -none-     character\nmeta         33  data.frame list"
  },
  {
    "objectID": "slides/slides-09.html#entscheidungen-über-entscheidungen",
    "href": "slides/slides-09.html#entscheidungen-über-entscheidungen",
    "title": "🔨 Topic Modeling",
    "section": "Entscheidungen über Entscheidungen",
    "text": "Entscheidungen über Entscheidungen\nUmsetzung zentraler Schritte: 2.Modell-Einstellung\n\nWelches Verfahren bzw. welchen Algorithmus wählen?\n\nMatrixfactorisierung (LSA, NMF)\nProbabilistische Modelle (LDA, CTM, STM)\nDeep Learning (BERT, GPT-2)\n\nWelche Parameter bzw. Hyperparameter sind wie zu berücksichtigen?\n\nAnzahl der Iterationen\nSeed für Reproduzierbarkeit\nInitialisierungsmethode\n\nWie viele Themen (K) sollen identifiziert werden?"
  },
  {
    "objectID": "slides/slides-09.html#die-suche-nach-der-optimalen-anzahl-von-themen",
    "href": "slides/slides-09.html#die-suche-nach-der-optimalen-anzahl-von-themen",
    "title": "🔨 Topic Modeling",
    "section": "Die Suche nach der optimalen Anzahl von Themen",
    "text": "Die Suche nach der optimalen Anzahl von Themen\nUmsetzung zentraler Schritte: 2.Modell-Einstellung\n\nWahl von K (ob das Modell angewiesen wird, 5, 15 oder 100 Themen zu identifizieren) hat erheblichen Einfluss auf die Ergebnisse:\n\nje kleiner K, desto breiter und allgemeiner sind die Themen\nje größer K, desto feinkörniger und spezifischer, aber auch überlappender und weniger exklusiv sind\n\nkeine allgemeingültige Lösung für die Bestimmung, da abhängig von vielen Faktoren, z.B.\n\nals was Themen im Kontext der Analyse theoretisch definiert sind\ndie Beschaffenheit des Korpus"
  },
  {
    "objectID": "slides/slides-09.html#how-to-find-k",
    "href": "slides/slides-09.html#how-to-find-k",
    "title": "🔨 Topic Modeling",
    "section": "How to find K",
    "text": "How to find K\n2.Modell-Einstellung: Suche nach dem Modell mit dem optimalen K\n\nDas stm-Paket (v1.3.7, Roberts et al., 2019) bietet zwei integrierte Lösungen, um das optimale K zu finden:\n\nsearchK() Funktion\nVerwendung des Argumentes K = 0 bei der Schätzung des Modells\nEmpfehlung: (Manuelles) Training und Bewertung!\n\nEntscheidung basiert u.a. auf:\n\nStastischem Fit (z.B. Coherence, Perplexity)\nInterpretierbarkeit (z.B. Top Features, Top Documents)\nRank-1-Metrik (z.B. Häufigkeit bestimmter Themen)"
  },
  {
    "objectID": "slides/slides-09.html#manuell-trainiert-exploriert",
    "href": "slides/slides-09.html#manuell-trainiert-exploriert",
    "title": "🔨 Topic Modeling",
    "section": "Manuell trainiert & exploriert",
    "text": "Manuell trainiert & exploriert\nUmsetzung zentraler Schritte: 2.Modell-Einstellung\n\n\n\n\n# Set up parallel processing using furrr\nfuture::plan(future::multisession()) \n\n# Estimate models\nstm_search  &lt;- tibble(\n    k = seq(from = 4, to = 20, by = 2)\n    ) %&gt;%\n    mutate(\n        mdl = furrr::future_map(\n            k, \n            ~stm::stm(\n                documents = majority_report_chat_stm$documents,\n                vocab = majority_report_chat_stm$vocab, \n                prevalence =~ platform + debate + message_during_debate, \n                K = ., \n                seed = 42,\n                max.em.its = 1000,\n                data = majority_report_chat_stm$meta,\n                init.type = \"Spectral\",\n                verbose = TRUE),\n            .options = furrr::furrr_options(seed = 42)\n            )\n    )\n\n\n\nstm_search$mdl\n\n[[1]]\nA topic model with 4 topics, 23060 documents and a 11035 word dictionary.\n\n[[2]]\nA topic model with 6 topics, 23060 documents and a 11035 word dictionary.\n\n[[3]]\nA topic model with 8 topics, 23060 documents and a 11035 word dictionary.\n\n[[4]]\nA topic model with 10 topics, 23060 documents and a 11035 word dictionary.\n\n[[5]]\nA topic model with 12 topics, 23060 documents and a 11035 word dictionary.\n\n[[6]]\nA topic model with 14 topics, 23060 documents and a 11035 word dictionary.\n\n[[7]]\nA topic model with 16 topics, 23060 documents and a 11035 word dictionary.\n\n[[8]]\nA topic model with 18 topics, 23060 documents and a 11035 word dictionary.\n\n[[9]]\nA topic model with 20 topics, 23060 documents and a 11035 word dictionary."
  },
  {
    "objectID": "slides/slides-09.html#berechnung-der-modell-diagnostik",
    "href": "slides/slides-09.html#berechnung-der-modell-diagnostik",
    "title": "🔨 Topic Modeling",
    "section": "Berechnung der Modell-Diagnostik",
    "text": "Berechnung der Modell-Diagnostik\n2.Modell-Einstellung: Suche nach dem Modell mit dem optimalen K\n\n# Create heldout\nheldout &lt;- make.heldout(\n  majority_report_chat_stm$documents,\n  majority_report_chat_stm$vocab,\n  seed = 42)\n\n# Create model diagnostics\nstm_results &lt;- stm_search %&gt;%\n  mutate(\n    exclusivity = map(mdl, exclusivity),\n    semantic_coherence = map(mdl, semanticCoherence, majority_report_chat_stm$documents),\n    eval_heldout = map(mdl, eval.heldout, heldout$missing),\n    residual = map(mdl, checkResiduals, majority_report_chat_stm$documents),\n    bound =  map_dbl(mdl, function(x) max(x$convergence$bound)),\n    lfact = map_dbl(mdl, function(x) lfactorial(x$settings$dim$K)),\n    lbound = bound + lfact,\n    iterations = map_dbl(mdl, function(x) length(x$convergence$bound))\n    )"
  },
  {
    "objectID": "slides/slides-09.html#überblick-über-modell-diagnostik",
    "href": "slides/slides-09.html#überblick-über-modell-diagnostik",
    "title": "🔨 Topic Modeling",
    "section": "Überblick über Modell-Diagnostik",
    "text": "Überblick über Modell-Diagnostik\n2.Modell-Einstellung: Suche nach dem Modell mit dem optimalen K\n\nstm_results\n\n# A tibble: 9 × 10\n      k mdl    exclusivity semantic_coherence eval_heldout residual        bound\n  &lt;dbl&gt; &lt;list&gt; &lt;list&gt;      &lt;list&gt;             &lt;list&gt;       &lt;list&gt;          &lt;dbl&gt;\n1     4 &lt;STM&gt;  &lt;dbl [4]&gt;   &lt;dbl [4]&gt;          &lt;named list&gt; &lt;named list&gt; -745060.\n2     6 &lt;STM&gt;  &lt;dbl [6]&gt;   &lt;dbl [6]&gt;          &lt;named list&gt; &lt;named list&gt; -739258.\n3     8 &lt;STM&gt;  &lt;dbl [8]&gt;   &lt;dbl [8]&gt;          &lt;named list&gt; &lt;named list&gt; -734111.\n4    10 &lt;STM&gt;  &lt;dbl [10]&gt;  &lt;dbl [10]&gt;         &lt;named list&gt; &lt;named list&gt; -729647.\n5    12 &lt;STM&gt;  &lt;dbl [12]&gt;  &lt;dbl [12]&gt;         &lt;named list&gt; &lt;named list&gt; -725092.\n6    14 &lt;STM&gt;  &lt;dbl [14]&gt;  &lt;dbl [14]&gt;         &lt;named list&gt; &lt;named list&gt; -723050.\n7    16 &lt;STM&gt;  &lt;dbl [16]&gt;  &lt;dbl [16]&gt;         &lt;named list&gt; &lt;named list&gt; -722053.\n8    18 &lt;STM&gt;  &lt;dbl [18]&gt;  &lt;dbl [18]&gt;         &lt;named list&gt; &lt;named list&gt; -718276.\n9    20 &lt;STM&gt;  &lt;dbl [20]&gt;  &lt;dbl [20]&gt;         &lt;named list&gt; &lt;named list&gt; -719322.\n# ℹ 3 more variables: lfact &lt;dbl&gt;, lbound &lt;dbl&gt;, iterations &lt;dbl&gt;"
  },
  {
    "objectID": "slides/slides-09.html#kurzer-crashkurs",
    "href": "slides/slides-09.html#kurzer-crashkurs",
    "title": "🔨 Topic Modeling",
    "section": "Kurzer Crashkurs",
    "text": "Kurzer Crashkurs\nÜberblick über die verschiedenen Evaluationskritierien\n\nHeld-Out Likelihood misst, wie gut ein Modell ungesehene Daten vorhersagt (ABER: kein allgemeingültiger Schwellenwert, nur Vergleich identischer Daten). Höhere Werte weisen auf eine bessere Vorhersageleistung hin.\nLower bound ist eine Annäherung an die Log-Likelihood des Modells. Ein höherer Wert deutet auf eine bessere Anpassung an die Daten hin.\nResiduen geben die Differenz zwischen den beobachteten und den vorhergesagten Werten an. Kleinere Residuen deuten auf eine bessere Modellanpassung hin. Im Idealfall sollten die Residuen so klein wie möglich sein.\nSemantische Kohärenz misst, wie semantisch verwandt die wichtigsten Wörter eines Themas sind, wobei höhere Werte auf kohärentere Themen hinweisen."
  },
  {
    "objectID": "slides/slides-09.html#vergleich-des-statistischen-fits",
    "href": "slides/slides-09.html#vergleich-des-statistischen-fits",
    "title": "🔨 Topic Modeling",
    "section": "Vergleich des statistischen Fits",
    "text": "Vergleich des statistischen Fits\n2.Modell-Einstellung: Suche nach dem Modell mit dem optimalen K\n\n\nExpand for full code\n# Visualize\nstm_results %&gt;%\n  transmute(\n    k,\n    `Lower bound` = lbound,\n    Residuals = map_dbl(residual, \"dispersion\"),\n    `Semantic coherence` = map_dbl(semantic_coherence, mean),\n    `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\")) %&gt;%\n  gather(Metric, Value, -k) %&gt;%\n  ggplot(aes(k, Value, color = Metric)) +\n    geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +\n    geom_point(size = 3) +\n    scale_x_continuous(breaks = seq(from = 4, to = 20, by = 2)) +\n    facet_wrap(~Metric, scales = \"free_y\") +\n    labs(x = \"K (Anzahl der Themen)\",\n         y = NULL,\n         title = \"Statistischer Fit der STM-Modelle\",\n         subtitle = \"Kohärenz sollte hoch, Residuen niedrig sein\"\n    ) +\n    theme_pubr()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "slides/slides-09.html#hohe-kohärenz-bei-hoher-exklusivität",
    "href": "slides/slides-09.html#hohe-kohärenz-bei-hoher-exklusivität",
    "title": "🔨 Topic Modeling",
    "section": "Hohe Kohärenz bei hoher Exklusivität",
    "text": "Hohe Kohärenz bei hoher Exklusivität\n2.Modell-Einstellung: Suche nach dem Modell mit dem optimalen K\n\n\nExpand for full code\n# Models for comparison\nmodels_for_comparison = c(12, 14, 18)\n\n# Create figures\nfig_excl &lt;- stm_results %&gt;% \n  # Edit data\n  select(k, exclusivity, semantic_coherence) %&gt;%\n  filter(k %in% models_for_comparison) %&gt;%\n  unnest(cols = c(exclusivity, semantic_coherence))  %&gt;%\n  mutate(k = as.factor(k)) %&gt;%\n  # Build graph\n  ggplot(aes(semantic_coherence, exclusivity, color = k)) +\n    geom_point(size = 2, alpha = 0.7) +\n    labs(\n      x = \"Semantic coherence\",\n      y = \"Exclusivity\"\n      # title = \"Comparing exclusivity and semantic coherence\",\n      # subtitle = \"Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity\"\n      ) +\n      theme_pubr()  \n\n# Create plotly\nfig_excl %&gt;% plotly::ggplotly()"
  },
  {
    "objectID": "slides/slides-09.html#extraktion-der-beta--gamma-matrix",
    "href": "slides/slides-09.html#extraktion-der-beta--gamma-matrix",
    "title": "🔨 Topic Modeling",
    "section": "Extraktion der Beta- & Gamma-Matrix",
    "text": "Extraktion der Beta- & Gamma-Matrix\n2.Modell-Einstellung: Interpretierbarkeit der Top Features\n\n# Define model\ntpm_k14 &lt;- stm_results %&gt;% \n   filter(k == 14) |&gt; \n   pull(mdl) %&gt;% .[[1]]\n\n\n\n\n\ntpm_k14 %&gt;% \n  tidy(., matrix = \"frex\") \n\n# A tibble: 154,490 × 2\n   topic term                \n   &lt;int&gt; &lt;chr&gt;               \n 1     1 look_like           \n 2     1 look_like_go        \n 3     1 like_go             \n 4     1 hahahahahahaha      \n 5     1 look_like_s         \n 6     1 think_go            \n 7     1 check_moderator     \n 8     1 fact_check_moderator\n 9     1 moderator_fact      \n10     1 moderator_fact_check\n# ℹ 154,480 more rows\n\n\n\n\ntpm_k14 %&gt;% \n  tidy(.,matrix = \"gamma\", \n    document_names = names(majority_report_chat_stm$documents)\n    ) \n\n# A tibble: 322,840 × 3\n   document                                 topic   gamma\n   &lt;chr&gt;                                    &lt;int&gt;   &lt;dbl&gt;\n 1 ChwKGkNJR2poT3pVdVlnREZha1FyUVlkblNrWS1B     1 0.0261 \n 2 ChwKGkNLbXd3LXpVdVlnREZWd0wxZ0FkYW9FSWdB     1 0.0265 \n 3 ChwKGkNKR1RsdV9VdVlnREZkNFhyUVlkZ2d3Tk5n     1 0.0123 \n 4 ChwKGkNOQ3kxUExVdVlnREZVS1k1UWNkQ0t3Mlhn     1 0.0200 \n 5 ChwKGkNPcW5fZkxVdVlnREZlSFJsQWtkbThZaUtR     1 0.0232 \n 6 ChwKGkNNUHZzdlhVdVlnREZha1FyUVlkblNrWS1B     1 0.0236 \n 7 ChwKGkNLT1JuX2pVdVlnREZZX0FsQWtkcEw4Wmd3     1 0.434  \n 8 ChwKGkNLRElvZmpVdVlnREZaWExGZ2tkTy1ZSXVR     1 0.0118 \n 9 ChwKGkNNblNqZm5VdVlnREZhX0l3Z1FkZUg0bHZn     1 0.0356 \n10 ChwKGkNMeUkyUHZVdVlnREZXQUhyUVlkTUJvZ193     1 0.00307\n# ℹ 322,830 more rows"
  },
  {
    "objectID": "slides/slides-09.html#extraktion-der-top-features-nach-thema",
    "href": "slides/slides-09.html#extraktion-der-top-features-nach-thema",
    "title": "🔨 Topic Modeling",
    "section": "Extraktion der Top Features nach Thema",
    "text": "Extraktion der Top Features nach Thema\n2.Modell-Einstellung: Interpretierbarkeit der Top Features\n\n\n\n# Create gamma data\ntop_gamma_k14 &lt;- tpm_k14 %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::summarise(\n    gamma = mean(gamma),\n    .groups = \"drop\") %&gt;%\n  dplyr::arrange(desc(gamma))\n\n# Create beta data\ntop_beta_k14 &lt;- tpm_k14 %&gt;%\n  tidytext::tidy(.) %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::arrange(-beta) %&gt;%\n  dplyr::top_n(7, wt = beta) %&gt;% \n  dplyr::select(topic, term) %&gt;%\n  dplyr::summarise(\n    terms_beta = toString(term),\n    .groups = \"drop\")\n\n\n\n# Merge gamma & beta data\ntop_topics_terms_k14 &lt;- top_beta_k14 %&gt;% \n  dplyr::left_join(\n    top_gamma_k14, \n    by = \"topic\") %&gt;%\n  dplyr::mutate(\n          topic = paste0(\"Topic \", topic),\n          topic = reorder(topic, gamma)\n      )"
  },
  {
    "objectID": "slides/slides-09.html#beschreiben-top-features-ein-topic-sinnvoll",
    "href": "slides/slides-09.html#beschreiben-top-features-ein-topic-sinnvoll",
    "title": "🔨 Topic Modeling",
    "section": "Beschreiben Top Features ein Topic sinnvoll?",
    "text": "Beschreiben Top Features ein Topic sinnvoll?\n2.Modell-Einstellung: Interpretierbarkeit der Top Features\n\n\ntop_topics_terms_k14 %&gt;%\n  mutate(across(gamma, ~round(.,3))) %&gt;% \n  dplyr::arrange(-gamma) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(\n    table.width = gt::pct(90), \n    table.font.size = \"12px\"\n    )\n\n\n\n\n\n\n\n\ntopic\nterms_beta\ngamma\n\n\n\n\nTopic 8\nmake, 's, lul, emma, fuchsia, liar, kekl\n0.115\n\n\nTopic 7\ngood, right, now, yes, plan, lie, bad\n0.113\n\n\nTopic 12\nkamala, want, biden, eat, take, vote, god\n0.109\n\n\nTopic 5\nget, s, wow, mad, omg, thank, nice\n0.101\n\n\nTopic 4\nlmao, omegalul, red, green, orange, baby, kekw\n0.079\n\n\nTopic 3\ntime, sam, love, man, need, old, big\n0.075\n\n\nTopic 11\nsay, oh, ..., know, look, shit, yeah\n0.075\n\n\nTopic 1\ngo, like, fact, debate, look, check, keep\n0.058\n\n\nTopic 13\ntrump, just, donald, lose, racist, win, can\n0.056\n\n\nTopic 9\nlol, one, give, ...., wtf, china, okay\n0.051\n\n\nTopic 6\npeople, think, go, back, work, try, change\n0.050\n\n\nTopic 10\nlet, talk, ’s, can, like, sound, see\n0.045\n\n\nTopic 2\nstop, start, please, israel, use, laugh, agree\n0.039\n\n\nTopic 14\nface, guy, don, bring, real, country, rolling_on_the_floor_laughe\n0.034"
  },
  {
    "objectID": "slides/slides-09.html#extraktion-zusammenführung-der-daten",
    "href": "slides/slides-09.html#extraktion-zusammenführung-der-daten",
    "title": "🔨 Topic Modeling",
    "section": "Extraktion & Zusammenführung der Daten",
    "text": "Extraktion & Zusammenführung der Daten\n2.Modell-Einstellung: Interpretierbarkeit der Top Documents\n\n\n\n# Prepare for merging\ntopic_gammas_k14 &lt;- tpm_k14 %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(document) %&gt;% \n  tidyr::pivot_wider(\n    id_cols = document, \n    names_from = \"topic\", \n    names_prefix = \"gamma_topic_\",\n    values_from = \"gamma\")\n      \ngammas_k14 &lt;- tpm_k14 %&gt;%\n  tidytext::tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(document) %&gt;% \n  dplyr::slice_max(gamma) %&gt;% \n  dplyr::mutate(\n    main_topic = ifelse(\n      gamma &gt; 0.5, topic, NA)) %&gt;% \n  rename(\n    top_topic = topic,\n    top_gamma = gamma) %&gt;% \n  ungroup() %&gt;% \n  left_join(.,\n    topic_gammas_k14,\n    by = join_by(document))\n\n\n\n# Identify empty documents\nempty_docs &lt;- Matrix::rowSums(\n  as(majority_report_chat_trim, \"Matrix\")) == 0 \nempty_docs_ids &lt;- majority_report_chat_trim@docvars$docname[empty_docs]\n\n# Merge with original data\nchats_topics &lt;- chats_filtered %&gt;%\n  filter(!(message_id %in% empty_docs_ids)) %&gt;% \n  filter(streamer == \"the_majority_report\") %&gt;%   \n  bind_cols(gammas_k14) %&gt;% \n  select(-document)"
  },
  {
    "objectID": "slides/slides-09.html#angereicherter-datensatz",
    "href": "slides/slides-09.html#angereicherter-datensatz",
    "title": "🔨 Topic Modeling",
    "section": "Angereicherter Datensatz",
    "text": "Angereicherter Datensatz\n2.Modell-Einstellung: Interpretierbarkeit der Top Documents\n\nchats_topics %&gt;% glimpse\n\nRows: 23,060\nColumns: 50\n$ streamer              &lt;chr&gt; \"the_majority_report\", \"the_majority_report\", \"t…\n$ url                   &lt;chr&gt; \"https://www.youtube.com/watch?v=lzobJil9Sgc\", \"…\n$ platform              &lt;chr&gt; \"youtube\", \"youtube\", \"youtube\", \"youtube\", \"you…\n$ debate                &lt;chr&gt; \"presidential\", \"presidential\", \"presidential\", …\n$ user_name             &lt;chr&gt; \"Scott Plant\", \"Rebecca W\", \"Galactic News Netwo…\n$ user_id               &lt;chr&gt; \"UC4mxlnk193JrXVAp6K-vEpQ\", \"UCeenHJ1v62biyOyKwL…\n$ user_display_name     &lt;chr&gt; \"Scott Plant\", \"Rebecca W\", \"Galactic News Netwo…\n$ user_badges           &lt;list&gt; [], [], [], [], [], [], [], [], [], [], [], [],…\n$ message_timestamp     &lt;dbl&gt; -152, -151, -145, -138, -137, -132, -126, -126, …\n$ message_id            &lt;chr&gt; \"ChwKGkNJR2poT3pVdVlnREZha1FyUVlkblNrWS1B\", \"Chw…\n$ message_type          &lt;chr&gt; \"text_message\", \"text_message\", \"text_message\", …\n$ message_content       &lt;chr&gt; \"Donnie will say, \\\"That is my own sperm.\\\"\", \"w…\n$ message_emotes        &lt;list&gt; [], [], [[\"UCkszU2WH9gy1mb0dV-11UJg/ssIfY7OFG5O…\n$ message_length        &lt;int&gt; 40, 45, 52, 38, 10, 32, 8, 14, 2, 90, 20, 36, 20…\n$ message_timecode      &lt;Period&gt; -2M -32S, -2M -31S, -2M -25S, -2M -18S, -2M -…\n$ message_time          &lt;chr&gt; \"23:57:28\", \"23:57:29\", \"23:57:35\", \"23:57:42\", …\n$ message_during_debate &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_has_badge        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_premium       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_subscriber    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_turbo         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_moderator     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_partner       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_subgifter     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_broadcaster   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_vip           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_twitchdj      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_founder       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_staff         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_game_dev      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_ambassador    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_no_audio         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_no_video         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ top_topic             &lt;int&gt; 11, 7, 4, 3, 4, 3, 1, 4, 9, 4, 8, 3, 1, 1, 3, 13…\n$ top_gamma             &lt;dbl&gt; 0.4435422, 0.3412468, 0.7627751, 0.5663056, 0.46…\n$ main_topic            &lt;int&gt; NA, NA, 4, 3, NA, NA, NA, 4, NA, 4, NA, NA, NA, …\n$ gamma_topic_1         &lt;dbl&gt; 0.026098022, 0.026450828, 0.012260893, 0.0200424…\n$ gamma_topic_2         &lt;dbl&gt; 0.014058480, 0.016014175, 0.006519458, 0.1322111…\n$ gamma_topic_3         &lt;dbl&gt; 0.043655546, 0.066001729, 0.018185091, 0.5663056…\n$ gamma_topic_4         &lt;dbl&gt; 0.03876696, 0.14976529, 0.76277514, 0.03011074, …\n$ gamma_topic_5         &lt;dbl&gt; 0.186801763, 0.043801244, 0.020095565, 0.0373451…\n$ gamma_topic_6         &lt;dbl&gt; 0.021470740, 0.024622665, 0.009041711, 0.0174135…\n$ gamma_topic_7         &lt;dbl&gt; 0.036282513, 0.341246826, 0.017882159, 0.0289225…\n$ gamma_topic_8         &lt;dbl&gt; 0.04538521, 0.14311198, 0.06168206, 0.03558740, …\n$ gamma_topic_9         &lt;dbl&gt; 0.021910232, 0.023552979, 0.012143933, 0.0165138…\n$ gamma_topic_10        &lt;dbl&gt; 0.020656194, 0.020843309, 0.013562820, 0.0161670…\n$ gamma_topic_11        &lt;dbl&gt; 0.443542243, 0.027314995, 0.019249172, 0.0206369…\n$ gamma_topic_12        &lt;dbl&gt; 0.044397591, 0.044680183, 0.019709728, 0.0343713…\n$ gamma_topic_13        &lt;dbl&gt; 0.027631227, 0.037326873, 0.011473224, 0.0214522…\n$ gamma_topic_14        &lt;dbl&gt; 0.029343282, 0.035266919, 0.015419046, 0.0229199…"
  },
  {
    "objectID": "slides/slides-09.html#top-topic-im-fokus",
    "href": "slides/slides-09.html#top-topic-im-fokus",
    "title": "🔨 Topic Modeling",
    "section": "Top Topic im Fokus",
    "text": "Top Topic im Fokus\n2.Modell-Einstellung: Passen Top Document zum Thema?\n\n\nExpand for full code\nchats_topics %&gt;% \n  filter(top_topic == 8) %&gt;% \n  arrange(-top_gamma) %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(message_id, user_name, message_time, message_content, top_gamma, top_topic) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(table.font.size = \"10px\")\n\n\n\n\n\n\n\n\nmessage_id\nuser_name\nmessage_time\nmessage_content\ntop_gamma\ntop_topic\n\n\n\n\nChwKGkNKdlRqY1BwdVlnREZRREV3Z1FkV2I4U1hn\nDavid Davis\n01:29:52\n:watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon:\n0.9628609\n8\n\n\nChwKGkNLMlp3cUxzdVlnREZVN0NsQWtkT0JBRTN3\nJules Winnfeild 🏳️‍⚧️\n01:42:09\n:face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape:\n0.9583042\n8\n\n\nChwKGkNNclk0dFBZdVlnREZZYWg1UWNkUlhvNVB3\nCanalEduge\n00:14:24\n:face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape:\n0.9437816\n8\n\n\nChwKGkNJaVdpNVBmdVlnREZTV1Q1UWNkUWg0dEJn\nJules Winnfeild 🏳️‍⚧️\n00:43:27\n:face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape:\n0.9315330\n8\n\n\nChwKGkNORG1uTWpvdVlnREZkd3VyUVlkSVFrVU5R\n#BobbleHead\n01:25:34\nWORLDSTAR own's the Trademark on the Algorithm that identified all the Pedophiles = Blame T.M.Z. #ReleaseTheBlackBaby\n0.9313871\n8\n\n\nChwKGkNPV09fYmJwdVlnREZXc3ByUVlkbk9Vc3d3\nDavid Davis\n01:29:26\n:watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon:\n0.9303014\n8\n\n\nChwKGkNMV244NkRvdVlnREZUMFRyUVlkYmZzUmpB\n#BobbleHead\n01:24:12\nWORLDSTAR own's the Trademark on the Algorithm that identified all the Pedophiles = Blame T.M.Z.\n0.9287878\n8\n\n\nChwKGkNLN0N4ZTdldVlnREZWNDZyUVlkRmZJRklR\nCorporations8MyBaby\n00:42:10\n:face-fuchsia-tongue-out::face-fuchsia-tongue-out::face-fuchsia-tongue-out::face-fuchsia-tongue-out:\n0.9269656\n8\n\n\n9c014ab4-89a7-4f9d-97c8-be3da2868f58\nnightbot\n00:10:47\nJoin the official Majority Report discord community! https://discord.gg/majority\n0.9215614\n8\n\n\nfcb53a8b-4b75-4557-b3eb-d273b7069d88\nnightbot\n00:26:14\nJoin the official Majority Report discord community! https://discord.gg/majority\n0.9215614\n8"
  },
  {
    "objectID": "slides/slides-09.html#thema-12-im-fokus",
    "href": "slides/slides-09.html#thema-12-im-fokus",
    "title": "🔨 Topic Modeling",
    "section": "Thema 12 im Fokus",
    "text": "Thema 12 im Fokus\n2.Modell-Einstellung: Passen Top Document zum Thema?\n\n\nExpand for full code\nchats_topics %&gt;% \n  filter(top_topic == 12) %&gt;% \n  arrange(-top_gamma) %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(message_id, user_name, message_time, message_content, top_gamma, top_topic) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(table.font.size = \"10px\")\n\n\n\n\n\n\n\n\nmessage_id\nuser_name\nmessage_time\nmessage_content\ntop_gamma\ntop_topic\n\n\n\n\nChwKGkNNZXg5LUxxdVlnREZkcVc1UWNkeGpNTDJB\nSamSedersLeftTeste\n01:35:27\nThe vice president is BLACK BLACK BLACK BLACK BLACK BLACK\n0.9227423\n12\n\n\nChwKGkNNdVU0NTNndVlnREZkNEwxZ0FkbWxFSFN3\nRilly Kewl\n00:48:18\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNJU1BtS0RndVlnREZRREV3Z1FkV2I4U1hn\nRilly Kewl\n00:48:23\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNJS1NvcVRndVlnREZRMHUxZ0FkU1FFSzZB\nRilly Kewl\n00:48:31\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNLSG9uYUxodVlnREZhY0cxZ0FkSVJjSGdB\nRilly Kewl\n00:52:56\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNPbU90YV9odVlnREZWZ3FyUVlkaUpnNUpn\nRilly Kewl\n00:53:23\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNLYmxuZjdzdVlnREZiMHUxZ0FkT0owN0h3\nRilly Kewl\n01:45:21\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNLX3F4cl90dVlnREZWbzAxZ0FkdzVFTTR3\nRilly Kewl\n01:47:38\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNOYm1yTVB0dVlnREZWb0gxZ0FkQnF3QWRR\nRilly Kewl\n01:47:46\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nbad4de96-6c3f-4495-9bd5-da395d9af90b\ngrandshadowfox\n01:07:37\nGrandshadowfox subscribed with Prime. They've subscribed for 16 months! 15 months\n0.9064436\n12"
  },
  {
    "objectID": "slides/slides-09.html#thema-4-im-fokus",
    "href": "slides/slides-09.html#thema-4-im-fokus",
    "title": "🔨 Topic Modeling",
    "section": "Thema 4 im Fokus",
    "text": "Thema 4 im Fokus\n2.Modell-Einstellung: Passen Top Document zum Thema?\n\n\nExpand for full code\nchats_topics %&gt;% \n  filter(top_topic == 4) %&gt;% \n  arrange(-top_gamma) %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(message_id, user_name, message_time, message_content, top_gamma, top_topic) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(table.font.size = \"10px\")\n\n\n\n\n\n\n\n\nmessage_id\nuser_name\nmessage_time\nmessage_content\ntop_gamma\ntop_topic\n\n\n\n\nChwKGkNLX3Z3SzdkdVlnREZhb0NyUVlkVER3aVRn\nrhys\n00:35:28\n:text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over:\n0.9687024\n4\n\n\nChwKGkNJN1hsSXJudVlnREZWTGNGZ2tkYnFnYmJB\nJules Winnfeild 🏳️‍⚧️\n01:18:56\n:fish-orange-wide-eyes::fish-orange-wide-eyes::fish-orange-wide-eyes::fish-orange-wide-eyes::fish-orange-wide-eyes:\n0.9649579\n4\n\n\nChwKGkNJQ3ZzYTdXdVlnREZSek1GZ2tkMndnZ1Bn\nfish Monger\n00:04:09\nideas:finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one:\n0.9647715\n4\n\n\nChwKGkNNcmVxb0RmdVlnREZhb0NyUVlkVER3aVRn\nrhys\n00:42:48\n:text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over:\n0.9630820\n4\n\n\n32d36382-5eaf-4da6-a2dc-c9683b98162b\nnightbot\n00:01:27\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\n4ffbae78-db39-40e9-bcf8-b5c0965fe2a4\nnightbot\n00:09:42\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\na08570c3-f835-4568-9332-b97bf22ee61b\nnightbot\n02:01:22\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\n46b82320-e59d-486e-a58f-acf35b03fe4a\nnightbot\n02:09:43\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\n191d1514-cc7e-4a65-8c9e-0ce5d28f1a5d\nnightbot\n02:22:30\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\ned759097-6071-4394-b810-5adafd52f652\nnightbot\n02:35:23\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4"
  },
  {
    "objectID": "slides/slides-09.html#fließender-übergang-in-die-analyse",
    "href": "slides/slides-09.html#fließender-übergang-in-die-analyse",
    "title": "🔨 Topic Modeling",
    "section": "Fließender Übergang in die Analyse",
    "text": "Fließender Übergang in die Analyse\nUmsetzung zentraler Schritte: 3.Analyse & Interpretation\nstm ermöglicht den Einfluss unabhängiger Variablen zu modellieren, genauer auf:\n\ndie Prävalenz von Themen (prevalence-Argument)\nden Inhalt von Themen (content-Argument)\n\nInterpreation:\n\nIdentifikation & Ausschluss von „Background“-Topics\nIdentifikation & Labelling von relevanten Topics\nGgf. Gruppierung in übergreifende Kontexte (z.B. „politische Themen“)\nNutzung für deskriptive oder inferenzstatistische Verfahren"
  },
  {
    "objectID": "slides/slides-09.html#user-mit-den-meisten-beiträgen-zu-thema-4",
    "href": "slides/slides-09.html#user-mit-den-meisten-beiträgen-zu-thema-4",
    "title": "🔨 Topic Modeling",
    "section": "User mit den meisten Beiträgen zu Thema 4",
    "text": "User mit den meisten Beiträgen zu Thema 4\n3.Analyse & Interpretation - Beispiel für deskriptive Verfahren\n\n\nchats_topics %&gt;% \n  filter(top_topic == 8) %&gt;% \n  count(user_name, sort = TRUE) %&gt;% \n  mutate(\n    prop = round(n/sum(n)*100, 2)) %&gt;% \n  slice_head(n = 10) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() \n\n\n\n\n\n\n\n\nuser_name\nn\nprop\n\n\n\n\nbuuuuuuuuuuuuuuuuuuuuuut\n59\n1.83\n\n\nsauvignoncitizen\n50\n1.55\n\n\nSay What\n49\n1.52\n\n\nJules Winnfeild 🏳️‍⚧️\n47\n1.45\n\n\nasiak\n46\n1.42\n\n\nhardradajm\n40\n1.24\n\n\nBob Carmody\n34\n1.05\n\n\nT.R.\n33\n1.02\n\n\nmaj_k1bbles\n31\n0.96\n\n\nogdimwit\n31\n0.96"
  },
  {
    "objectID": "slides/slides-09.html#prävalenz-vs.-häufigkeit",
    "href": "slides/slides-09.html#prävalenz-vs.-häufigkeit",
    "title": "🔨 Topic Modeling",
    "section": "Prävalenz vs. Häufigkeit",
    "text": "Prävalenz vs. Häufigkeit\n3.Analyse & Interpretation - Beispiel für deskriptive Verfahren\n\n\n\n\nExpand for full code\ntop_gamma_k14 %&gt;% \n  ggplot(aes(as.factor(topic), gamma)) +\n  geom_col(fill = \"#F57350\") +\n  labs(\n    x = \"Topic\",\n    y = \"Mean gamma\"\n  ) +\n  coord_flip() +\n  scale_y_reverse() +\n  scale_x_discrete(position = \"top\") +\n  theme_pubr()\n\n\n\n\n\n\n\n\n\n\n\n\nExpand for full code\nchats_topics %&gt;% \n  mutate(across(top_topic, as.factor)) %&gt;% \n  ggplot(aes(top_topic, y = after_stat(prop), group = 1)) +\n  geom_bar(fill = \"#1DA1F2\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    x = \"\", \n    y = \"Relative frequency\"\n  ) +\n  coord_flip() +\n  theme_pubr()"
  },
  {
    "objectID": "slides/slides-09.html#einfluss-von-meta-variablen",
    "href": "slides/slides-09.html#einfluss-von-meta-variablen",
    "title": "🔨 Topic Modeling",
    "section": "Einfluss von Meta-Variablen",
    "text": "Einfluss von Meta-Variablen\n3.Analyse & Interpretation - Beispiel für inferenzstatistische Verfahren\n\neffects &lt;- estimateEffect(\n  formula =~ platform + debate + message_during_debate,\n  stmobj = tpm_k14, \n  metadata = chats_topics)\n\nWarning in estimateEffect(formula = ~platform + debate + message_during_debate, : Covariate matrix is singular.  See the details of ?estimateEffect() for some common causes.\n             Adding a small prior 1e-5 for numerical stability.\n\n\n\n\n\n\nsummary(effects, topics = 12)\n\n\nCall:\nestimateEffect(formula = ~platform + debate + message_during_debate, \n    stmobj = tpm_k14, metadata = chats_topics)\n\n\nTopic 12:\n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)              0.130299  29.076022   0.004  0.99642   \nplatformyoutube         -0.044308  29.075963  -0.002  0.99878   \ndebatevice presidential -0.054949  29.075974  -0.002  0.99849   \nmessage_during_debate    0.012174   0.004523   2.691  0.00712 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nsummary(effects, topics = 8)\n\n\nCall:\nestimateEffect(formula = ~platform + debate + message_during_debate, \n    stmobj = tpm_k14, metadata = chats_topics)\n\n\nTopic 8:\n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)              0.343159  29.348608   0.012    0.991\nplatformyoutube         -0.253151  29.348565  -0.009    0.993\ndebatevice presidential -0.233550  29.348558  -0.008    0.994\nmessage_during_debate    0.001028   0.004541   0.226    0.821"
  },
  {
    "objectID": "slides/slides-09.html#shiny-app-als-hilfe-für-die-analyse",
    "href": "slides/slides-09.html#shiny-app-als-hilfe-für-die-analyse",
    "title": "🔨 Topic Modeling",
    "section": "Shiny-App als Hilfe für die Analyse",
    "text": "Shiny-App als Hilfe für die Analyse\nVisualisierung mit stminsights (v4.1.0, Schwemmer, 2021)"
  },
  {
    "objectID": "slides/slides-09.html#die-4-rs",
    "href": "slides/slides-09.html#die-4-rs",
    "title": "🔨 Topic Modeling",
    "section": "Die 4 R`s",
    "text": "Die 4 R`s\nUmsetzung zentraler Schritte: 4.Validierung\n\nReliabilität/Robustheit: Kommen wir mit anderen Instrumenten zu ähnlichen Ergebnissen? (Roberts et al., 2016; Wilkerson & Casas, 2017)\nReproduzierbarkeit: Können wir mit den gleichen Daten & Instrumenten die Ergebnisse reproduzieren? (Chung-hong Chan et al., 2024)\nReplizierbarkeit: Lassen sich unsere Ergebnisse für andere Daten reproduzieren? (Breuer & Haim, 2024; Long, 2021)\n\n\nReliabilität/Robustheit: Add Graphik aus Hase Reproduzierbarkeit: Open Source Software nutzen, mit z.B. „Quarto“ arbeiten (sequenzielle Reihenfolge der Codeausführung garantieren!), Kompendium (Code & Daten in einheitlicher Struktur; Docker), Abhängigkeiten, z.B. von Paket-Versionen, reduzieren Replizierbarkeit: Präregistrierung, auf statistische Power achten (Poweranalyse, z. B. mit Simulationen?), selbst exakte/konzeptuelle Replikationen durchführen"
  },
  {
    "objectID": "slides/slides-09.html#messen-wir-was-wir-messen-wollen",
    "href": "slides/slides-09.html#messen-wir-was-wir-messen-wollen",
    "title": "🔨 Topic Modeling",
    "section": "Messen wir, was wir messen wollen",
    "text": "Messen wir, was wir messen wollen\nVerschiedenen Möglichkeit der Qualitätssicherung\n\nValidierung hilft zu verstehen, wo wir falsch liegen und wie falsch wir liegen.\nQualitätssicherung z.B. via (Jana Bernhard et al., 2023; Quinn et al., 2009) …\n\nTheoretischer (!) Ableitung von Messungen (Chen et al., 2023)\nVergleich mit manueller Codierung (z.B. Chan & Sältzer, 2020)\nVergleich mit externen Ereignissen"
  },
  {
    "objectID": "slides/slides-09.html#validieren-validieren-validieren",
    "href": "slides/slides-09.html#validieren-validieren-validieren",
    "title": "🔨 Topic Modeling",
    "section": "Validieren, Validieren, Validieren",
    "text": "Validieren, Validieren, Validieren\nKritisiche Anmerkungen zum Topic Modeling\n\nAutomated text analysis methods can substantially reduce the costs and time of analyzing massive collections of political texts. When applied to any one problem, however, the output of the models may be misleading or simply wrong. […] What should be avoided, then, is the blind use of any method without a validation step. (Grimmer & Stewart, 2013, p. S.5)\n\n\nKlassifikationsmodell klassifiziert alle Dokumente, ein Diktionär spuckt für jedes Dokument ein Ergebnis aus, ein Topic Model findet immer die vorgegebene Anzahl an Themen.\nOb es sich dabei auch um inhaltlich sinnvolle Ergebnisse handelt, kann und muss durch manuelle Validierungen festgestellt werden.\nModerne Verfahren (z.B. BERT) potentiell besser geeignet für bestimmte Texte."
  },
  {
    "objectID": "slides/slides-09.html#and-now-you",
    "href": "slides/slides-09.html#and-now-you",
    "title": "🔨 Topic Modeling",
    "section": "🧪 And now … you!",
    "text": "🧪 And now … you!\nNext steps\n\nLaden das .zip-Archiv stm_session_09.RData.zip von StudOn herunter und entpacke die Dateien an einen Ort deiner Wahl.\nÖffnet RStudio.\nFührt folgenden Code-Chunk aus:\n\n\ninstall.packages(\"stminsights\")\nlibrary(stminsights)\nrun_stminsights()\n\n\nLadet den Datensatz in die App.\nMacht euch mit den verschiedenen Funtionen der App vertraut und versucht, die Ergebnisse aus der Sitzung zu reproduzieren."
  },
  {
    "objectID": "slides/slides-09.html#references",
    "href": "slides/slides-09.html#references",
    "title": "🔨 Topic Modeling",
    "section": "References",
    "text": "References\n\n\nBreuer, J., & Haim, M. (2024). Are we replicating yet? Reproduction and replication in communication research. Media and Communication, 12. https://doi.org/10.17645/mac.8382\n\n\nChan, C., & Sältzer, M. (2020). Oolong: An r package for validating automated content analysis tools. Journal of Open Source Software, 5(55), 2461. https://doi.org/10.21105/joss.02461\n\n\nChen, Y., Peng, Z., Kim, S.-H., & Choi, C. W. (2023). What We Can Do and Cannot Do with Topic Modeling: A Systematic Review. Communication Methods and Measures, 17(2), 111–130. https://doi.org/10.1080/19312458.2023.2167965\n\n\nChung-hong Chan, Tim Schatto-Eckrodt, & Johannes Gruber. (2024). What makes computational communication science (ir)reproducible? Computational Communication Research, 6(1), 1. https://doi.org/10.5117/ccr2024.1.5.chan\n\n\nDenny, M. J., & Spirling, A. (2018). Text Preprocessing For Unsupervised Learning: Why It Matters, When It Misleads, And What To Do About It. Political Analysis, 26(2), 168–189. https://doi.org/10.1017/pan.2017.44\n\n\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267–297. https://doi.org/10/f458q9\n\n\nJana Bernhard, Martin Teuffenbach, & Hajo G. Boomgaarden. (2023). Topic Model Validation Methods and their Impact on Model Selection and Evaluation. Computational Communication Research, 5(1), 1. https://doi.org/10.5117/ccr2023.1.13.bern\n\n\nLong, J. A. (2021). Improving the replicability and generalizability of inferences in quantitative communication research. Annals of the International Communication Association, 45(3), 207–220. https://doi.org/10.1080/23808985.2021.1979421\n\n\nMaier, D., Niekler, A., Wiedemann, G., & Stoltenberg, D. (2020). How Document Sampling and Vocabulary Pruning Affect the Results of Topic Models. Computational Communication Research, 2(2), 139–152. https://doi.org/10.5117/ccr2020.2.001.maie\n\n\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., Häussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93–118. https://doi.org/10.1080/19312458.2018.1430754\n\n\nQuinn, K. M., Monroe, B. L., Colaresi, M., Crespin, M. H., & Radev, D. R. (2009). How to Analyze Political Attention with Minimal Assumptions and Costs. American Journal of Political Science, 54(1), 209–228. https://doi.org/10.1111/j.1540-5907.2009.00427.x\n\n\nRoberts, M. E., Stewart, B. M., & Tingley, D. (2016). Navigating the local modes of big data: The case of topic models (pp. 51–97). Cambridge University Press. https://doi.org/10.1017/cbo9781316257340.004\n\n\nRoberts, M. E., Stewart, B. M., & Tingley, D. (2019). stm: An R Package for Structural Topic Models. Journal of Statistical Software, 91(1), 1–40. https://doi.org/10.18637/jss.v091.i02\n\n\nSchwemmer, C. (2021). Stminsights: A shiny application for inspecting structural topic models. https://github.com/cschwem2er/stminsights\n\n\nWilkerson, J., & Casas, A. (2017). Large-Scale Computerized Text Analysis in Political Science: Opportunities and Challenges. Annual Review of Political Science, 20(1), 529–544. https://doi.org/10.1146/annurev-polisci-052615-025542"
  },
  {
    "objectID": "sessions/session-02.html",
    "href": "sessions/session-02.html",
    "title": "Session 2",
    "section": "",
    "text": "✍️ Start working on R-Video-Tutorials.\n✍️ Book your time slot for the mandatory feedback session on StudOn.\n✍️ Remeber the registration deadline (28./29.10) for the seminar examination on campo."
  },
  {
    "objectID": "sessions/session-02.html#prepare",
    "href": "sessions/session-02.html#prepare",
    "title": "Session 2",
    "section": "",
    "text": "✍️ Start working on R-Video-Tutorials.\n✍️ Book your time slot for the mandatory feedback session on StudOn.\n✍️ Remeber the registration deadline (28./29.10) for the seminar examination on campo."
  },
  {
    "objectID": "sessions/session-02.html#participate",
    "href": "sessions/session-02.html#participate",
    "title": "Session 2",
    "section": "Participate",
    "text": "Participate\n🖥️ Session 02"
  },
  {
    "objectID": "sessions/session-02.html#suggested-readings",
    "href": "sessions/session-02.html#suggested-readings",
    "title": "Session 2",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nDavidson, B. I., Wischerath, D., Racek, D., Parry, D. A., Godwin, E., Hinds, J., Van Der Linden, D., Roscoe, J. F., Ayravainen, L. E. M., & Cork, A. (2023). Platform-controlled social media APIs threaten open science. https://osf.io/ps32z\nDriel, I. I. van, Giachanou, A., Pouwels, J. L., Boeschoten, L., Beyens, I., & Valkenburg, P. M. (2022). Promises and Pitfalls of Social Media Data Donations. Communication Methods and Measures, 1–17. https://doi.org/10.1080/19312458.2022.2109608\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Digital trace data (1st ed., pp. 100–118). Routledge. https://www.taylorfrancis.com/books/9781003024583/chapters/10.4324/9781003024583-8\nOlteanu, A., Castillo, C., Diaz, F., & Kıcıman, E. (2019). Social data: Biases, methodological pitfalls, and ethical boundaries. Frontiers in Big Data, 2, 13. https://doi.org/10.3389/fdata.2019.00013\nReeves, B., Ram, N., Robinson, T. N., Cummings, J. J., Giles, C. L., Pan, J., Chiatti, A., Cho, M., Roehrick, K., Yang, X., Gagneja, A., Brinberg, M., Muise, D., Lu, Y., Luo, M., Fitzgerald, A., & Yeykelis, L. (2021). Screenomics : A Framework to Capture and Analyze Personal Life Experiences and the Ways that Technology Shapes Them. HumanComputer Interaction, 36(2), 150–201. https://doi.org/10.1080/07370024.2019.1578652"
  },
  {
    "objectID": "sessions/session-02.html#useful-tools-resources",
    "href": "sessions/session-02.html#useful-tools-resources",
    "title": "Session 2",
    "section": "Useful tools & resources",
    "text": "Useful tools & resources\n\nPeeters, S., & Hagen, S. (2022). The 4CAT Capture and Analysis Toolkit: A Modular Tool for Transparent and Traceable Social Media Research. Computational Communication Research, 4(2), 571–589. https://doi.org/10.5117/ccr2022.2.007.hage\nPeeters, S. (2022). Zeeschuimer. Zenodo. https://zenodo.org/record/6826877"
  },
  {
    "objectID": "sessions/session-02.html#section",
    "href": "sessions/session-02.html#section",
    "title": "Session 2",
    "section": "",
    "text": "Back to course schedule ⏎"
  },
  {
    "objectID": "sessions/session-05.html",
    "href": "sessions/session-05.html",
    "title": "Session 5",
    "section": "",
    "text": "## Literature\n\n\n\n\n\n\nThe following sections list the mandatory articles for the each presenting group. Additional optional articles are provided under Further reading.\n\n\n\n\n📚 Thema 3: (Wirkungs-)Effekte der -Nutzung/Interaktion\n\nXi, D., Xu, W., Tang, L., & Han, B. (2024). The impact of streamer emotions on viewer gifting behavior: Evidence from entertainment live streaming. Internet Research, 34(3), 748–783. https://doi.org/10.1108/INTR-05-2022-0350\nBründl, S., Matt, C., Hess, T., & Engert, S. (2023). How synchronous participation affects the willingness to subscribe to social live streaming services: The role of co-interactive behavior on twitch. European Journal of Information Systems, 32(5), 800–817. https://doi.org/10.1080/0960085X.2022.2062468\nWolff, G. H., & Shen, C. (2022). Audience size, moderator activity, gender, and content diversity: Exploring user participation and financial commitment on Twitch.tv. New Media & Society, 146144482110699. https://doi.org/10.1177/14614448211069996\n\n\nFurther Readings\n\nHan, C., Seering, J., Kumar, D., Hancock, J. T., & Durumeric, Z. (2023). Hate Raids on Twitch: Echoes of the Past, New Modalities, and Implications for Platform Governance. Proceedings of the ACM on Human-Computer Interaction, 7(CSCW1), 1–28. https://doi.org/10.1145/3579609\nMeisner, C. (2023). Networked Responses to Networked Harassment? Creators’ Coordinated Management of “Hate Raids” on Twitch. Social Media + Society, 9(2), 20563051231179696. https://doi.org/10.1177/20563051231179696\nTomlinson, C. (2024). Community Grievances, personal responsibility, and DIY protection: Frustrations and solution-seeking among marginalized Twitch streamers. Convergence: The International Journal of Research into New Media Technologies, 30(1), 358–374. https://doi.org/10.1177/13548565231184060\nHou, F., Guan, Z., Li, B., & Chong, A. Y. L. (2020). Factors influencing people’s continuous watching intention and consumption intention in live streaming. Internet Research, 30(1), 141–163. https://doi.org/10.1108/INTR-04-2018-0177\nChinchilla, P., & Kim, J. (2024). “Let’s Chill and Chat”: Exploring the Effects of Streamers’ Self-Disclosure on Parasocial Interaction via Social Presence. International Journal of HumanComputer Interaction, 1–11. https://doi.org/10.1080/10447318.2024.2390263\n\n\n\n\n📚 Thema 4: (Wirkungs-)Effekte von TV-Wahldebatten\n\nLe Pennec, C., & Pons, V. (2023). How do campaigns shape vote choice? Multicountry evidence from 62 elections and 56 TV debates*. The Quarterly Journal of Economics, 138(2), 703–767. https://doi.org/10.1093/qje/qjad002\nWaldvogel, T., König, P., Wagschal, U., Becker, B., & Weishaupt, S. (2022). It’s the emotion, stupid! Emotional responses to televised debates and their impact on voting intention. Open Political Science, 5(1), 13–28. https://doi.org/10.1515/openps-2022-0146\nJennings, F. J., Bramlett, J. C., McKinney, M. S., & Hardy, M. M. (2020). Tweeting Along Partisan Lines: Identity-Motivated Elaboration and Presidential Debates. Social Media + Society, 6(4), 2056305120965518. https://doi.org/10.1177/2056305120965518\nWarner, B. R., Park, J., Kim, G.-E., McKinney, M. S., & Paul, Wm. B. (2024). Do Presidential Primary Debates Increase Political Polarization? American Behavioral Scientist, 68(1), 80–96. https://doi.org/10.1177/00027642211026613\n\n\nFurther Readings\n\nBenoit, W. L., Hansen, G. J., & Verser, R. M. (2003). A meta-analysis of the effects of viewing u.s. Presidential debates. Communication Monographs, 70(4), 335–350. https://doi.org/10.1080/0363775032000179133\nMcKinney, M. S., & Warner, B. R. (2013). Do Presidential Debates Matter? Examining a Decade of Campaign Debate Effects. Argumentation and Advocacy, 49(4), 238–258. https://doi.org/10.1080/00028533.2013.11821800\n\n\n\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/session-10.html",
    "href": "sessions/session-10.html",
    "title": "Session 10",
    "section": "",
    "text": "🖥️ Session 10"
  },
  {
    "objectID": "sessions/session-10.html#participate",
    "href": "sessions/session-10.html#participate",
    "title": "Session 10",
    "section": "",
    "text": "🖥️ Session 10"
  },
  {
    "objectID": "sessions/session-10.html#useful-packages",
    "href": "sessions/session-10.html#useful-packages",
    "title": "Session 10",
    "section": "Useful packages",
    "text": "Useful packages\n\nquanteda 🌐 | \nBenoit, K., Watanabe, K., Wang, H., Nulty, P., Obeng, A., Müller, S., & Matsuo, A. (2018). Quanteda: An r package for the quantitative analysis of textual data. Journal of Open Source Software, 3(30), 774. https://doi.org/10.21105/joss.00774\nquanteda.sentiment \ntidymodels 🌐 | \nKuhn, M., & Wickham, H. (2020). Tidymodels: A collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org\nellmer 🌐 | \nWickham, H., & Cheng, J. (2024). Ellmer: Chat with large language models. https://ellmer.tidyverse.org\nrollama 🌐 | \nGruber, J. B., & Weber, M. (2024). Rollama: Communicate with ’ollama’. https://jbgruber.github.io/rollama/\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/session-07.html",
    "href": "sessions/session-07.html",
    "title": "Session 7",
    "section": "",
    "text": "🖥️ Session 07"
  },
  {
    "objectID": "sessions/session-07.html#participate",
    "href": "sessions/session-07.html#participate",
    "title": "Session 7",
    "section": "",
    "text": "🖥️ Session 07"
  },
  {
    "objectID": "sessions/session-07.html#suggested-readings",
    "href": "sessions/session-07.html#suggested-readings",
    "title": "Session 7",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nAtteveldt, W. van, Trilling, D., & Arcíla, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O’Reilly."
  },
  {
    "objectID": "sessions/session-07.html#useful-packages",
    "href": "sessions/session-07.html#useful-packages",
    "title": "Session 7",
    "section": "Useful packages",
    "text": "Useful packages\n\nchat_downloader 🌐 | \n\ntwitch-dl 🌐 | )"
  },
  {
    "objectID": "sessions/session-07.html#useful-resources",
    "href": "sessions/session-07.html#useful-resources",
    "title": "Session 7",
    "section": "Useful resources",
    "text": "Useful resources\n\n🌐 twitchtracker.com\n🌐 twitchemotes.com\n🌐 twitchmetrics.net/emotes\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/session-08.html",
    "href": "sessions/session-08.html",
    "title": "Session 8",
    "section": "",
    "text": "🖥️ Session 08"
  },
  {
    "objectID": "sessions/session-08.html#participate",
    "href": "sessions/session-08.html#participate",
    "title": "Session 8",
    "section": "",
    "text": "🖥️ Session 08"
  },
  {
    "objectID": "sessions/session-08.html#suggested-readings",
    "href": "sessions/session-08.html#suggested-readings",
    "title": "Session 8",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nAtteveldt, W. van, Trilling, D., & Arcíla, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nGrimmer, J., Roberts, M. E., & Stewart, B. M. (2022). Text as data: A new framework for machine learning and the social sciences. Princeton University Press.\nJurafsky, D., & Martin, J. H. (2024). Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition with language models (3rd ed.). https://web.stanford.edu/~jurafsky/slp3/\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O’Reilly.\n\n\n… with focus on n-gram methods\n\nNicholls, T. (2019). Detecting Textual Reuse in News Stories, At Scale. International Journal of Communication, 13(0), 25. https://ijoc.org/index.php/ijoc/article/view/9904\nArendt, F., & Karadas, N. (2017). Content analysis of mediated associations: An automated text-analytic approach. Communication Methods and Measures, 11(2), 105–120. https://doi.org/10.1080/19312458.2016.1276894\n\n\n\n… with focus on topic modeling\n\nChen, Y., Peng, Z., Kim, S.-H., & Choi, C. W. (2023). What We Can Do and Cannot Do with Topic Modeling: A Systematic Review. Communication Methods and Measures, 17(2), 111–130. https://doi.org/10.1080/19312458.2023.2167965\nHase, V. (2023). Automated Content Analysis (F. Oehmer-Pedrazzi, S. H. Kessler, E. Humprecht, K. Sommer, & L. Castro, Eds.; pp. 23–36). Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-36179-2_3\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., Häussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93–118. https://doi.org/10.1080/19312458.2018.1430754"
  },
  {
    "objectID": "sessions/session-08.html#useful-packages",
    "href": "sessions/session-08.html#useful-packages",
    "title": "Session 8",
    "section": "Useful packages",
    "text": "Useful packages\n\nquanteda 🌐 | \nBenoit, K., Watanabe, K., Wang, H., Nulty, P., Obeng, A., Müller, S., & Matsuo, A. (2018). Quanteda: An r package for the quantitative analysis of textual data. Journal of Open Source Software, 3(30), 774. https://doi.org/10.21105/joss.00774\ntextreuse 🌐 | \nLi, Y., & Mullen, L. (2024). Textreuse: Detect text reuse and document similarity. https://docs.ropensci.org/textreuse (website) https://github.com/ropensci/textreuse\ntidytext 🌐 | \nSilge, J., & Robinson, D. (2016). Tidytext: Text mining and analysis using tidy data principles in r. The Journal of Open Source Software, 1(3), 37. https://doi.org/10.21105/joss.00037\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "exercises/exercise-09.html",
    "href": "exercises/exercise-09.html",
    "title": "Topic Modeling with stminsights",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "exercises/exercise-09.html#background",
    "href": "exercises/exercise-09.html#background",
    "title": "Topic Modeling with stminsights",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: Topic Modeling\n\n\n\nTranscripts & Chats of the Live-Streams from  hasanabi and  zackrawrr and | TheMajorityReport for the Presidential (Harris vs. Trump) and Vice-Presidential (Vance vs. Walz) Debates 2024\n\n\n\nThe best way to learn R is by trying. This document tries to display a version of the “normal” data processing procedure.\nUse tidytuesday data as an example to showcase the potential"
  },
  {
    "objectID": "exercises/exercise-09.html#preparation",
    "href": "exercises/exercise-09.html#preparation",
    "title": "Topic Modeling with stminsights",
    "section": "Preparation",
    "text": "Preparation\n\nPackages\nThe pacman::p_load() function from the pacman package is used to load the packages, which has several advantages over the conventional method with library():\n\nConcise syntax\nAutomatic installation (if the package is not already installed)\nLoading multiple packages at once\nAutomatic search for dependencies\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, \n    magrittr, janitor,\n    ggpubr, \n    gt, gtExtras,\n    countdown, \n    quanteda, # quanteda text processing\n    quanteda.textplots, quanteda.textstats, quanteda.textmodels,\n    tidytext, \n    udpipe, spacyr, # POS tagging\n    stm, stminsights,\n    easystats, tidyverse\n)\n\n\n\nImport und Vorverarbeitung der Daten\n\n\n\n\n\n\nInformation\n\n\n\nFor information about how the data, especially the topic modeling results, were prepared, processed and estimated, please see the tutorial.\n\n\n\n# Import base data\nchats &lt;- qs::qread(here(\"local_data/chat-debates_full.qs\"))$correct\n\n# Import corpora\nchats_spacyr &lt;- qs::qread(here(\"local_data/chat-corpus_spacyr.qs\"))\nstm_search &lt;- qs::qread(here(\"local_data/stm-majority_report-search.qs\"))\nstm_results &lt;- qs::qread(here(\"local_data/stm-majority_report-results.qs\"))\n\n\nVorverarbeitung der Daten\n\nchats_valid &lt;- chats %&gt;% \n  mutate(\n    across(c(debate, platform), ~as.factor(.x))\n  ) \n\n\n\nVorverarbeitung des Korpus\n\n# spacyr-Korpus zu Tokens\nchat_spacyr_toks &lt;- chats_spacyr %&gt;% \n  as.tokens(\n    use_lemma = TRUE\n  ) %&gt;% \n  tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = FALSE,\n    remove_url = FALSE, \n    split_hyphens = FALSE,\n    split_tags = FALSE,\n  ) %&gt;% \n  tokens_remove(\n    pattern = stopwords(\"en\")\n  ) %&gt;% \n  tokens_ngrams(n = 1:3) \n\n\n\nAdd docvars\n\n# Get document names from the original data\noriginal_docnames &lt;- chats$message_id\n\n# Get document names from the tokens object\ntoken_docnames &lt;- docnames(chat_spacyr_toks)\n\n# Identify missing documents\nmissing_docs &lt;- setdiff(original_docnames, token_docnames)\n\n# Exclude \"empty\" messages\nchats_filtered &lt;- chats_valid %&gt;% \n  filter(!message_id %in% missing_docs)\n\n# Add docvars\ndocvars(chat_spacyr_toks) &lt;- chats_filtered\n\n\n\nFokus on The Majority Report\n\n# Subset tokens based on docvars\nmajority_report_chat_toks &lt;- tokens_subset(\n  chat_spacyr_toks, streamer == \"the_majority_report\")\n\n\n# Convert to DFM\nmajority_report_chat_dfm &lt;- majority_report_chat_toks %&gt;% \n  dfm()\n\n# Pruning\nmajority_report_chat_trim &lt;- majority_report_chat_dfm %&gt;% \n    dfm_trim(\n        min_docfreq = 50/nrow(chats),\n        max_docfreq = 0.99, \n        docfreq_type = \"prop\"\n   )\n\n# Convert for stm topic modeling\nmajority_report_chat_stm &lt;- majority_report_chat_trim %&gt;% \n   convert(to = \"stm\")\n\n\nempty_docs &lt;- Matrix::rowSums(\n  as(majority_report_chat_trim, \"Matrix\")) == 0 \nempty_docs_ids &lt;- majority_report_chat_trim@docvars$docname[empty_docs]\n\nchats_model &lt;- chats_filtered %&gt;% \n  filter(!(message_id %in% empty_docs_ids)) %&gt;% \n  filter(streamer == \"the_majority_report\")\n\n\n\nExport topic models\n\nK = 12\n\n# Get model\ntpm_k12 &lt;- stm_results %&gt;% \n   filter(k == 12) |&gt; \n   pull(mdl) %&gt;% .[[1]]\n\n# Estimate effects\neffects_k12 &lt;- estimateEffect(\n  formula =~ platform + debate + message_during_debate,\n  stmobj = tpm_k12, \n  metadata = chats_model)\n\n\n\nK = 14\n\n# Get model\ntpm_k14 &lt;- stm_results %&gt;% \n   filter(k == 14) |&gt; \n   pull(mdl) %&gt;% .[[1]]\n\n# Estimate effects\neffects_k14 &lt;- estimateEffect(\n  formula =~ platform + debate + message_during_debate,\n  stmobj = tpm_k14, \n  metadata = chats_model)\n\n\n\nK = 18\n\n# Get model\ntpm_k18 &lt;- stm_results %&gt;% \n   filter(k == 18) |&gt; \n   pull(mdl) %&gt;% .[[1]]\n\n# Estimate effects\neffects_k18 &lt;- estimateEffect(\n  formula =~ platform + debate + message_during_debate,\n  stmobj = tpm_k18, \n  metadata = chats_model)"
  },
  {
    "objectID": "exercises/exercise-09.html#praktische-übung",
    "href": "exercises/exercise-09.html#praktische-übung",
    "title": "Topic Modeling with stminsights",
    "section": "🛠️ Praktische Übung",
    "text": "🛠️ Praktische Übung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation ausgeführt haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf  des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in das Tutorial (.qmd oder .html). Beim Tutorial handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Tutorial also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\nPrepare and save workspace\n\n# Set names required by stminsights\ndata &lt;- majority_report_chat_trim\nout &lt;- majority_report_chat_stm\n\n# Clean workspace\nrm(list = setdiff(\n  ls(),\n  c(\"data\", \"out\",\n    \"tpm_k12\", \"tpm_k14\", \"tpm_k18\", \n    \"effects_k12\", \"effects_k14\", \"effects_k18\")))\n\n# Save workspace\nsave.image(here(\"stm_session_09.RData\"))\n\n\n\nStart stminsights\n\nlibrary(stminsights)\nrun_stminsights()"
  },
  {
    "objectID": "exercises/exercise-03_collab.html",
    "href": "exercises/exercise-03_collab.html",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "",
    "text": "Link to slides\n Download source file"
  },
  {
    "objectID": "exercises/exercise-03_collab.html#background",
    "href": "exercises/exercise-03_collab.html#background",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: Hollywood Age Gaps\n\n\n\n\nAn informational site showing the age gap between movie love interests.\n\nThe data follows certain rules:\n\nThe two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)\nThe youngest of the two actors is at least 17 years old\nNot animated characters\n\n\n\n\nThe best way to learn R is by trying. This document tries to display a version of the “normal” data processing procedure.\nUse tidytuesday data as an example to showcase the potential"
  },
  {
    "objectID": "exercises/exercise-03_collab.html#preparation",
    "href": "exercises/exercise-03_collab.html#preparation",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "Preparation",
    "text": "Preparation\n\nPackages\nThe pacman::p_load() package is used to load the packages, which has several advantages over the conventional method with library():\n\nConcise syntax\nAutomatic installation (if the package is not already installed)\nLoading multiple packages at once\nAutomatic search for dependencies\n\n\npacman::p_load(\n  here, \n  magrittr, \n  tidyverse,\n  janitor,\n  easystats,\n  sjmisc,\n  ggpubr)\n\n\n\nImport und Vorverarbeitung der Daten\n\nVariablennamen und -beschreibungen\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nmovie_name\nName of the film\n\n\nrelease_year\nRelease year\n\n\ndirector\nDirector of the film\n\n\nage_difference\nAge difference between the characters in whole years\n\n\ncouple_number\nAn identifier for the couple in case multiple couples are listed for this film\n\n\nactor_1_name\nThe name of the older actor in this couple\n\n\nactor_2_name\nThe name of the younger actor in this couple\n\n\nactor_1_birthdate\nThe birthdate of the older member of the couple\n\n\nactor_2_birthdate\nThe birthdate of the younger member of the couple\n\n\nactor_1_age\nThe age of the older actor when the film was released\n\n\nactor_2_age\nThe age of the younger actor when the film was released\n\n\n\n\n# Import data from URL\nage_gaps &lt;- read_csv(\"http://hollywoodagegap.com/movies.csv\") %&gt;% \n  janitor::clean_names()\n\n\n# Correct data\nage_gaps_correct &lt;- age_gaps %&gt;% \n  mutate(\n    across(ends_with(\"_birthdate\"), ~as.Date(.)) # set dates to dates\n  )"
  },
  {
    "objectID": "exercises/exercise-03_collab.html#praktische-übung",
    "href": "exercises/exercise-03_collab.html#praktische-übung",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "🛠️ Praktische Übung",
    "text": "🛠️ Praktische Übung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation ausgeführt haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf  des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\n🔎 Welche Rolle spielt das Geschlecht?\n\n\n\n\n\n\nSpielt das Geschlecht eine Rolle?\n\n\n\n\nDer folgende Abschitt befasst sich nun ergänzend mit der Frage, welche Rolle das Geschlecht mit Blick auf die “Gültigkeit” der vorherigen Ergebnisse spielt\nDazu sind jedoch weitere Explorations- und Überarbeitungsschritte notwendig\n\n\n\n\n\n📋 Exercise 1: Übeprüfung der _gender-Variablen\n\n\n\n\n\n\nArbeitsauftrag 1.1\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_correct die Variablen actor_1_gender und actor_2_gender an.\n\n\n\n# INSERT CODE HERE\n\n\n\n\n\n\n\nArbeitsauftrag 1.2\n\n\n\nNutzen Sie die Funktion sjmisc::flat_talbe() und den Datensatz age_gaps_correct um eine Kreuztabelle der Variablen actor_1_gender und actor_2_gender zu erstellen.\n\n\n\n# INSERT CODE HERE\n\n\n\n🔎 Sind die Daten “konsistent”?\n\nÜberprüfung der Sortierung\n\nage_gaps_correct %&gt;% \n  summarise(\n      p1_older = mean(actor_1_age &gt; actor_2_age), # older person first?\n      p1_male  = mean(actor_1_gender == \"man\"),  # male person first? \n      p_1_first_alpha = mean(actor_1_name &lt; actor_2_name) # alphabetical order?\n  )\n\n# A tibble: 1 × 3\n  p1_older p1_male p_1_first_alpha\n     &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;\n1    0.813   0.987           0.496\n\n\n\n\n\nÜberprüfung der Anzahl pro Paare pro Film\n\n# Create data\ncouples &lt;- age_gaps_correct %&gt;% \n  group_by(movie_name) %&gt;% \n  summarise(n = n()) \n\n# Distribution\ncouples %&gt;% frq(n)\n\nn &lt;integer&gt; \n# total N=863 valid N=863 mean=1.39 sd=0.75\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    1 | 628 | 72.77 |   72.77 |  72.77\n    2 | 162 | 18.77 |   18.77 |  91.54\n    3 |  54 |  6.26 |    6.26 |  97.80\n    4 |  14 |  1.62 |    1.62 |  99.42\n    5 |   3 |  0.35 |    0.35 |  99.77\n    6 |   1 |  0.12 |    0.12 |  99.88\n    7 |   1 |  0.12 |    0.12 | 100.00\n &lt;NA&gt; |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n# Movies with a loot of couples \ncouples %&gt;% \n  filter(n &gt; 3) %&gt;% \n  arrange(desc(n))\n\n# A tibble: 19 × 2\n   movie_name                      n\n   &lt;chr&gt;                       &lt;int&gt;\n 1 Love Actually                   7\n 2 The Family Stone                6\n 3 A View to a Kill                5\n 4 He's Just Not That Into You     5\n 5 Mona Lisa Smile                 5\n 6 A Star Is Born                  4\n 7 American Pie                    4\n 8 Boogie Nights                   4\n 9 Book Club                       4\n10 Closer                          4\n11 Pushing Tin                     4\n12 Sex and the City                4\n13 Soul Food                       4\n14 Tag                             4\n15 The Favourite                   4\n16 The Girl on the Train           4\n17 The Other Woman                 4\n18 Tomorrow Never Dies             4\n19 Twilight                        4\n\n\n\n\nKorrekturen\n\nage_gaps_consistent &lt;- age_gaps_correct %&gt;% \n  # If multiple couples, assign couple number by movie\n  mutate(\n      couple_number = row_number(),\n      .by = \"movie_name\"\n  ) %&gt;% \n  # Change data structure (one line per actor in a coulpe of a movie)\n  pivot_longer(\n    cols = starts_with(c(\"actor_1_\", \"actor_2_\")),\n    names_to = c(NA, NA, \".value\"),\n    names_sep = \"_\"\n  ) %&gt;% \n  # Put older actor first\n  arrange(desc(age_difference), movie_name, birthdate) %&gt;% \n    mutate(\n    position = row_number(),\n    .by = c(\"movie_name\", \"couple_number\")\n  ) %&gt;% \n  pivot_wider(\n    names_from = \"position\",\n    names_glue = \"actor_{position}_{.value}\",\n    values_from = c(\"name\", \"gender\", \"birthdate\", \"age\")\n  ) %&gt;% \n  mutate(\n    couple_structure = case_when(\n      actor_1_gender == \"woman\" & actor_2_gender == \"woman\" ~ 1,\n      actor_1_gender == \"man\" & actor_2_gender == \"man\" ~ 2,\n      actor_1_gender != \"man\" ~ 3, \n      actor_1_gender == \"man\" ~ 4,\n    ),\n    older_male_hetero  = sjmisc::rec(\n      couple_structure, \n      rec=\"3=0; 4=1; ELSE=NA\", \n      to.factor = TRUE\n    )\n  )\n\n\n\n🔎 Die zweite Datenexploration\n\n\n📋 Exercise 2: Alterskombinationen im Überblick\n\n\n\n\n\n\nArbeitauftrag 2\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_consistent die Variablen couple_structure und older_male_hetero an.\n\n\n\n# INSERT CODE HERE\n\n\n\n📋 Exercise 3: Wie sind die Altersunterschiede unterteilt, unter Berücksichtiung des Geschlechts?\n\n\n\n\n\n\nArbeitsauftrag 3.1 (graphische Überprüfung)\n\n\n\n\nErstellen Sie, auf Basis des Datensatzes age_gaps_consistent, einen ggplot.\nNutzen Sie im Argument aes() die Variable age_difference als x-Variable und older_male_hetero für das Argument fill.\nNutzen Sie geom_bar zur Erzeugung des Plots.\nOptional: Verwenden Sie theme_pubr\n\n\n\n\n# INSERT CODE HERE\n\n\n\n\n\n\n\nArbeitsauftrag 3.2 (Überprüfung durch Modellierung)\n\n\n\n\nErstellen Sie ein lineares Modell (lm), das die Variable age_difference als abhängige Variable und die Variablen release_year und older_male_hetero als unabhängige Variablen verwendet. Nutzen Sie dazu den Datensatz age_gaps_consistent.\nGeben Sie die Parameter des Modells mit der Funktion parameters::parameters() aus.\nBewerten Sie die Modellleistung mit der Funktion performance::model_performance().\nErstellen Sie einen Bericht über das Modell mit der Funktion report::report().\n\n\n\n\n# INSERT CODE HERE"
  },
  {
    "objectID": "exercises/exercise-07_solution.html",
    "href": "exercises/exercise-07_solution.html",
    "title": "Twitch Chat Analysis",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "exercises/exercise-07_solution.html#background",
    "href": "exercises/exercise-07_solution.html#background",
    "title": "Twitch Chat Analysis",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: Twitch Chat & Transcripts\n\n\n\nTranscripts & Chats of the Live-Streams from  hasanabi and  zackrawrr and | TheMajorityReport for the Presidential (Harris vs. Trump) and Vice-Presidential (Vance vs. Walz) Debates 2024\n\n\n\nThe best way to learn R is by trying. This document tries to display a version of the “normal” data processing procedure.\nUse tidytuesday data as an example to showcase the potential"
  },
  {
    "objectID": "exercises/exercise-07_solution.html#preparation",
    "href": "exercises/exercise-07_solution.html#preparation",
    "title": "Twitch Chat Analysis",
    "section": "Preparation",
    "text": "Preparation\n\nPackages\nThe pacman::p_load() function from the pacman package is used to load the packages, which has several advantages over the conventional method with library():\n\nConcise syntax\nAutomatic installation (if the package is not already installed)\nLoading multiple packages at once\nAutomatic search for dependencies\n\n\npacman::p_load(\n    here, taylor,\n    magrittr, janitor,\n    ggpubr, \n    gt, gtExtras,\n    countdown, \n    quanteda, # quanteda text processing\n    quanteda.textplots, \n    easystats, tidyverse\n)\n\n\n\nImport und Vorverarbeitung der Daten\n\nchats &lt;- qs::qread(here(\"local_data/chat-debates_full.qs\"))$correct\ntranscripts &lt;- qs::qread(here(\"local_data/transcripts-debates_full.qs\"))$correct"
  },
  {
    "objectID": "exercises/exercise-07_solution.html#praktische-übung",
    "href": "exercises/exercise-07_solution.html#praktische-übung",
    "title": "Twitch Chat Analysis",
    "section": "🛠️ Praktische Übung",
    "text": "🛠️ Praktische Übung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation ausgeführt haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf  des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in das Tutorial (.qmd oder .html). Beim Tutorial handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Tutorial also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden."
  },
  {
    "objectID": "exercises/exercise-07_solution.html#kennenlernen-des-chat-datensatzes",
    "href": "exercises/exercise-07_solution.html#kennenlernen-des-chat-datensatzes",
    "title": "Twitch Chat Analysis",
    "section": "🔎 Kennenlernen des Chat-Datensatzes",
    "text": "🔎 Kennenlernen des Chat-Datensatzes\n\n📋 Exercise 1: Create corpus\n\nCreate new dataset corp_chats\n\nBased on the dataset chats, create a corpus object with the quanteda package.\nUse the corpus() function with the docid_field argument set to “message_id” and the text_field argument set to “message_content”.\nCheck if the transformation was successful by using the summary() function.\n\n\n\n# Create new dataset clean_tidy_tweets\ncorp_chats &lt;- chats %&gt;% \n  quanteda::corpus(\n    docid_field = \"message_id\", \n    text_field = \"message_content\"\n  )\n\n# Check\nsummary(corp_chats)\n\n\n\n📋 Exercise 2: Tokenization & DFM conversion\n\nCreate new datasets toks_chats & dfm_chats\n\n\nBased on the dataset corp_chats, create tokens using the tokens() function from the quanteda package.\nConvert the tokens to a document-feature matrix (DFM) using the dfm() function from the quanteda package.\nCheck if the transformations were successful (e.g. by using the print() function).\n\n\n# Create tokens\ntoks_chats &lt;- corp_chats %&gt;%\n    quanteda::tokens() \n \n# Create DFM\ndfm_chats &lt;- toks_chats %&gt;%\n    quanteda::dfm()\n\n# Check\ntoks_chats %&gt;% print()\ndfm_chats %&gt;% print()\n\n\n\n📋 Exercise 3: Analyse DFM\n\nBased on dfm_chats\n\nUse the textstat_frequency() function from the quanteda package to get the top 50 tokens.\nDisplay the results.\n\nBased on the results, what preprocessing steps could be useful?\n\n\n# Top 50 Tokens\ndfm_chats %&gt;% \n  quanteda.textstats::textstat_frequency(n = 50) \n\n\n\n📋 Exercise 4: Preprocessing\n\nCreate a new dataset dfm_chats_preprocessed\n\nBased on corp_chats, preprocess the data according to the steps you think are necessary (e.g. removing punctuation, symbols, numbers, URLs, and stopwords).\nDepending on the steps you choose, you might need to use the tokens_remove() function from the quanteda package.\nCreate a new DFM object dfm_chats_preprocessed.\nUse the textstat_frequency() function from the quanteda package on the newly created dataset to get the top 50 tokens and compare the result with the results of Exercise 3.\n\n\n\n# Preprocessing\ndfm_chats_preprocessed &lt;- corp_chats %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = TRUE,\n    remove_url = TRUE, \n    split_hyphens = FALSE,\n    split_tags = FALSE\n  ) %&gt;% \n  quanteda::tokens_remove(\n    pattern = quanteda::stopwords(\"en\")\n  ) %&gt;% \n  quanteda::dfm(\n    tolower = TRUE\n  )\n\n# Check\ndfm_chats_preprocessed %&gt;% \n  quanteda.textstats::textstat_frequency(n = 50)"
  },
  {
    "objectID": "exercises/exercise-03.html",
    "href": "exercises/exercise-03.html",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "exercises/exercise-03.html#background",
    "href": "exercises/exercise-03.html#background",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: Hollywood Age Gaps\n\n\n\n\nAn informational site showing the age gap between movie love interests.\n\nThe data follows certain rules:\n\nThe two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)\nThe youngest of the two actors is at least 17 years old\nNot animated characters\n\n\n\n\nThe best way to learn R is by trying. This document tries to display a version of the “normal” data processing procedure.\nUse tidytuesday data as an example to showcase the potential"
  },
  {
    "objectID": "exercises/exercise-03.html#preparation",
    "href": "exercises/exercise-03.html#preparation",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "Preparation",
    "text": "Preparation\n\nPackages\nThe pacman::p_load() package is used to load the packages, which has several advantages over the conventional method with library():\n\nConcise syntax\nAutomatic installation (if the package is not already installed)\nLoading multiple packages at once\nAutomatic search for dependencies\n\n\npacman::p_load(\n  here, \n  magrittr, \n  tidyverse,\n  janitor,\n  easystats,\n  sjmisc,\n  ggpubr)\n\n\n\nImport und Vorverarbeitung der Daten\n\nVariablennamen und -beschreibungen\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nmovie_name\nName of the film\n\n\nrelease_year\nRelease year\n\n\ndirector\nDirector of the film\n\n\nage_difference\nAge difference between the characters in whole years\n\n\ncouple_number\nAn identifier for the couple in case multiple couples are listed for this film\n\n\nactor_1_name\nThe name of the older actor in this couple\n\n\nactor_2_name\nThe name of the younger actor in this couple\n\n\nactor_1_birthdate\nThe birthdate of the older member of the couple\n\n\nactor_2_birthdate\nThe birthdate of the younger member of the couple\n\n\nactor_1_age\nThe age of the older actor when the film was released\n\n\nactor_2_age\nThe age of the younger actor when the film was released\n\n\n\n\n# Import data from URL\nage_gaps &lt;- read_csv(\"http://hollywoodagegap.com/movies.csv\") %&gt;% \n  janitor::clean_names()\n\n\n# Correct data\nage_gaps_correct &lt;- age_gaps %&gt;% \n  mutate(\n    across(ends_with(\"_birthdate\"), ~as.Date(.)) # set dates to dates\n  )"
  },
  {
    "objectID": "exercises/exercise-03.html#praktische-übung",
    "href": "exercises/exercise-03.html#praktische-übung",
    "title": "Exercise: Hollywood Age Gaps",
    "section": "🛠️ Praktische Übung",
    "text": "🛠️ Praktische Übung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation ausgeführt haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf  des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in das Tutorial (.qmd oder .html). Beim Tutorial handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Tutorial also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\n🔎 Welche Rolle spielt das Geschlecht?\n\n\n\n\n\n\nSpielt das Geschlecht eine Rolle?\n\n\n\n\nDer folgende Abschitt befasst sich nun ergänzend mit der Frage, welche Rolle das Geschlecht mit Blick auf die “Gültigkeit” der vorherigen Ergebnisse spielt\nDazu sind jedoch weitere Explorations- und Überarbeitungsschritte notwendig\n\n\n\n\n\n📋 Exercise 1: Übeprüfung der _gender-Variablen\n\n\n\n\n\n\nArbeitsauftrag 1.1\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_correct die Variablen actor_1_gender und actor_2_gender an.\n\n\n\n# INSERT CODE HERE\n\n\n\n\n\n\n\nArbeitsauftrag 1.2\n\n\n\nNutzen Sie die Funktion sjmisc::flat_talbe() und den Datensatz age_gaps_correct um eine Kreuztabelle der Variablen actor_1_gender und actor_2_gender zu erstellen.\n\n\n\n# INSERT CODE HERE\n\n\n\n🔎 Sind die Daten “konsistent”?\n\nÜberprüfung der Sortierung\n\nage_gaps_correct %&gt;% \n  summarise(\n      p1_older = mean(actor_1_age &gt; actor_2_age), # older person first?\n      p1_male  = mean(actor_1_gender == \"man\"),  # male person first? \n      p_1_first_alpha = mean(actor_1_name &lt; actor_2_name) # alphabetical order?\n  )\n\n# A tibble: 1 × 3\n  p1_older p1_male p_1_first_alpha\n     &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;\n1    0.813   0.987           0.495\n\n\n\n\n\nÜberprüfung der Anzahl pro Paare pro Film\n\n# Create data\ncouples &lt;- age_gaps_correct %&gt;% \n  group_by(movie_name) %&gt;% \n  summarise(n = n()) \n\n# Distribution\ncouples %&gt;% frq(n)\n\nn &lt;integer&gt; \n# total N=864 valid N=864 mean=1.39 sd=0.75\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    1 | 629 | 72.80 |   72.80 |  72.80\n    2 | 162 | 18.75 |   18.75 |  91.55\n    3 |  54 |  6.25 |    6.25 |  97.80\n    4 |  14 |  1.62 |    1.62 |  99.42\n    5 |   3 |  0.35 |    0.35 |  99.77\n    6 |   1 |  0.12 |    0.12 |  99.88\n    7 |   1 |  0.12 |    0.12 | 100.00\n &lt;NA&gt; |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n# Movies with a loot of couples \ncouples %&gt;% \n  filter(n &gt; 3) %&gt;% \n  arrange(desc(n))\n\n# A tibble: 19 × 2\n   movie_name                      n\n   &lt;chr&gt;                       &lt;int&gt;\n 1 Love Actually                   7\n 2 The Family Stone                6\n 3 A View to a Kill                5\n 4 He's Just Not That Into You     5\n 5 Mona Lisa Smile                 5\n 6 A Star Is Born                  4\n 7 American Pie                    4\n 8 Boogie Nights                   4\n 9 Book Club                       4\n10 Closer                          4\n11 Pushing Tin                     4\n12 Sex and the City                4\n13 Soul Food                       4\n14 Tag                             4\n15 The Favourite                   4\n16 The Girl on the Train           4\n17 The Other Woman                 4\n18 Tomorrow Never Dies             4\n19 Twilight                        4\n\n\n\n\nKorrekturen\n\nage_gaps_consistent &lt;- age_gaps_correct %&gt;% \n  # If multiple couples, assign couple number by movie\n  mutate(\n      couple_number = row_number(),\n      .by = \"movie_name\"\n  ) %&gt;% \n  # Change data structure (one line per actor in a coulpe of a movie)\n  pivot_longer(\n    cols = starts_with(c(\"actor_1_\", \"actor_2_\")),\n    names_to = c(NA, NA, \".value\"),\n    names_sep = \"_\"\n  ) %&gt;% \n  # Put older actor first\n  arrange(desc(age_difference), movie_name, birthdate) %&gt;% \n    mutate(\n    position = row_number(),\n    .by = c(\"movie_name\", \"couple_number\")\n  ) %&gt;% \n  pivot_wider(\n    names_from = \"position\",\n    names_glue = \"actor_{position}_{.value}\",\n    values_from = c(\"name\", \"gender\", \"birthdate\", \"age\")\n  ) %&gt;% \n  mutate(\n    couple_structure = case_when(\n      actor_1_gender == \"woman\" & actor_2_gender == \"woman\" ~ 1,\n      actor_1_gender == \"man\" & actor_2_gender == \"man\" ~ 2,\n      actor_1_gender != \"man\" ~ 3, \n      actor_1_gender == \"man\" ~ 4,\n    ),\n    older_male_hetero  = sjmisc::rec(\n      couple_structure, \n      rec=\"3=0; 4=1; ELSE=NA\", \n      to.factor = TRUE\n    )\n  )\n\n\n\n🔎 Die zweite Datenexploration\n\n\n📋 Exercise 2: Alterskombinationen im Überblick\n\n\n\n\n\n\nArbeitauftrag 2\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_consistent die Variablen couple_structure und older_male_hetero an.\n\n\n\n# INSERT CODE HERE\n\n\n\n📋 Exercise 3: Wie sind die Altersunterschiede unterteilt, unter Berücksichtiung des Geschlechts?\n\n\n\n\n\n\nArbeitsauftrag 3.1 (graphische Überprüfung)\n\n\n\n\nErstellen Sie, auf Basis des Datensatzes age_gaps_consistent, einen ggplot.\nNutzen Sie im Argument aes() die Variable age_difference als x-Variable und older_male_hetero für das Argument fill.\nNutzen Sie geom_bar zur Erzeugung des Plots.\nOptional: Verwenden Sie theme_pubr\n\n\n\n\n# INSERT CODE HERE\n\n\n\n\n\n\n\nArbeitsauftrag 3.2 (Überprüfung durch Modellierung)\n\n\n\n\nErstellen Sie ein lineares Modell (lm), das die Variable age_difference als abhängige Variable und die Variablen release_year und older_male_hetero als unabhängige Variablen verwendet. Nutzen Sie dazu den Datensatz age_gaps_consistent.\nGeben Sie die Parameter des Modells mit der Funktion parameters::parameters() aus.\nBewerten Sie die Modellleistung mit der Funktion performance::model_performance().\nErstellen Sie einen Bericht über das Modell mit der Funktion report::report().\n\n\n\n\n# INSERT CODE HERE"
  },
  {
    "objectID": "tutorials/tutorial-10.html",
    "href": "tutorials/tutorial-10.html",
    "title": "🔨 Sentiment Analysis with R",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "tutorials/tutorial-10.html#background",
    "href": "tutorials/tutorial-10.html#background",
    "title": "🔨 Sentiment Analysis with R",
    "section": "Background",
    "text": "Background"
  },
  {
    "objectID": "tutorials/tutorial-10.html#preparation",
    "href": "tutorials/tutorial-10.html#preparation",
    "title": "🔨 Sentiment Analysis with R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, \n    magrittr, janitor,\n    ggpubr, ggdist, ggsci,\n    gt, gtExtras,\n    countdown, \n    quanteda, # quanteda text processing\n    quanteda.textplots, quanteda.textstats, \n    quanteda.textmodels, quanteda.sentiment, \n    ellmer, rollama,\n    tidymodels,\n    easystats, tidyverse\n)\n\n\n# Import base data\nchats &lt;- qs::qread(here(\"local_data/chats.qs\"))\ncorp_chats &lt;- chats %&gt;% \n    quanteda::corpus(docid_field = \"message_id\", text_field = \"message_content\")"
  },
  {
    "objectID": "tutorials/tutorial-10.html#codechunks-aus-der-sitzung",
    "href": "tutorials/tutorial-10.html#codechunks-aus-der-sitzung",
    "title": "🔨 Sentiment Analysis with R",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nPraktische Anwedung von quanteda.sentiment\n\nchats_polarity &lt;- corp_chats %&gt;% \n  textstat_polarity(\n    dictionary = data_dictionary_LSD2015) %&gt;% \n  rename(polarity = sentiment)\n\n\nchats_polarity %&gt;% \n    head(n = 10) \n\n                                 doc_id  polarity\n1  dc03b89a-722d-4eaa-a895-736533a68aca  0.000000\n2  6be50e12-2fd5-436f-b253-b2358b618380  0.000000\n3  f5e41904-7f01-4f03-ad6c-2c0f07d70ed0  1.098612\n4  92dc6519-eb54-4c18-abef-27201314b22f -1.098612\n5  92055088-7067-48c0-aa11-9c6103bdf4c4  0.000000\n6  03ad4706-aa67-4ddc-a1e4-6f8ca981778e  0.000000\n7  00c5dd9c-41b8-4430-8b2e-be67c5e363ac  0.000000\n8  923c7eac-d92e-4cac-876a-07d4fa45cb57  0.000000\n9  6bdfb03d-fdbd-48b6-9b81-2fc56785fd67 -1.098612\n10 7f25fc9f-b000-41fa-91b8-60672cd3e608  0.000000\n\n\n\nchats_valence &lt;- corp_chats %&gt;% \n  textstat_valence(\n    dictionary = data_dictionary_AFINN) %&gt;% \n  rename(valence = sentiment)\n\n\nchats_valence %&gt;% \n    head(n = 10)\n\n                                 doc_id valence\n1  dc03b89a-722d-4eaa-a895-736533a68aca       0\n2  6be50e12-2fd5-436f-b253-b2358b618380       0\n3  f5e41904-7f01-4f03-ad6c-2c0f07d70ed0       0\n4  92dc6519-eb54-4c18-abef-27201314b22f      -5\n5  92055088-7067-48c0-aa11-9c6103bdf4c4       0\n6  03ad4706-aa67-4ddc-a1e4-6f8ca981778e       0\n7  00c5dd9c-41b8-4430-8b2e-be67c5e363ac       0\n8  923c7eac-d92e-4cac-876a-07d4fa45cb57       0\n9  6bdfb03d-fdbd-48b6-9b81-2fc56785fd67       0\n10 7f25fc9f-b000-41fa-91b8-60672cd3e608       0\n\n\n\n\nPraktische Anwedung von vader\n\nchats_vader &lt;- chats %&gt;% \n  mutate(\n    vader_output = map(message_content, ~vader::get_vader(.x)), \n    # Extract word-level scores\n    word_scores = map(vader_output, ~ .x[\n        names(.x) != \"compound\" &\n        names(.x) != \"pos\" & \n        names(.x) != \"neu\" & \n        names(.x) != \"neg\" & \n        names(.x) != \"but_count\"]),  \n    compound = map_dbl(vader_output, ~ as.numeric(.x[\"compound\"])),\n    pos = map_dbl(vader_output, ~ as.numeric(.x[\"pos\"])),\n    neu = map_dbl(vader_output, ~ as.numeric(.x[\"neu\"])),\n    neg = map_dbl(vader_output, ~ as.numeric(.x[\"neg\"])),\n    but_count = map_dbl(vader_output, ~ as.numeric(.x[\"but_count\"]))\n  )\n\n\nqs::qsave(chats_vader, file = here(\"local_data/chats-vader.qs\"))\n\n\nchats_vader %&gt;% \n    select(message_id, compound:but_count) %&gt;% \n    head(n = 20)\n\n# A tibble: 20 × 6\n   message_id                           compound   pos   neu   neg but_count\n   &lt;chr&gt;                                   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 dc03b89a-722d-4eaa-a895-736533a68aca    0         0 1     0             0\n 2 6be50e12-2fd5-436f-b253-b2358b618380    0         0 1     0             0\n 3 f5e41904-7f01-4f03-ad6c-2c0f07d70ed0    0         0 1     0             0\n 4 92dc6519-eb54-4c18-abef-27201314b22f   -0.586     0 0.513 0.487         0\n 5 92055088-7067-48c0-aa11-9c6103bdf4c4    0         0 1     0             0\n 6 03ad4706-aa67-4ddc-a1e4-6f8ca981778e    0         0 1     0             0\n 7 00c5dd9c-41b8-4430-8b2e-be67c5e363ac    0         0 1     0             0\n 8 923c7eac-d92e-4cac-876a-07d4fa45cb57    0         0 1     0             0\n 9 6bdfb03d-fdbd-48b6-9b81-2fc56785fd67    0         0 1     0             0\n10 7f25fc9f-b000-41fa-91b8-60672cd3e608    0         0 1     0             0\n11 a00e2ca8-2e76-4941-b360-b6b311701cba    0         0 1     0             0\n12 637e5e96-9f26-4a87-955e-74f2fb29685a    0         0 1     0             0\n13 4b0a6fbe-54d6-4d06-8d08-875112abcd92    0         0 1     0             0\n14 cf57874e-a239-4bce-a766-4bb7636847b7    0         0 1     0             0\n15 51b66d60-0f6b-43a6-a40c-cb6d51cde1a9    0         0 1     0             0\n16 08d7ae3c-1180-4e26-940e-de763fbe6f18    0         0 1     0             0\n17 72494412-fe24-44ad-9a02-de22e8e54724    0         0 1     0             0\n18 93a9da3e-63ab-4eea-bb51-73bff8dadf13    0         0 1     0             0\n19 3aa667c1-a8b1-4f18-94a6-920b8a9ee37b    0         0 1     0             0\n20 daebee85-4885-48f2-8086-9b9172285792   -0.586     0 0.513 0.487         0\n\n\n\n\nZusammenführung Dictionary-Sentiments\n\nchats_sentiment &lt;- chats %&gt;% \n    left_join(chats_polarity, by = join_by(\"message_id\" == \"doc_id\")) %&gt;%\n    left_join(chats_valence, by = join_by(\"message_id\" == \"doc_id\")) %&gt;% \n    left_join(chats_vader %&gt;% \n        select(message_id, vader_output, word_scores, compound, pos, neu, neg, but_count), \n        by = \"message_id\")\n\n\nchats_sentiment %&gt;% \n    select(message_id, polarity, valence, compound) %&gt;%\n    datawizard::describe_distribution()\n\nVariable |  Mean |   SD | IQR |         Range | Skewness | Kurtosis |      n | n_Missing\n----------------------------------------------------------------------------------------\npolarity | -0.06 | 0.65 |   0 | [-4.44, 3.61] |    -0.18 |     1.30 | 913375 |         0\nvalence  | -0.04 | 1.38 |   0 | [-5.00, 5.00] |    -0.21 |     2.54 | 913375 |         0\ncompound |  0.01 | 0.30 |   0 | [-1.00, 1.00] |    -0.12 |     1.11 | 913170 |       205\n\n\n\n\nVerschiedene VADER-Visualisierungen\n\nchats_vader_sample &lt;- chats_vader %&gt;%\n    filter(message_length &lt; 100) %&gt;%\n    slice_sample(n = 10) \n\n\nchats_vader_sample %&gt;%\n    ggplot(aes(x = message_content, y = compound, fill = compound &gt; 0)) +\n        geom_bar(stat = \"identity\", width = 0.7) +\n        scale_fill_manual(values = c(\"TRUE\" = \"blue\", \"FALSE\" = \"red\"), labels = c(\"Positive\", \"Negative\")) +\n        labs(\n            title = \"Overall Compound Sentiment for Each Sentence\",\n            x = \"Sentences\",\n            y = \"Compound Sentiment\",\n            fill = \"Sentiment\") +\n        coord_flip() +  # Flip for easier readability\n        theme_minimal() +\n        theme(\n            axis.text.x = element_text(angle = 45, hjust = 1))  # Label wrapping and adjusting angle\n\n\n\n\n\n\n\n\n\nchats_vader_sample %&gt;% \n    mutate(\n        pos_pct = pos * 100,\n        neu_pct = neu * 100,\n        neg_pct = neg * 100) %&gt;% \n  select(message_content, pos_pct, neu_pct, neg_pct) %&gt;% \n  pivot_longer(\n    cols = c(pos_pct, neu_pct, neg_pct),\n    names_to = \"sentiment\",\n    values_to = \"percentage\") %&gt;% \n  mutate(\n    sentiment = factor(\n        sentiment,\n        levels = c(\"pos_pct\", \"neu_pct\", \"neg_pct\"),\n        labels = c(\"Positive\", \"Neutral\", \"Negative\"))) %&gt;% \n  ggplot(aes(x = message_content, y = percentage, fill = sentiment)) +\n    geom_bar(stat = \"identity\", width = 0.7) +\n    scale_fill_manual(values = c(\"Positive\" = \"blue\", \"Neutral\" = \"gray\", \"Negative\" = \"red\")) +\n    labs(\n        title = \"Proportion of Positive, Neutral, and Negative Sentiment\",\n        x = \"Sentences\",\n        y = \"Percentage\",\n        fill = \"Sentiment\") +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nChat mit LLMs in R\n\nellmer\n\nellmer_chat_llama &lt;- ellmer::chat_ollama(\n    model = \"llama3.2\"\n)\n\nellmer_chat_llama$chat(\"Why is the sky blue?\")\n\nThe reason the sky appears blue is due to a phenomenon called Rayleigh \nscattering. This process occurs when sunlight interacts with tiny molecules of \ngases in the Earth's atmosphere, such as nitrogen and oxygen.\n\nHere's what happens:\n\n1. Sunlight enters the Earth's atmosphere and is made up of different \nwavelengths of light, which are like colors.\n2. The shorter wavelengths (blue and violet) of light are scattered more than \nthe longer wavelengths (red, orange, and yellow).\n3. This scattering occurs because the smaller molecules in the atmosphere are \nmore effective at reflecting the shorter wavelengths.\n4. As a result, when sunlight hits an observer's eye, it has to travel through \nmore of these tiny molecules, which scatter the blue light.\n5. Our eyes perceive this scattered blue light as the color of the sky.\n\nHowever, that's not the whole story. There are also other factors at play:\n\n* The Earth's atmosphere is not uniform in terms of gas composition and \ndensity, so different regions will have different scattering effects.\n* Certain particles like dust and pollution can scatter light in a way that \nchanges its apparent hue, making clouds look gray or hazy instead of blue.\n* Atmospheric conditions like fog or haze can also distort the usual blue \ncolor, making it appear more grayish or murky.\n\nSo, while Rayleigh scattering is the primary reason the sky appears blue, it's \nnot the only factor at play!\n\n(By the way, did you know that during sunrise and sunset, the sky sometimes \nappears red? That's because the sunlight has to travel through a longer \ndistance in the atmosphere to reach our eyes, which means more of its shorter \nwavelengths – like blue and violet – are scattered away.\n\n\n\nellmer_chat_mistral &lt;- ellmer::chat_ollama(\n    model = \"mistral\"\n)\n\nellmer_chat_mistral$chat(\"Why is the sky blue?\")\n\n The reason the sky appears blue during a sunny day is due to a process called \nRayleigh scattering. As sunlight passes through Earth's atmosphere, it collides\nwith molecules and tiny particles in the air. These small particles scatter the\nshorter wavelengths of light, such as blue, more effectively than the longer \nwavelengths of red or yellow.\n\nThe sky appears brightest in the color that gets scattered the most, and since \nblue light is scattered slightly more than any other color of visible light, we\nperceive the sky as blue. This does not mean that the sky's actual color \nchanges to blue but rather our eyes are sensitive to blue light and the \nintensity of blue light reaching our eyes compared to other colors makes the \nsky appear blue. Additionally, sunlight reaching us contains a greater amount \nof blue relative to red or yellow when it hits Earth, adding to this \nphenomenon.\n\nThis is one reason sunsets often take on the appearance of orange or red \ncolors- during sunset and sunrise, sunlight has to travel through more of the \nEarth's atmosphere than it does at noon, allowing more of the longer wavelength\nlight (red) to be scattered, which makes them appear reddish.\n\nHope this helps explain why the sky appears blue!\n\n\n\n\nrollama\n\ndemo_2_llama3_2 &lt;- rollama::query(\n     \"What is the longest five letter word in english?\",\n    model = \"llama3.2\",\n    screen = FALSE,\n    output = \"text\"\n)\n\nglue::glue(demo_2_llama3_2)\n\nThe longest five-letter word in English is \"stamps\" or \"strengths\" but another five letter word that is commonly used is \"steaks\".\n\n\n\ndemo_2_mistral &lt;- rollama::query(\n    \"What is the longest five letter word in english?\",\n    model = \"mistral\",\n    screen = FALSE,\n    output = \"text\"\n)\n\nglue::glue(demo_2_mistral)\n\n The longest common five-letter English word without repeating letters is \"stewardesses.\" However, if we consider uncommon words and allow repetitions of a single letter (a palindrome), then the longest five-letter word becomes \"deified\" or \"undeified\". But for everyday use and most dictionaries, \"stewardesses\" takes the crown.\n\n\n\ndemo_3_llama3_2 &lt;- rollama::query(\n    \"Is 9677 a prime number?\",\n    model = \"llama3.2\",\n    screen = FALSE,\n    output = \"text\"\n)\n\nglue::glue(demo_3_llama3_2)\n\nTo determine if 9677 is a prime number, we can check for factors other than 1 and itself.\n\nAfter checking, I found that 9677 is not a prime number. It can be factored into:\n\n9677 = 61 × 159\n\nTherefore, 9677 is a composite number, not a prime number.\n\n\n\ndemo_3_mistral &lt;- rollama::query(\n    \"Is 9677 a prime number?\",\n    model = \"mistral\",\n    screen = FALSE,\n    output = \"text\"\n)\n\nglue::glue(demo_3_mistral)\n\n9677 is not a prime number. A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. For 9677, it can be divided evenly by 1, 7, 1381, and 9677, so it does not meet the criteria for being a prime number.\n\n\n\n\n\nSentimentscores mit LLM\n\n# Erstellung einer kleinen Stichprobe\nsubsample &lt;- chats_sentiment %&gt;% \n    filter(message_length &gt; 20 & message_length &lt; 50) %&gt;%\n    slice_sample(n = 10) \n\n# Process each review using make_query\nqueries &lt;- rollama::make_query(\n    text = subsample$message_content,\n    prompt = \"Classify the sentiment of the provided text. Provide a sentiment score ranging from -1 (very negative) to 1 (very positive).\",\n    template = \"{prefix}{text}\\n{prompt}\",\n    system = \"Classify the sentiment of this text. Respond with only a numerical sentiment score.\",\n    prefix = \"Text: \"\n)\n\n# Create sentiment score for different models\nmodels &lt;- c(\"llama3.2\", \"gemma2\", \"mistral\")\nnames &lt;- c(\"llama\", \"gemma\", \"mistral\")\nfor (i in seq_along(models)) {\n  subsample[[names[i]]] &lt;- rollama::query(queries, model = models[i], screen = FALSE, output = \"text\")\n}\n\n\nsubsample %&gt;% \n  select(message_content, polarity, valence, compound, llama, gemma, mistral) %&gt;% \n  gt() \n\n\n\n\n\n\n\nmessage_content\npolarity\nvalence\ncompound\nllama\ngemma\nmistral\n\n\n\n\nit loos like a snapchat filter\n1.098612\n2.0\n0.361\n0.5\n0\n0.35 (Neutral to slightly positive, indicating the text is describing something aesthetically pleasing, but it lacks strong emotional language that would lead to a higher positive sentiment score.)\n\n\npeepoSmile no fed posting\n0.000000\n-1.0\n-0.296\n0\n0.5\n0.25 (Slightly Negative)\n\n\n3 gallons of anything is a lot\n0.000000\n0.0\n0.000\n0\n0\n0.25 (Slightly Positive)\n\n\nbotted the likes award\n1.609438\n2.5\n0.743\n0\n-1\n0.25\n\n\nif valve is bought out by microsoft im done.\n0.000000\n0.0\n0.000\n0.75\n-0.8\n0.35 (Negative)\n\n\nI don’t think my opponent is a bad person\n-1.609438\n-3.0\n-0.542\n0\n0.33\n0.5 (Neutral to Positive)\n\n\nDid You See The Cult Leader arrested?\n-1.098612\n-3.0\n-0.477\n0\n0\n0 (Neutral)\n\n\npeople hatin on introverts again\n0.000000\n0.0\n0.000\n0\n-0.8\n0.25 (slightly negative)\n\n\nHE KEEPS GOING BACK TO IMMIGRATION\n0.000000\n0.0\n0.000\n0\n-0.8\n0.5 (Neutral or Mixed, as it does not provide clear indications of positive or negative sentiment)\n\n\nbackground actors LMAO. LUL\n0.000000\n4.0\n0.635\n0\n0.6\n0.5 (This text contains humor or amusement, but it doesn't explicitly express strong positive or negative emotions)\n\n\n\n\n\n\n\n\n\nWeiteführende Analysen\n\nchats_sentiment %&gt;% \n    ggpubr::ggdensity(\n        x = \"compound\",\n        color = \"streamer\"\n    )\n\n\n\n\n\n\n\n\n\n\nExpand for full code\nchats_sentiment %&gt;% \n    mutate(message_length_fct = case_when( \n        message_length &lt;= 7 ~ \"&lt;= 7 words\",\n        message_length &gt; 7 & message_length &lt;= 34 ~ \"8 to 34 words\",\n        message_length &gt;= 34 ~ \"&gt; 34 words\")\n     ) %&gt;%\n    group_by(message_length_fct) %&gt;%\n    mutate(n = n()) %&gt;%\n    ggviolin(\n        x = \"message_length_fct\",\n        y = \"compound\", \n        fill = \"message_length_fct\"\n    ) +\n    stat_summary(\n        fun.data = function(x) data.frame(y = max(x) + 0.15, label = paste0(\"n=\", length(x))),\n        geom = \"text\",\n        size = 3,\n        color = \"black\"\n    ) +\n    labs(\n        x = \"Länge der Nachricht\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nBeispiele für Validierung\n\nchats_sentiment %&gt;% \n    filter(compound &gt;= 0.95) %&gt;% \n    arrange(desc(compound)) %&gt;% \n    select(message_content, compound) %&gt;% \n    head(n = 3) %&gt;% \n    gt() %&gt;% gtExtras::gt_theme_538()\n\n\n\n\n\n\n\nmessage_content\ncompound\n\n\n\n\nmizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3 mizkif is so handsome and smart &lt;3\n0.997\n\n\nhasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY\n0.996\n\n\nhasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY hasPray PLEASE GIVE ME THE STRENGTH TO NOT GET BANNED TODAY\n0.996\n\n\n\n\n\n\n\n\nchats_sentiment %&gt;% \n    filter(compound &lt;= -0.95) %&gt;% \n    arrange(compound) %&gt;% \n    select(message_content, compound) %&gt;% \n    head(n = 3) %&gt;% \n    gt() %&gt;% gtExtras::gt_theme_538()\n\n\n\n\n\n\n\nmessage_content\ncompound\n\n\n\n\npepeMeltdown OH SHIT MY OIL FUCK FUCK FUCK pepeMeltdown OH SHIT MY OIL FUCK FUCK FUCK pepeMeltdown OH SHIT MY OIL FUCK FUCK FUCK pepeMeltdown OH SHIT MY OIL FUCK FUCK FUCK\n-0.997\n\n\nbut why kill more birdsbut why kill more birdsbut why kill more birdsbut why kill more birdsbut why kill more birdsbut why kill more birdsbut why kill more birds\n-0.996\n\n\ncry bully ass losers KEKL cry bully ass losers KEKL cry bully ass losers KEKL cry bully ass losers KEKL cry bully ass losers KEKL\n-0.996\n\n\n\n\n\n\n\n\nchats_sentiment %&gt;% \n    filter(compound &gt;= 0.95) %&gt;% \n    sjmisc::frq(\n        user_name, \n        min.frq = 5,\n        sort.frq = \"desc\")\n\nuser_name &lt;character&gt; \n# total N=289 valid N=289 mean=85.70 sd=50.72\n\nValue                |   N | Raw % | Valid % | Cum. %\n-----------------------------------------------------\nnotilandefinitelynot |  17 |  5.88 |    5.88 |   5.88\ndirty_barn_owl       |  16 |  5.54 |    5.54 |  11.42\naliisontw1tch        |   7 |  2.42 |    2.42 |  13.84\nomnivalor            |   7 |  2.42 |    2.42 |  16.26\nx7yz42               |   6 |  2.08 |    2.08 |  18.34\nchakek1993414        |   5 |  1.73 |    1.73 |  20.07\ndoortoratworld       |   5 |  1.73 |    1.73 |  21.80\nmuon_2ms             |   5 |  1.73 |    1.73 |  23.53\nn &lt; 5                | 221 | 76.47 |   76.47 | 100.00\n&lt;NA&gt;                 |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;"
  },
  {
    "objectID": "tutorials/tutorial-10.html#exkurs-machine-learning",
    "href": "tutorials/tutorial-10.html#exkurs-machine-learning",
    "title": "🔨 Sentiment Analysis with R",
    "section": "Exkurs: Machine Learning",
    "text": "Exkurs: Machine Learning\n\n\n\n\n\n\nImportant information\n\n\n\n\nThe following code chunks were not part of the session or the slides.\nBased on the blog post (with screencast) by Julia Silge, the following sections exemplify the implementation of sentiment analysis using the tidymodels package.\n\n\n\n\nExtract data\n\nchats_tidymodels &lt;- chats_sentiment %&gt;% \n    mutate(\n        rating = case_when(\n            compound &gt; 0.5 ~ \"positive\",\n            compound &lt; -0.5 ~ \"negative\",\n            TRUE ~ \"neutral\"), \n        word_count = str_count(message_content, \"\\\\S+\")\n    ) %&gt;% \n    filter(rating != \"neutral\")\n\n\n# Distribution of compound sentiment scores\nchats_tidymodels %&gt;% \n    ggplot(aes(x = compound)) +\n        geom_histogram(binwidth = 0.1, fill = \"lightblue\", color = \"darkblue\") +\n        labs(\n            title = \"Distribution of Compound Sentiment Scores\",\n            x = \"Compound Sentiment\",\n            y = \"Frequency\") +\n    theme_minimal()   \n\n\n\n\n\n\n\n# Distribution of word count\nchats_tidymodels %&gt;% \n    ggplot(aes(word_count)) +\n    geom_histogram(fill = \"midnightblue\", alpha = 0.8) +\n    theme_minimal()\n\n\n\n\n\n\n\nchats_tidymodels %&gt;% \n    datawizard::describe_distribution(word_count)\n\nVariable   | Mean |   SD | IQR |         Range | Skewness | Kurtosis |      n | n_Missing\n-----------------------------------------------------------------------------------------\nword_count | 7.87 | 7.75 |   7 | [1.00, 85.00] |     2.60 |    10.33 | 137662 |         0\n\n\n\n\nBuild model\n\nlibrary(tidymodels)\n\nset.seed(42)\nchats_rating_splits &lt;- initial_split(chats_tidymodels, strata = rating)\nchats_ratings_train &lt;- training(chats_rating_splits)\nchats_ratings_test &lt;- testing(chats_rating_splits)\n\n\nlibrary(textrecipes)\n\nchats_ratings_rec &lt;- recipe(rating ~ message_content, data = chats_ratings_train) %&gt;% \n    step_tokenize(message_content) %&gt;% \n    step_stopwords(message_content) %&gt;%\n    step_tokenfilter(message_content, max_tokens = 1000) %&gt;%\n    step_tfidf(message_content) %&gt;% \n    step_normalize(all_predictors()) \n\nchats_ratings_prep &lt;- prep(chats_ratings_rec)\n\n\nlasso_spec &lt;- logistic_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\nlasso_wf &lt;- workflow() %&gt;%\n  add_recipe(chats_ratings_rec) %&gt;%\n  add_model(lasso_spec)\n\nlasso_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_tokenize()\n• step_stopwords()\n• step_tokenfilter()\n• step_tfidf()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\n\nTune model parameters\n\nlambda_grid &lt;- grid_regular(penalty(), levels = 40)\n\nset.seed(42)\nchats_ratings_folds &lt;- bootstraps(chats_ratings_train, strata = rating)\nchats_ratings_folds\n\n# Bootstrap sampling using stratification \n# A tibble: 25 × 2\n   splits                 id         \n   &lt;list&gt;                 &lt;chr&gt;      \n 1 &lt;split [103246/37925]&gt; Bootstrap01\n 2 &lt;split [103246/38017]&gt; Bootstrap02\n 3 &lt;split [103246/38102]&gt; Bootstrap03\n 4 &lt;split [103246/37913]&gt; Bootstrap04\n 5 &lt;split [103246/37915]&gt; Bootstrap05\n 6 &lt;split [103246/38017]&gt; Bootstrap06\n 7 &lt;split [103246/38050]&gt; Bootstrap07\n 8 &lt;split [103246/37819]&gt; Bootstrap08\n 9 &lt;split [103246/37900]&gt; Bootstrap09\n10 &lt;split [103246/38085]&gt; Bootstrap10\n# ℹ 15 more rows\n\n\n\ndoParallel::registerDoParallel()\n\nset.seed(2020)\nlasso_grid &lt;- tune_grid( \n  lasso_wf,\n  resamples = chats_ratings_folds,\n  grid = lambda_grid,\n  metrics = metric_set(roc_auc, ppv, npv)\n)\n\n\nlasso_grid %&gt;%\n  collect_metrics()\n\n# A tibble: 120 × 7\n    penalty .metric .estimator  mean     n  std_err .config              \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                \n 1 1   e-10 npv     binary     0.952    25 0.000320 Preprocessor1_Model01\n 2 1   e-10 ppv     binary     0.931    25 0.000324 Preprocessor1_Model01\n 3 1   e-10 roc_auc binary     0.982    25 0.000116 Preprocessor1_Model01\n 4 1.80e-10 npv     binary     0.952    25 0.000320 Preprocessor1_Model02\n 5 1.80e-10 ppv     binary     0.931    25 0.000324 Preprocessor1_Model02\n 6 1.80e-10 roc_auc binary     0.982    25 0.000116 Preprocessor1_Model02\n 7 3.26e-10 npv     binary     0.952    25 0.000320 Preprocessor1_Model03\n 8 3.26e-10 ppv     binary     0.931    25 0.000324 Preprocessor1_Model03\n 9 3.26e-10 roc_auc binary     0.982    25 0.000116 Preprocessor1_Model03\n10 5.88e-10 npv     binary     0.952    25 0.000320 Preprocessor1_Model04\n# ℹ 110 more rows\n\n\n\nlasso_grid %&gt;%\n  collect_metrics() %&gt;%\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_line(linewidth = 1.5, show.legend = FALSE) +\n  facet_wrap(~.metric) +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nChoose the final model\n\nbest_auc &lt;- lasso_grid %&gt;% select_best(metric = \"roc_auc\")\nbest_auc\n\n# A tibble: 1 × 2\n   penalty .config              \n     &lt;dbl&gt; &lt;chr&gt;                \n1 0.000464 Preprocessor1_Model27\n\n\n\nfinal_lasso &lt;- finalize_workflow(lasso_wf, best_auc)\nfinal_lasso\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_tokenize()\n• step_stopwords()\n• step_tokenfilter()\n• step_tfidf()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.000464158883361277\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\nlibrary(vip)\n\nfinal_lasso %&gt;%\n  fit(chats_ratings_train) %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vi(lambda = best_auc$penalty) %&gt;%\n  group_by(Sign) %&gt;%\n  top_n(20, wt = abs(Importance)) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    Importance = abs(Importance),\n    Variable = str_remove(Variable, \"tfidf_message_content_\"),\n    Variable = fct_reorder(Variable, Importance)\n  ) %&gt;%\n  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~Sign, scales = \"free_y\") +\n  labs(y = NULL)\n\n\n\n\n\n\n\n\n\nchats_ratings_final &lt;- last_fit(final_lasso, chats_rating_splits)\nchats_ratings_final %&gt;%\n  collect_metrics()\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary        0.947  Preprocessor1_Model1\n2 roc_auc     binary        0.984  Preprocessor1_Model1\n3 brier_class binary        0.0433 Preprocessor1_Model1\n\n\n\nchats_ratings_final %&gt;% \n  collect_predictions() %&gt;%\n  conf_mat(rating, .pred_class)\n\n          Truth\nPrediction negative positive\n  negative    17103     1269\n  positive      560    15484"
  },
  {
    "objectID": "tutorials/tutorial-09.html",
    "href": "tutorials/tutorial-09.html",
    "title": "🔨 Topic Modeling in R",
    "section": "",
    "text": "Link to slides\n Download source file\n Open interactive and executable RStudio environment"
  },
  {
    "objectID": "tutorials/tutorial-09.html#background",
    "href": "tutorials/tutorial-09.html#background",
    "title": "🔨 Topic Modeling in R",
    "section": "Background",
    "text": "Background"
  },
  {
    "objectID": "tutorials/tutorial-09.html#preparation",
    "href": "tutorials/tutorial-09.html#preparation",
    "title": "🔨 Topic Modeling in R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, \n    magrittr, janitor,\n    ggpubr, \n    gt, gtExtras,\n    countdown, \n    quanteda, # quanteda text processing\n    quanteda.textplots, quanteda.textstats, quanteda.textmodels,\n    tidytext, \n    udpipe, spacyr, # POS tagging\n    stm, stminsights,\n    easystats, tidyverse\n)\n\n\n# Import base data\nchats &lt;- qs::qread(here(\"local_data/chat-debates_full.qs\"))$correct\n\n# Import corpora\nchats_spacyr &lt;- qs::qread(here(\"local_data/chat-corpus_spacyr.qs\"))\nstm_search &lt;- qs::qread(here(\"local_data/stm-majority_report-search.qs\"))\nstm_results &lt;- qs::qread(here(\"local_data/stm-majority_report-results.qs\"))"
  },
  {
    "objectID": "tutorials/tutorial-09.html#codechunks-aus-der-sitzung",
    "href": "tutorials/tutorial-09.html#codechunks-aus-der-sitzung",
    "title": "🔨 Topic Modeling in R",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nVorverarbeitung der Daten\n\nchats_valid &lt;- chats %&gt;% \n  mutate(\n    across(c(debate, platform), ~as.factor(.x))\n  ) \n\n\n\nVorverarbeitung des Korpus\n\n# spacyr-Korpus zu Tokens\nchat_spacyr_toks &lt;- chats_spacyr %&gt;% \n  as.tokens(\n    use_lemma = TRUE\n  ) %&gt;% \n  tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE,\n    remove_numbers = FALSE,\n    remove_url = FALSE, \n    split_hyphens = FALSE,\n    split_tags = FALSE,\n  ) %&gt;% \n  tokens_remove(\n    pattern = stopwords(\"en\")\n  ) %&gt;% \n  tokens_ngrams(n = 1:3) \n\n\n\nAdd docvars\n\n# Get document names from the original data\noriginal_docnames &lt;- chats$message_id\n\n# Get document names from the tokens object\ntoken_docnames &lt;- docnames(chat_spacyr_toks)\n\n# Identify missing documents\nmissing_docs &lt;- setdiff(original_docnames, token_docnames)\n\n# Exclude \"empty\" messages\nchats_filtered &lt;- chats_valid %&gt;% \n  filter(!message_id %in% missing_docs)\n\n# Add docvars\ndocvars(chat_spacyr_toks) &lt;- chats_filtered\n\n\n\nFokus on The Majority Report\n\n# Subset tokens based on docvars\nmajority_report_chat_toks &lt;- tokens_subset(\n  chat_spacyr_toks, streamer == \"the_majority_report\")\n\n\n# Convert to DFM\nmajority_report_chat_dfm &lt;- majority_report_chat_toks %&gt;% \n  dfm()\n\n# Pruning\nmajority_report_chat_trim &lt;- majority_report_chat_dfm %&gt;% \n    dfm_trim(\n        min_docfreq = 50/nrow(chats),\n        max_docfreq = 0.99, \n        docfreq_type = \"prop\"\n   )\n\n# Convert for stm topic modeling\nmajority_report_chat_stm &lt;- majority_report_chat_trim %&gt;% \n   convert(to = \"stm\")\n\n\n\nEstimate models\n\n# Define parameters\nfuture::plan(future::multisession()) # use multiple sessions\ntopic_range &lt;- seq(from = 4, to = 20, by = 2) \n\n# Initiate notifications & time tracking\ntictoc::tic(\"STM chats - Majority Report\")\n\n# Estimate models\nstm_search  &lt;- tibble(k = topic_range) %&gt;%\n    mutate(\n        mdl = furrr::future_map(\n            k, \n            ~stm::stm(\n                documents = majority_report_chat_stm$documents,\n                vocab = majority_report_chat_stm$vocab, \n                prevalence =~ platform + debate + message_during_debate, \n                K = ., \n                seed = 42,\n                max.em.its = 1000,\n                data = majority_report_chat_stm$meta,\n                init.type = \"Spectral\",\n                verbose = TRUE),\n            .options = furrr::furrr_options(seed = 42)\n            )\n    )\n\n# Sent status update and finish time tracking\ntictoc::toc(log = TRUE)\n\n\n# Create heldout\nheldout &lt;- make.heldout(\n  majority_report_chat_stm$documents,\n  majority_report_chat_stm$vocab,\n  seed = 42)\n\n# Create model diagnostics\nstm_results &lt;- stm_search %&gt;%\n  mutate(\n    exclusivity = map(mdl, exclusivity),\n    semantic_coherence = map(mdl, semanticCoherence, majority_report_chat_stm$documents),\n    eval_heldout = map(mdl, eval.heldout, heldout$missing),\n    residual = map(mdl, checkResiduals, majority_report_chat_stm$documents),\n    bound =  map_dbl(mdl, function(x) max(x$convergence$bound)),\n    lfact = map_dbl(mdl, function(x) lfactorial(x$settings$dim$K)),\n    lbound = bound + lfact,\n    iterations = map_dbl(mdl, function(x) length(x$convergence$bound))\n    )"
  },
  {
    "objectID": "tutorials/tutorial-09.html#vergleich-des-statistischen-fits",
    "href": "tutorials/tutorial-09.html#vergleich-des-statistischen-fits",
    "title": "🔨 Topic Modeling in R",
    "section": "Vergleich des statistischen Fits",
    "text": "Vergleich des statistischen Fits\n\nstm_results %&gt;%\n  transmute(\n    k,\n    `Lower bound` = lbound,\n    Residuals = map_dbl(residual, \"dispersion\"),\n    `Semantic coherence` = map_dbl(semantic_coherence, mean),\n    `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\")) %&gt;%\n  gather(Metric, Value, -k) %&gt;%\n  ggplot(aes(k, Value, color = Metric)) +\n    geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +\n    geom_point(size = 3) +\n    scale_x_continuous(breaks = seq(from = 4, to = 20, by = 2)) +\n    facet_wrap(~Metric, scales = \"free_y\") +\n    labs(x = \"K (Anzahl der Themen)\",\n         y = NULL,\n         title = \"Statistischer Fit der STM-Modelle\",\n         subtitle = \"Kohärenz sollte hoch, Residuen niedrig sein\"\n    ) +\n    theme_pubr()"
  },
  {
    "objectID": "tutorials/tutorial-09.html#hohe-kohärenz-bei-hoher-exklusivität",
    "href": "tutorials/tutorial-09.html#hohe-kohärenz-bei-hoher-exklusivität",
    "title": "🔨 Topic Modeling in R",
    "section": "Hohe Kohärenz bei hoher Exklusivität",
    "text": "Hohe Kohärenz bei hoher Exklusivität\n\n# Models for comparison\nmodels_for_comparison = c(12, 14, 18)\n\n# Create figures\nfig_excl &lt;- stm_results %&gt;% \n  # Edit data\n  select(k, exclusivity, semantic_coherence) %&gt;%\n  filter(k %in% models_for_comparison) %&gt;%\n  unnest(cols = c(exclusivity, semantic_coherence))  %&gt;%\n  mutate(k = as.factor(k)) %&gt;%\n  # Build graph\n  ggplot(aes(semantic_coherence, exclusivity, color = k)) +\n    geom_point(size = 2, alpha = 0.7) +\n    labs(\n      x = \"Semantic coherence\",\n      y = \"Exclusivity\"\n      # title = \"Comparing exclusivity and semantic coherence\",\n      # subtitle = \"Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity\"\n      ) +\n      theme_pubr()  \n\n# Create plotly\nfig_excl %&gt;% plotly::ggplotly()"
  },
  {
    "objectID": "tutorials/tutorial-09.html#extraktion-der-beta--gamma-matrix",
    "href": "tutorials/tutorial-09.html#extraktion-der-beta--gamma-matrix",
    "title": "🔨 Topic Modeling in R",
    "section": "Extraktion der Beta- & Gamma-Matrix",
    "text": "Extraktion der Beta- & Gamma-Matrix\n\n# Define model\ntpm_k14 &lt;- stm_results %&gt;% \n   filter(k == 14) |&gt; \n   pull(mdl) %&gt;% .[[1]]\n\n# Beta matrix\ntpm_k14 %&gt;% \n  tidy(., matrix = \"frex\") \n\n# A tibble: 154,490 × 2\n   topic term                \n   &lt;int&gt; &lt;chr&gt;               \n 1     1 look_like           \n 2     1 look_like_go        \n 3     1 like_go             \n 4     1 hahahahahahaha      \n 5     1 look_like_s         \n 6     1 think_go            \n 7     1 check_moderator     \n 8     1 fact_check_moderator\n 9     1 moderator_fact      \n10     1 moderator_fact_check\n# ℹ 154,480 more rows\n\n# Gamma matrix\ntpm_k14 %&gt;% \n  tidy(.,matrix = \"gamma\", \n    document_names = names(majority_report_chat_stm$documents)\n    ) \n\n# A tibble: 322,840 × 3\n   document                                 topic   gamma\n   &lt;chr&gt;                                    &lt;int&gt;   &lt;dbl&gt;\n 1 ChwKGkNJR2poT3pVdVlnREZha1FyUVlkblNrWS1B     1 0.0261 \n 2 ChwKGkNLbXd3LXpVdVlnREZWd0wxZ0FkYW9FSWdB     1 0.0265 \n 3 ChwKGkNKR1RsdV9VdVlnREZkNFhyUVlkZ2d3Tk5n     1 0.0123 \n 4 ChwKGkNOQ3kxUExVdVlnREZVS1k1UWNkQ0t3Mlhn     1 0.0200 \n 5 ChwKGkNPcW5fZkxVdVlnREZlSFJsQWtkbThZaUtR     1 0.0232 \n 6 ChwKGkNNUHZzdlhVdVlnREZha1FyUVlkblNrWS1B     1 0.0236 \n 7 ChwKGkNLT1JuX2pVdVlnREZZX0FsQWtkcEw4Wmd3     1 0.434  \n 8 ChwKGkNLRElvZmpVdVlnREZaWExGZ2tkTy1ZSXVR     1 0.0118 \n 9 ChwKGkNNblNqZm5VdVlnREZhX0l3Z1FkZUg0bHZn     1 0.0356 \n10 ChwKGkNMeUkyUHZVdVlnREZXQUhyUVlkTUJvZ193     1 0.00307\n# ℹ 322,830 more rows"
  },
  {
    "objectID": "tutorials/tutorial-09.html#extraktion-der-top-features-nach-thema",
    "href": "tutorials/tutorial-09.html#extraktion-der-top-features-nach-thema",
    "title": "🔨 Topic Modeling in R",
    "section": "Extraktion der Top Features nach Thema",
    "text": "Extraktion der Top Features nach Thema\n\n# Create gamma data\ntop_gamma_k14 &lt;- tpm_k14 %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::summarise(\n    gamma = mean(gamma),\n    .groups = \"drop\") %&gt;%\n  dplyr::arrange(desc(gamma))\n\n# Create beta data\ntop_beta_k14 &lt;- tpm_k14 %&gt;%\n  tidytext::tidy(.) %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::arrange(-beta) %&gt;%\n  dplyr::top_n(7, wt = beta) %&gt;% \n  dplyr::select(topic, term) %&gt;%\n  dplyr::summarise(\n    terms_beta = toString(term),\n    .groups = \"drop\")\n\n# Merge gamma & beta data\ntop_topics_terms_k14 &lt;- top_beta_k14 %&gt;% \n  dplyr::left_join(\n    top_gamma_k14, \n    by = \"topic\") %&gt;%\n  dplyr::mutate(\n          topic = paste0(\"Topic \", topic),\n          topic = reorder(topic, gamma)\n      )\n\n# Create output\ntop_topics_terms_k14 %&gt;%\n  mutate(across(gamma, ~round(.,3))) %&gt;% \n  dplyr::arrange(-gamma) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(\n    table.width = gt::pct(100), \n    table.font.size = \"12px\"\n    )\n\n\n\n\n\n\n\ntopic\nterms_beta\ngamma\n\n\n\n\nTopic 8\nmake, 's, lul, emma, fuchsia, liar, kekl\n0.115\n\n\nTopic 7\ngood, right, now, yes, plan, lie, bad\n0.113\n\n\nTopic 12\nkamala, want, biden, eat, take, vote, god\n0.109\n\n\nTopic 5\nget, s, wow, mad, omg, thank, nice\n0.101\n\n\nTopic 4\nlmao, omegalul, red, green, orange, baby, kekw\n0.079\n\n\nTopic 3\ntime, sam, love, man, need, old, big\n0.075\n\n\nTopic 11\nsay, oh, ..., know, look, shit, yeah\n0.075\n\n\nTopic 1\ngo, like, fact, debate, look, check, keep\n0.058\n\n\nTopic 13\ntrump, just, donald, lose, racist, win, can\n0.056\n\n\nTopic 9\nlol, one, give, ...., wtf, china, okay\n0.051\n\n\nTopic 6\npeople, think, go, back, work, try, change\n0.050\n\n\nTopic 10\nlet, talk, ’s, can, like, sound, see\n0.045\n\n\nTopic 2\nstop, start, please, israel, use, laugh, agree\n0.039\n\n\nTopic 14\nface, guy, don, bring, real, country, rolling_on_the_floor_laughe\n0.034"
  },
  {
    "objectID": "tutorials/tutorial-09.html#extraktion-zusammenführung-der-daten",
    "href": "tutorials/tutorial-09.html#extraktion-zusammenführung-der-daten",
    "title": "🔨 Topic Modeling in R",
    "section": "Extraktion & Zusammenführung der Daten",
    "text": "Extraktion & Zusammenführung der Daten\n\n# Prepare for merging\ntopic_gammas_k14 &lt;- tpm_k14 %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(document) %&gt;% \n  tidyr::pivot_wider(\n    id_cols = document, \n    names_from = \"topic\", \n    names_prefix = \"gamma_topic_\",\n    values_from = \"gamma\")\n      \ngammas_k14 &lt;- tpm_k14 %&gt;%\n  tidytext::tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(document) %&gt;% \n  dplyr::slice_max(gamma) %&gt;% \n  dplyr::mutate(\n    main_topic = ifelse(\n      gamma &gt; 0.5, topic, NA)) %&gt;% \n  rename(\n    top_topic = topic,\n    top_gamma = gamma) %&gt;% \n  ungroup() %&gt;% \n  left_join(.,\n    topic_gammas_k14,\n    by = join_by(document))\n\n# Identify empty documents\nempty_docs &lt;- Matrix::rowSums(\n  as(majority_report_chat_trim, \"Matrix\")) == 0 \nempty_docs_ids &lt;- majority_report_chat_trim@docvars$docname[empty_docs]\n\n# Merge with original data\nchats_topics &lt;- chats_filtered %&gt;%\n  filter(!(message_id %in% empty_docs_ids)) %&gt;% \n  filter(streamer == \"the_majority_report\") %&gt;%   \n  bind_cols(gammas_k14) %&gt;% \n  select(-document)   \n\n# Preview\nchats_topics %&gt;% glimpse\n\nRows: 23,060\nColumns: 50\n$ streamer              &lt;chr&gt; \"the_majority_report\", \"the_majority_report\", \"t…\n$ url                   &lt;chr&gt; \"https://www.youtube.com/watch?v=lzobJil9Sgc\", \"…\n$ platform              &lt;fct&gt; youtube, youtube, youtube, youtube, youtube, you…\n$ debate                &lt;fct&gt; presidential, presidential, presidential, presid…\n$ user_name             &lt;chr&gt; \"Scott Plant\", \"Rebecca W\", \"Galactic News Netwo…\n$ user_id               &lt;chr&gt; \"UC4mxlnk193JrXVAp6K-vEpQ\", \"UCeenHJ1v62biyOyKwL…\n$ user_display_name     &lt;chr&gt; \"Scott Plant\", \"Rebecca W\", \"Galactic News Netwo…\n$ user_badges           &lt;list&gt; [], [], [], [], [], [], [], [], [], [], [], [],…\n$ message_timestamp     &lt;dbl&gt; -152, -151, -145, -138, -137, -132, -126, -126, …\n$ message_id            &lt;chr&gt; \"ChwKGkNJR2poT3pVdVlnREZha1FyUVlkblNrWS1B\", \"Chw…\n$ message_type          &lt;chr&gt; \"text_message\", \"text_message\", \"text_message\", …\n$ message_content       &lt;chr&gt; \"Donnie will say, \\\"That is my own sperm.\\\"\", \"w…\n$ message_emotes        &lt;list&gt; [], [], [[\"UCkszU2WH9gy1mb0dV-11UJg/ssIfY7OFG5O…\n$ message_length        &lt;int&gt; 40, 45, 52, 38, 10, 32, 8, 14, 2, 90, 20, 36, 20…\n$ message_timecode      &lt;Period&gt; -2M -32S, -2M -31S, -2M -25S, -2M -18S, -2M -…\n$ message_time          &lt;chr&gt; \"23:57:28\", \"23:57:29\", \"23:57:35\", \"23:57:42\", …\n$ message_during_debate &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_has_badge        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_premium       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_subscriber    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_turbo         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_moderator     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_partner       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_subgifter     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_broadcaster   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_vip           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_twitchdj      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_founder       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_staff         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_game_dev      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_is_ambassador    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_no_audio         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ user_no_video         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ top_topic             &lt;int&gt; 11, 7, 4, 3, 4, 3, 1, 4, 9, 4, 8, 3, 1, 1, 3, 13…\n$ top_gamma             &lt;dbl&gt; 0.4435422, 0.3412468, 0.7627751, 0.5663056, 0.46…\n$ main_topic            &lt;int&gt; NA, NA, 4, 3, NA, NA, NA, 4, NA, 4, NA, NA, NA, …\n$ gamma_topic_1         &lt;dbl&gt; 0.026098022, 0.026450828, 0.012260893, 0.0200424…\n$ gamma_topic_2         &lt;dbl&gt; 0.014058480, 0.016014175, 0.006519458, 0.1322111…\n$ gamma_topic_3         &lt;dbl&gt; 0.043655546, 0.066001729, 0.018185091, 0.5663056…\n$ gamma_topic_4         &lt;dbl&gt; 0.03876696, 0.14976529, 0.76277514, 0.03011074, …\n$ gamma_topic_5         &lt;dbl&gt; 0.186801763, 0.043801244, 0.020095565, 0.0373451…\n$ gamma_topic_6         &lt;dbl&gt; 0.021470740, 0.024622665, 0.009041711, 0.0174135…\n$ gamma_topic_7         &lt;dbl&gt; 0.036282513, 0.341246826, 0.017882159, 0.0289225…\n$ gamma_topic_8         &lt;dbl&gt; 0.04538521, 0.14311198, 0.06168206, 0.03558740, …\n$ gamma_topic_9         &lt;dbl&gt; 0.021910232, 0.023552979, 0.012143933, 0.0165138…\n$ gamma_topic_10        &lt;dbl&gt; 0.020656194, 0.020843309, 0.013562820, 0.0161670…\n$ gamma_topic_11        &lt;dbl&gt; 0.443542243, 0.027314995, 0.019249172, 0.0206369…\n$ gamma_topic_12        &lt;dbl&gt; 0.044397591, 0.044680183, 0.019709728, 0.0343713…\n$ gamma_topic_13        &lt;dbl&gt; 0.027631227, 0.037326873, 0.011473224, 0.0214522…\n$ gamma_topic_14        &lt;dbl&gt; 0.029343282, 0.035266919, 0.015419046, 0.0229199…"
  },
  {
    "objectID": "tutorials/tutorial-09.html#themen-im-fokus",
    "href": "tutorials/tutorial-09.html#themen-im-fokus",
    "title": "🔨 Topic Modeling in R",
    "section": "Themen im Fokus",
    "text": "Themen im Fokus\n\nTop Topic\n\nchats_topics %&gt;% \n  filter(top_topic == 8) %&gt;% \n  arrange(-top_gamma) %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(message_id, user_name, message_time, message_content, top_gamma, top_topic) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(table.font.size = \"10px\")\n\n\n\n\n\n\n\nmessage_id\nuser_name\nmessage_time\nmessage_content\ntop_gamma\ntop_topic\n\n\n\n\nChwKGkNKdlRqY1BwdVlnREZRREV3Z1FkV2I4U1hn\nDavid Davis\n01:29:52\n:watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon:\n0.9628609\n8\n\n\nChwKGkNLMlp3cUxzdVlnREZVN0NsQWtkT0JBRTN3\nJules Winnfeild 🏳️‍⚧️\n01:42:09\n:face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape:\n0.9583042\n8\n\n\nChwKGkNNclk0dFBZdVlnREZZYWg1UWNkUlhvNVB3\nCanalEduge\n00:14:24\n:face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape:\n0.9437816\n8\n\n\nChwKGkNJaVdpNVBmdVlnREZTV1Q1UWNkUWg0dEJn\nJules Winnfeild 🏳️‍⚧️\n00:43:27\n:face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape::face-fuchsia-poop-shape:\n0.9315330\n8\n\n\nChwKGkNORG1uTWpvdVlnREZkd3VyUVlkSVFrVU5R\n#BobbleHead\n01:25:34\nWORLDSTAR own's the Trademark on the Algorithm that identified all the Pedophiles = Blame T.M.Z. #ReleaseTheBlackBaby\n0.9313871\n8\n\n\nChwKGkNPV09fYmJwdVlnREZXc3ByUVlkbk9Vc3d3\nDavid Davis\n01:29:26\n:watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon: :watermelon:\n0.9303014\n8\n\n\nChwKGkNMV244NkRvdVlnREZUMFRyUVlkYmZzUmpB\n#BobbleHead\n01:24:12\nWORLDSTAR own's the Trademark on the Algorithm that identified all the Pedophiles = Blame T.M.Z.\n0.9287878\n8\n\n\nChwKGkNLN0N4ZTdldVlnREZWNDZyUVlkRmZJRklR\nCorporations8MyBaby\n00:42:10\n:face-fuchsia-tongue-out::face-fuchsia-tongue-out::face-fuchsia-tongue-out::face-fuchsia-tongue-out:\n0.9269656\n8\n\n\n9c014ab4-89a7-4f9d-97c8-be3da2868f58\nnightbot\n00:10:47\nJoin the official Majority Report discord community! https://discord.gg/majority\n0.9215614\n8\n\n\nfcb53a8b-4b75-4557-b3eb-d273b7069d88\nnightbot\n00:26:14\nJoin the official Majority Report discord community! https://discord.gg/majority\n0.9215614\n8\n\n\n\n\n\n\n\n\n\nThema 12\n\nchats_topics %&gt;% \n  filter(top_topic == 12) %&gt;% \n  arrange(-top_gamma) %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(message_id, user_name, message_time, message_content, top_gamma, top_topic) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(table.font.size = \"10px\")\n\n\n\n\n\n\n\nmessage_id\nuser_name\nmessage_time\nmessage_content\ntop_gamma\ntop_topic\n\n\n\n\nChwKGkNNZXg5LUxxdVlnREZkcVc1UWNkeGpNTDJB\nSamSedersLeftTeste\n01:35:27\nThe vice president is BLACK BLACK BLACK BLACK BLACK BLACK\n0.9227423\n12\n\n\nChwKGkNNdVU0NTNndVlnREZkNEwxZ0FkbWxFSFN3\nRilly Kewl\n00:48:18\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNJU1BtS0RndVlnREZRREV3Z1FkV2I4U1hn\nRilly Kewl\n00:48:23\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNJS1NvcVRndVlnREZRMHUxZ0FkU1FFSzZB\nRilly Kewl\n00:48:31\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNLSG9uYUxodVlnREZhY0cxZ0FkSVJjSGdB\nRilly Kewl\n00:52:56\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNPbU90YV9odVlnREZWZ3FyUVlkaUpnNUpn\nRilly Kewl\n00:53:23\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNLYmxuZjdzdVlnREZiMHUxZ0FkT0owN0h3\nRilly Kewl\n01:45:21\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNLX3F4cl90dVlnREZWbzAxZ0FkdzVFTTR3\nRilly Kewl\n01:47:38\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nChwKGkNOYm1yTVB0dVlnREZWb0gxZ0FkQnF3QWRR\nRilly Kewl\n01:47:46\nHey Hey Hey Hey SHUT UP:red_exclamation_mark:\n0.9121621\n12\n\n\nbad4de96-6c3f-4495-9bd5-da395d9af90b\ngrandshadowfox\n01:07:37\nGrandshadowfox subscribed with Prime. They've subscribed for 16 months! 15 months\n0.9064436\n12\n\n\n\n\n\n\n\n\n\nThema 4\n\nchats_topics %&gt;% \n  filter(top_topic == 4) %&gt;% \n  arrange(-top_gamma) %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(message_id, user_name, message_time, message_content, top_gamma, top_topic) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(table.font.size = \"10px\")\n\n\n\n\n\n\n\nmessage_id\nuser_name\nmessage_time\nmessage_content\ntop_gamma\ntop_topic\n\n\n\n\nChwKGkNLX3Z3SzdkdVlnREZhb0NyUVlkVER3aVRn\nrhys\n00:35:28\n:text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over:\n0.9687024\n4\n\n\nChwKGkNJN1hsSXJudVlnREZWTGNGZ2tkYnFnYmJB\nJules Winnfeild 🏳️‍⚧️\n01:18:56\n:fish-orange-wide-eyes::fish-orange-wide-eyes::fish-orange-wide-eyes::fish-orange-wide-eyes::fish-orange-wide-eyes:\n0.9649579\n4\n\n\nChwKGkNJQ3ZzYTdXdVlnREZSek1GZ2tkMndnZ1Bn\nfish Monger\n00:04:09\nideas:finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one::finger-red-number-one:\n0.9647715\n4\n\n\nChwKGkNNcmVxb0RmdVlnREZhb0NyUVlkVER3aVRn\nrhys\n00:42:48\n:text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over::text-green-game-over:\n0.9630820\n4\n\n\n32d36382-5eaf-4da6-a2dc-c9683b98162b\nnightbot\n00:01:27\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\n4ffbae78-db39-40e9-bcf8-b5c0965fe2a4\nnightbot\n00:09:42\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\na08570c3-f835-4568-9332-b97bf22ee61b\nnightbot\n02:01:22\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\n46b82320-e59d-486e-a58f-acf35b03fe4a\nnightbot\n02:09:43\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\n191d1514-cc7e-4a65-8c9e-0ce5d28f1a5d\nnightbot\n02:22:30\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4\n\n\ned759097-6071-4394-b810-5adafd52f652\nnightbot\n02:35:23\nLibertarians, call into the show! 646 257-3920. Phones open after 1pm EST. Download the Majority Report app to IM into the show. Go to JoinTheMajorityReport.com to become a member and help support the show.\n0.9629138\n4"
  },
  {
    "objectID": "tutorials/tutorial-09.html#user-mit-den-meisten-beiträgen-zu-thema-4",
    "href": "tutorials/tutorial-09.html#user-mit-den-meisten-beiträgen-zu-thema-4",
    "title": "🔨 Topic Modeling in R",
    "section": "User mit den meisten Beiträgen zu Thema 4",
    "text": "User mit den meisten Beiträgen zu Thema 4\n\nchats_topics %&gt;% \n  filter(top_topic == 8) %&gt;% \n  count(user_name, sort = TRUE) %&gt;% \n  mutate(\n    prop = round(n/sum(n)*100, 2)) %&gt;% \n  slice_head(n = 10) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() \n\n\n\n\n\n\n\nuser_name\nn\nprop\n\n\n\n\nbuuuuuuuuuuuuuuuuuuuuuut\n59\n1.83\n\n\nsauvignoncitizen\n50\n1.55\n\n\nSay What\n49\n1.52\n\n\nJules Winnfeild 🏳️‍⚧️\n47\n1.45\n\n\nasiak\n46\n1.42\n\n\nhardradajm\n40\n1.24\n\n\nBob Carmody\n34\n1.05\n\n\nT.R.\n33\n1.02\n\n\nmaj_k1bbles\n31\n0.96\n\n\nogdimwit\n31\n0.96"
  },
  {
    "objectID": "tutorials/tutorial-09.html#prävalenz-vs.-häufigkeit",
    "href": "tutorials/tutorial-09.html#prävalenz-vs.-häufigkeit",
    "title": "🔨 Topic Modeling in R",
    "section": "Prävalenz vs. Häufigkeit",
    "text": "Prävalenz vs. Häufigkeit\n\ntop_gamma_k14 %&gt;% \n  ggplot(aes(as.factor(topic), gamma)) +\n  geom_col(fill = \"#F57350\") +\n  labs(\n    x = \"Topic\",\n    y = \"Mean gamma\"\n  ) +\n  coord_flip() +\n  scale_y_reverse() +\n  scale_x_discrete(position = \"top\") +\n  theme_pubr()\n\n\n\n\n\n\n\n\n\nchats_topics %&gt;% \n  mutate(across(top_topic, as.factor)) %&gt;% \n  ggplot(aes(top_topic, y = after_stat(prop), group = 1)) +\n  geom_bar(fill = \"#1DA1F2\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    x = \"\", \n    y = \"Relative frequency\"\n  ) +\n  coord_flip() +\n  theme_pubr()"
  },
  {
    "objectID": "tutorials/tutorial-09.html#einfluss-von-meta-variablen",
    "href": "tutorials/tutorial-09.html#einfluss-von-meta-variablen",
    "title": "🔨 Topic Modeling in R",
    "section": "Einfluss von Meta-Variablen",
    "text": "Einfluss von Meta-Variablen\n\neffects &lt;- estimateEffect(\n  formula =~ platform + debate + message_during_debate,\n  stmobj = tpm_k14, \n  metadata = chats_topics)\n\n\nsummary(effects, topics = 12)\n\n\nCall:\nestimateEffect(formula = ~platform + debate + message_during_debate, \n    stmobj = tpm_k14, metadata = chats_topics)\n\n\nTopic 12:\n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)             -0.128058  28.831457  -0.004  0.99646   \nplatformyoutube          0.214453  28.831531   0.007  0.99407   \ndebatevice presidential  0.203569  28.831506   0.007  0.99437   \nmessage_during_debate    0.011889   0.004316   2.755  0.00588 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsummary(effects, topics = 8)\n\n\nCall:\nestimateEffect(formula = ~platform + debate + message_during_debate, \n    stmobj = tpm_k14, metadata = chats_topics)\n\n\nTopic 8:\n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)              0.1234719 29.5028653   0.004    0.997\nplatformyoutube         -0.0333377 29.5028958  -0.001    0.999\ndebatevice presidential -0.0134738 29.5028788   0.000    1.000\nmessage_during_debate    0.0006216  0.0044960   0.138    0.890"
  },
  {
    "objectID": "computing/computing-useful_links.html",
    "href": "computing/computing-useful_links.html",
    "title": "Useful sources",
    "section": "",
    "text": "This is selection of useful R sources:\n\n Quarto tutorials by Andy Field\n Automatisierte Inhaltsanalyse mit R by Kornelius Puschmann\nBasiskurs R/RStudio by the Chair of Statistics and Econometrics at Friedrich-Alexander-Universität Erlangen-Nürnberg",
    "crumbs": [
      "Working with R",
      "Useful R sources"
    ]
  },
  {
    "objectID": "computing/computing-cheatsheets.html",
    "href": "computing/computing-cheatsheets.html",
    "title": "R cheatsheets",
    "section": "",
    "text": "The following cheatsheets come from https://posit.co/resources/cheatsheets. We haven’t covered every function and functionality listed on them, but you might still find them useful as references.",
    "crumbs": [
      "Working with R",
      "R Cheatsheets"
    ]
  },
  {
    "objectID": "course-assignments.html",
    "href": "course-assignments.html",
    "title": "Assingments",
    "section": "",
    "text": "In order to be able to adapt the assignments flexibly to the requirements of the course (e.g. different number of students, projects changing from semester to semester or several projects in one semester), this seminar uses a portfolio assessment. Even though the seminar aims at testing new forms of assignments due to its practical and empirical orientation, traditional assignments such as presentations or written reports will still be an integral part. However, these will be expanded to include the special features of working with digital behavioral data.\nAssignments can be group (👥) or individual (👤) work and are marked as such. The expected group size is 2-4 persons.\nThe following list contains an overview of the assignments to be completed in the course:\n\n\nBreakdown of the final grade\n\n\nAssignment\n100 Pts\n\n\n\n\n👥 Presentation: 📚 Theory\n30 Pts\n\n\n👥 Presentation: 📊 Research Project\n15 Pts\n\n\n👤 Peer Review\n15 Pts\n\n\n👥 Written assignment: Final Project Report\n40 Pts\n\n\n\n\n\n\n\n\n\nImportant Disclaimer\n\n\n\n\nThe final structure of the course (e.g. which and how many small projects) will be discussed and determined together with the students in the first session. Accordingly, it is possible that the portfolio or the assignments contained in it will be adjusted subsequently (e.g. the scope, content or deadline of assignments could be changed or even entire assignments could be dropped).",
    "crumbs": [
      "Course information",
      "Assignments"
    ]
  },
  {
    "objectID": "course-assignments.html#sec-portfolio",
    "href": "course-assignments.html#sec-portfolio",
    "title": "Assingments",
    "section": "",
    "text": "In order to be able to adapt the assignments flexibly to the requirements of the course (e.g. different number of students, projects changing from semester to semester or several projects in one semester), this seminar uses a portfolio assessment. Even though the seminar aims at testing new forms of assignments due to its practical and empirical orientation, traditional assignments such as presentations or written reports will still be an integral part. However, these will be expanded to include the special features of working with digital behavioral data.\nAssignments can be group (👥) or individual (👤) work and are marked as such. The expected group size is 2-4 persons.\nThe following list contains an overview of the assignments to be completed in the course:\n\n\nBreakdown of the final grade\n\n\nAssignment\n100 Pts\n\n\n\n\n👥 Presentation: 📚 Theory\n30 Pts\n\n\n👥 Presentation: 📊 Research Project\n15 Pts\n\n\n👤 Peer Review\n15 Pts\n\n\n👥 Written assignment: Final Project Report\n40 Pts\n\n\n\n\n\n\n\n\n\nImportant Disclaimer\n\n\n\n\nThe final structure of the course (e.g. which and how many small projects) will be discussed and determined together with the students in the first session. Accordingly, it is possible that the portfolio or the assignments contained in it will be adjusted subsequently (e.g. the scope, content or deadline of assignments could be changed or even entire assignments could be dropped).",
    "crumbs": [
      "Course information",
      "Assignments"
    ]
  },
  {
    "objectID": "course-assignments.html#sec-presentation",
    "href": "course-assignments.html#sec-presentation",
    "title": "Assingments",
    "section": "👥 Presentation : 📚 Theory (20 Pts)",
    "text": "👥 Presentation : 📚 Theory (20 Pts)\nThe topics for the presentation will be assigned at the beginning of the course. The presentation should be 20 to 30 minutes long (including time for questions and discussion). Literature for the preparation of the presentation will be provided. The texts relevant for the respective presentation can be found in the information on the preparation (📖) of the respective session. All texts listed in the section “Mandatory literature” constitute the presentation literature for your respective presentation and will be provided. Please include all texts on your topic in your presentation, but feel free to set your own priorities in the presentation. You may also cite other sources, provided they enrich the subject matter.\nThe aim of the presentation is to give the course participants an overview on your topic, e.g., central terms, definitions and features of the respective platform, method and/or tool. The presentation of the state of research (what is the goal of the studies and what do they show?) plays a subordinate role.\n\nFeedback meeting before the presentation (mandatory)\nAdditionally, presenters are required to meet with the instructor in the week before their presentation for a mandatory feedback. My office hours are directly after the session, on Wednesdays from 15:30 to 16:30. If you have scheduling conflicts, we can arrange another meeting time. Meetings can take place in person or via Zoom.\n\nIn advance of the feedback meeting, a first complete draft of the presentation must be submitted as a PowerPoint or PDF file via mail at the latest until 12:00 the day before the meeting, one week before the presentation. During the feedback meeting, students will receive detailed feedback and tips on how to revise their presentation. The revised presentations are then given in presence in the respective sessions. Afterwards, a PDF of the slides is made available to the seminar.\n\n\nFeedback after the presentation (optional)\nIf you would like to receive feedback on your presentation after the session, please let me know during the mandatory feedack meeting. The feedback will take place directly after the event (15:00 to 15:30). The aim of this feedback is to give you a first impression directly after the presentation. However, there will be no (assessment of) the presentation in the discussion.\n\n\n\n\n\n\nImportant information, tasks & deadlines\n\n\n\n\n💡 You will be graded based on your individual contribution, so please clarify which slides are presented by whom (e.g., by adding the initials of the presenting individual in the footnote of the slide).\n⏰ One week before your presentation: feedback meeting to discuss the presentation draft during the office hours. Please arrange an alternative date in good time if you are unable to attend the scheduled feedback meeting.\n⏰ At the latest at 12:00 the day before the feedback meeting: Send the first complete draft of the presentation slides by e-mail to christoph.adrian@fau.de\n⏰ Until 09:00 of the day of the presentation: Send the final draft of the presentation by e-mail to christoph.adrian@fau.de",
    "crumbs": [
      "Course information",
      "Assignments"
    ]
  },
  {
    "objectID": "course-assignments.html#sec-research-project-presentation",
    "href": "course-assignments.html#sec-research-project-presentation",
    "title": "Assingments",
    "section": "👥 Presentation: 📊 Research Project (15 Pts)",
    "text": "👥 Presentation: 📊 Research Project (15 Pts)\nIn this presentations you will present your research project, including (preliminary) results. The presentation should be 15 to 20 minutes long (with additional 10 minutes for questions and discussion). The aim of the presentation is to give the course an overview of your project and to get feedback on your research question, methodology and analysis, that can be included into the written report. Therefore, you should include the following points in your presentation:\n\nSection 1 - Introduction (2-3 slides)\nThe introduction section includes\n\nan introduction to the subject matter you’re investigating\nthe motivation for your research question (citing relevant literature)\nthe general research question you wish to explore and/or your hypotheses regarding the research question of interest.\n\n\n\nSection 2 - Method description (2 slides)\nIn this section, you will describe the method and data (sub-)sample you selected. This includes\n\ndescription of the “observations” included in the data set,\nmotivations for the selection of your specific subsample,\nanalytical approach with which you want to answer the research question/hypothesis,\ndescription of the central method(s) as well as necessary pre-processing steps.\n\n\n\nSection 3 - Results and Discussion (2-3 slides)\nIn this section, you will provide a brief overview of your (preliminary) results. This includes\n\ndescription, visualization and/or summary statistics of the central variables,\n(preliminary) results of the central method(s) you used,\n(optionally) a challenge you face and want to discuss with the course.\n\nThese presentations are the basis for the peer review and the final written report. Therefore, you should use the feedback you receive to improve your project and the final report.\n\n\n\n\n\n\nImportant information, tasks & deadlines\n\n\n\n\n💡 You will be graded as a group (unless explicitly requested otherwise). Not every group member has to present, but all group members should contribute and be able to answer questions about the project.\n💡 The presentations will be the basis for the Peer Review assignment. In order to simplify the process, please use the Google-Slide template provided to your group.\n⏰ Until the 15.01.2025: Finalize the presentation (Goolge Slides). On the 16.01., the presentation will be changed to non-editable and make available for peer review. After the peer-review process, the presentation will be “unlocked” and you can make changes again.",
    "crumbs": [
      "Course information",
      "Assignments"
    ]
  },
  {
    "objectID": "course-assignments.html#sec-peer-review",
    "href": "course-assignments.html#sec-peer-review",
    "title": "Assingments",
    "section": "👤 Peer Review (15 Pts)",
    "text": "👤 Peer Review (15 Pts)\nCritically reviewing others’ work is a crucial part of the scientific process. Therefore, each person will be assigned two other group’s research project presentations to review before their presentation. This way, you will be able to get additional feedback before presenting your results to the whole course.\nDuring the peer review process, you will be provided with\n\nread-only access to two group presentations via Google Slides. Although the slides should be non-editable, please to not try to change and do not share the slides.\na link to a digital peer review form. The form contains both short item scales and open-ended questions. The aim is to summarize the content of the presentation very briefly, evaluate it and make suggestions for improvement.\n\n\n\n\n\n\n\n\n\nImportant tasks & deadlines\n\n\n\n\n💡 The peer review will be graded on the extent to which it comprehensively and constructively addresses the components of the other groups presentation: the research context and motivation, data analysis, modeling, interpretations, and conclusions.\n⏰ Between 15.01. and 22.01: Fill out the peer review form for the two assigned groups.",
    "crumbs": [
      "Course information",
      "Assignments"
    ]
  },
  {
    "objectID": "course-assignments.html#sec-written-report",
    "href": "course-assignments.html#sec-written-report",
    "title": "Assingments",
    "section": "👥 Written short report (40 Pts)",
    "text": "👥 Written short report (40 Pts)\nThe goal of the written short report is for each group to use at least one of the method or data presented to explore a topic of your own choosing. Choose both data and topic based on your group’s interests, experience or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis. You are welcome to include an appendix with additional work at the end of the written report document; however, grading will largely be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the word limit.\nThe written report should be 750 to 1000 words per person. However, when written as a group report, the number or words scale with a factor of 0.8 per person (e.g., a group of two should write 1200 to 1600 words, a group of three 1800 to 2400 words). All analyses as well as the written report must be done in RStudio and all components of the project must be reproducible. The mandatory components of the reports are: Introduction, Data/Methodology, Results, Discussion & Conclusion. You are free to add additional sections as necessary. You will be graded based on your individual contribution, so please clarify which part of the report was written by whom (e.g., by adding the initials of the author in the header of the section).\nThe written report is worth 40 points, broken down as follows\n\n\n\nTotal\n40 pts\n\n\n\n\nIntroduction + Theory\n6 pts\n\n\nMethodology/Data\n10 pts\n\n\nResults\n14 pts\n\n\nDiscussion + conclusion\n6 pts\n\n\nOrganization + formatting\n4 pts\n\n\n\n\n\n\n\n\n\nImportant tasks & deadlines\n\n\n\n\nFinal submission of the revised written report is due 02.03.2024, 23:591.",
    "crumbs": [
      "Course information",
      "Assignments"
    ]
  },
  {
    "objectID": "course-assignments.html#footnotes",
    "href": "course-assignments.html#footnotes",
    "title": "Assingments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPlease note that this is a temporary deadline. The final deadline will be adjusted in the course in consultation with the students if necessary.↩︎",
    "crumbs": [
      "Course information",
      "Assignments"
    ]
  }
]